# Bulk Generation of Synthetic Data

This tutorial shows how to use `instructor` to generate large quantities of synthetic data at scale using Open AI's new Batch API.

??? tips "Why use the batch API?"

    Batch Jobs are 50% cheaper than running an inference job on demand

    | Model             | Pricing                     | Pricing with Batch API\*   |
    | ----------------- | --------------------------- | -------------------------- |
    | gpt-4o            | US$5.00 / 1M input tokens   | US$2.50 / 1M input tokens  |
    |                   | US$15.00 / 1M output tokens | US$7.50 / 1M output tokens |
    | gpt-4o-2024-05-13 | US$5.00 / 1M input tokens   | US$2.50 / 1M input tokens  |
    |                   | US$15.00 / 1M output tokens | US$7.50 / 1M output tokens |

    This makes them perfect for non time-sensitive tasks that involve large quantities of data.

## Getting Started

let's see how we can use the `BatchJob` object to create a `.jsonl` file which is compatible with the Batch API.

```python hl_lines="12-20 38-44"
from instructor.batch import BatchJob
from pydantic import BaseModel
import enum
import json

emails = [
    "Hello there I'm a Nigerian prince and I want to give you money",
    "Meeting with Thomas has been set at Friday next week",
    "Here are some weekly product updates from our marketing team",
]

messages = [ #(1)!
    [
        {
            "role": "system",
            "content": f"Classify the following email {email}",
        }
    ]
    for email in emails
]


class Labels(str, enum.Enum):
    """Enumeration for single-label text classification."""

    SPAM = "spam"
    NOT_SPAM = "not_spam"


class SinglePrediction(BaseModel):
    """
    Class for a single class label prediction.
    """

    class_label: Labels


with open("./test.jsonl", "w") as f: #(2)!
    for line in BatchJob.create_from_messages(
        messages_batch=messages,
        model="gpt-3.5-turbo",
        response_model=SinglePrediction,
    ):
        f.write(json.dumps(line) + "\n")
```

1.  We first generate a list of messages that mimic what we would have generated in a normal `openai` api call

2.  We then use the `create_from_messages` class method to specify the model and response_model that we want. `instructor` will handle the generation of the openai schema behind the scenes.

Once we've got this new `.jsonl` file, we can then use the new `instructor` cli's `batch` command to create a new batch job.

```bash
> % ls -a | grep test.jsonl
test.jsonl

> % instructor batch create-from-file --file-path test.jsonl
```

This will create a table much like what you see below

| Batch ID                       | Created At          | Status    | Failed | Completed | Total |
| ------------------------------ | ------------------- | --------- | ------ | --------- | ----- |
| batch_csIMxHsY8sUBtSDhMjn2v0I6 | 2024-07-15 22:30:49 | completed | 0      | 3         | 3     |

Once our batch job is complete, the status will change to `completed`.

??? "Cancelling A Job"

    If you'd like to cancel a batch job midway, you can do so too with the instructor `batch` cli command

    ```bash
    instructor batch cancel --batch-id <batch id here>
    ```

We can then download the file generated by the batch job using the cli command

```bash
instructor batch download-file --batch-id <batch id> --download-file-path <output file path>.jsonl
```

This will then create a `.jsonl` file with the generated content at the path that you specify.

## Parsing the generated response

We can then parse the generated response by using the `.parse_from_file` command provided by the `BatchJob` class.

```python hl_lines="21-23"
from instructor.batch import BatchJob
import enum
from pydantic import BaseModel


class Labels(str, enum.Enum):
    """Enumeration for single-label text classification."""

    SPAM = "spam"
    NOT_SPAM = "not_spam"


class SinglePrediction(BaseModel):
    """
    Class for a single class label prediction.
    """

    label: Labels


parsed, unparsed = BatchJob.parse_from_file(
    file_path="./output.jsonl", response_model=SinglePrediction
)
```

This will then return a list of responses that have been succesfully parsed into the `SinglePrediction` Base Model and a second list `unparsed` which contains the JSON objects which were not able to.
