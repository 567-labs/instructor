# Structured Output Integrations

Welcome to the Instructor integrations guide. This section provides detailed information about using structured outputs with various AI model providers.

## Supported Providers

Instructor supports a wide range of AI model providers, each with their own capabilities and features:

### OpenAI-Compatible Models

- [OpenAI](./openai.md) - GPT-3.5, GPT-4, and other OpenAI models
- [Azure OpenAI](./azure.md) - Microsoft's Azure-hosted OpenAI models

### Open Source & Self-Hosted Models

- [Ollama](./ollama.md) - Run open-source models locally
- [llama-cpp-python](./llama-cpp-python.md) - Python bindings for llama.cpp
- [Together AI](./together.md) - Host and run open source models

### Cloud AI Providers

- [Anthropic](./anthropic.md) - Claude and Claude 2 models
- [Google](./google.md) - PaLM and Gemini models
- [Vertex AI](./vertex.md) - Google Cloud's AI platform
- [Cohere](./cohere.md) - Command and other Cohere models
- [Groq](./groq.md) - High-performance inference platform
- [Mistral](./mistral.md) - Mistral's hosted models
- [Fireworks](./fireworks.md) - High-performance model inference
- [Cerebras](./cerebras.md) - AI accelerator platform

### Model Management

- [LiteLLM](./litellm.md) - Unified interface for multiple providers

## Common Concepts

All integrations share some common concepts:

- [Data Validation](../concepts/validation.md)
- [Streaming Support](../concepts/partial.md)
- [Model Validation](../concepts/models.md)
- [Instructor Hooks](../concepts/hooks.md)

## Need Help?

If you need help with a specific integration:

1. Check the provider-specific documentation
2. Look at the [examples](../examples/index.md)
3. Check our [GitHub issues](https://github.com/jxnl/instructor/issues)
4. Join our [Discord community](https://discord.gg/CV8sPM5k5Y)
