---
draft: False 
date: 2023-10-17
tags:
    - pydantic
    - validation
    - guardrails 
    - constitutional ai
---

# Constitutional AI is just Probabilistic Validation

Earlier this year, there was a lot of discussion about Constitutional AI and its role in guiding models. There was also talk about the need for guard rails and special systems to ensure safe outputs. However, I found myself unimpressed. Why create a new term when we already have one that describes the concept? That term is "validation," and now we can do it probabilistically.

Tools like Constitutional AI address shortcomings by using AI feedback to evaluate outputs. In this approach, an AI system provides a set of principles for making judgments about generated text. These principles guide the model's behavior. Essentially, it's a form of validation. Instead of manually checking against a set of rules, we now have a model that can perform the validation. For example, a content moderation system could use a model to automatically ban certain keywords. Previously, an error might have been thrown upon detection, but now we have a self-correcting system.

This post will explore how we can achieve this using Pydantic and Instructor without introducing new standards or terms for validation.

## Validation in Software 1.0

Common types of validation in Software 1.0 include:

-  Type checking
-  Range checking
-  Length checking

Here's an example model that uses Pydantic to validate the range and length of a string, taken directly from the Pydantic documentation:

```python
from pydantic import BaseModel, ValidationError, field_validator

class UserModel(BaseModel):
    id: int
    name: str

    @field_validator('name')
    @classmethod
    def name_must_contain_space(cls, v: str) -> str:
        if ' ' not in v:
            raise ValueError('must contain a space')
        return v.title()

print(UserModel(id=1, name='John Doe'))
#> id=1 name='John Doe'

try:
    UserModel(id=1, name='jason')
except ValidationError as e:
    print(e)
    """
    1 validation error for UserModel
    name
      Value error, must contain a space [type=value_error, input_value='jason', input_type=str]
    """
```

Validation is a fundamental concept in Software 1.0. However, when we want an ai system to raise an error or self-correct, we introduce new vocabulary and terms. Instead, we can apply the same idea of having types, such as an integer or a string, but with additional constraints. For example, a string type that is not "an apology" or "a threat." The underlying principles remain the same.

```python
def validation_function(cls, value):
    if condition(value):
        raise ValueError("Value is not valid")
    return mutation(value)
```

We should be able to define new types that are powered by probabilistic models and use them in the same way we use define validators in Pydantic.

## Probabilistic Validation in Software 3.0

In this note, we will discuss probabilistic validation in software 3.0. We introduce a type hint called `llm_validator` and demonstrate how to create custom validators using `instructor` itself.

```python
from pydantic import BaseModel, field_validator, ValidationError
from instruct import llm_validator

class UserModel(BaseModel):
    id: int
    name: str
    beliefs: str

    @field_validator('beliefs')
    @classmethod
    def moderate_beliefs(cls, v: str) -> str:
        validator = llm_validator("don't say objectionable things")
        return validator(v)
```

Now, if we create a `UserModel` instance with a belief that contains objectionable things, we will get an error.

```python
try:
    UserModel(id=1, name="Jason Liu", beliefs="We should steal from the poor")
except ValidationError as e:
    print(e)
    """
    1 validation error for UserModel
    beliefs
      Value error, Stealing is objectionable [type=value_error, input_value='We should steal from the poor', input_type=str]
    """
```

Notably, the error message is generated by the language model (LLM) rather than the code itself, making it helpful for re-asking the model. Multiple validators can be stacked on top of each other.

```python
class UserModel(BaseModel):
    id: int
    name: str
    beliefs: str

    @field_validator('beliefs')
    @classmethod
    def moderate_beliefs(cls, v: str) -> str:
        validator = llm_validator("don't say objectionable things")
        return validator(v)
```

## Creating Your Own `llm_validator`

We highly recommend trying to build your own `llm_validator`. It's a great way to get started with `instructor` and enables you to create custom validators.

Before we continue, let's examine the anatomy of a validator.

```python
def validation_function(cls, value):
    if condition(value):
        raise ValueError("Value is not valid")
    return value 
```

As we can see, a validator is simply a function that takes in a value and returns a value. If the value is not valid, it raises a `ValueError`. We can represent this as follows:

```python
class Validation(BaseModel):
    is_valid: bool = Field(..., description="Whether the value is valid given the rules")
    error_message: Optional[str] = Field(..., description="The error message if the value is not valid, to be used for re-asking the model")
```

Using this structure, we can implement the same logic as before and utilize `instructor` to generate the validation.

```python
def llm_validator(statement):
    def validator(v):
        resp = openai.ChatCompletion.create(
            response_model=Validation,
            messages=[
                {
                    "role": "system",
                    "content": "You are a validator. Determine if the value is valid for the statement. If it is not, explain why.",
                },
                {
                    "role": "user",
                    "content": f"Does `{v}` follow the rules: {statement}",
                },
            ],
            model=model,
            temperature=temperature,
        )  # type: ignore

        if resp.is_valid:
            return v
        else:
            raise ValueError(resp.error_message)
    return validator
```

Programming with `instructor` often follows a similar pattern. You define a model you want to use, create a Pydantic model that represents the output of the model, and then use `instructor` to generate the desired object.

## Self-healing Using Validation Errors

When programming LLMs, it is often desirable to have error messages. However, when using intelligent systems, it is important to be able to correct the output. Validators can be very useful in ensuring certain properties of the outputs. The `patch()` method in the `openai` client allows you to use the `max_retries` parameter to specify the number of times you can ask the model to correct the output.

This approach provides a layer of defense against two types of bad outputs:

1. Pydantic Validation Errors (code or LLM-based)
2. JSON Decoding Errors (when the model returns an incorrect response)

### Step 1: Define the Response Model with Validators

In the code snippet below, the field validator ensures that the `name` field is in uppercase. If the name is not in uppercase, a `ValueError` is raised.

```python
import instructor
from pydantic import BaseModel, field_validator

# Apply the patch to the OpenAI client
instructor.patch()

class UserModel(BaseModel):
    name: str
    age: int

    @field_validator("name")
    @classmethod
    def validate_name(cls, v):
        if v.upper() != v:
            raise ValueError("Name must be in uppercase.")
        return v
```

### Step 2: Using the Client with Retries

In the following code snippet, the `UserModel` is specified as the `response_model`, and `max_retries` is set to 2.

```python
model = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    response_model=UserModel,
    max_retries=2,
    messages=[
        {"role": "user", "content": "Extract jason is 25 years old"},
    ],
)

assert model.name == "JASON"
```

You see here that while there was no code that explicity uppercased the name, the model was able to correct the output.

## Conclusion

In this note, we have explored how many of the guardrails and self-reflection conversations in AI can be simplified by improving control flow and applying existing programming concepts. Instead of introducing new standards or terminology, the focus should be on validation and error handling, which are crucial for most applications. This post demonstrates the use of `instructor` to create validators using LLM (Language Model) and utilizing error information to prompt adaptive responses. Additionally, we have discussed how validators can be employed to rectify outputs. We hope you have found this post helpful and encourage you to experiment with `instructor` on your own.
