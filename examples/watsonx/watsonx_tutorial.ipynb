{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Started in Minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Instructor with a single command:\n",
    "\n",
    "```bash\n",
    "pip install -U instructor\n",
    "pip install ibm-watsonx-ai\n",
    "```\n",
    "\n",
    "Now, let's see Instructor in action with a simple example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "\n",
    "client = Model(\n",
    "    model_id=\"meta-llama/llama-3-1-70b-instruct\",\n",
    "    credentials=Credentials(\n",
    "        api_key = os.environ.get(\"WATSONX_API_KEY\"),\n",
    "        url = os.environ.get(\"WATSONX_URL\")),\n",
    "    project_id= os.environ.get(\"WATSONX_PROJECT_ID\")\n",
    ")\n",
    "\n",
    "# Patch the client with Watsonx\n",
    "instructor_client=instructor.from_watsonx(\n",
    "    client=client,\n",
    "    mode=instructor.Mode.WATSONX_TOOLS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your desired output structure\n",
    "class UserInfo(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "user_info = instructor_client.messages.create(\n",
    "    response_model=UserInfo,\n",
    "    max_retries=3,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"John Doe is 30 years old.\"}],\n",
    ")\n",
    "print(user_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_info.name)\n",
    "print(user_info.age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Hooks\n",
    "Instructor provides a powerful hooks system that allows you to intercept and log various stages of the LLM interaction process. Here's a simple example demonstrating how to use hooks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hook functions\n",
    "def log_kwargs(**kwargs):\n",
    "    print(f\"Function called with kwargs: {kwargs}\")\n",
    "\n",
    "def log_exception(exception: Exception):\n",
    "    print(f\"An exception occurred: {str(exception)}\")\n",
    "\n",
    "instructor_client.on(\"completion:kwargs\", log_kwargs)\n",
    "instructor_client.on(\"completion:error\", log_exception)\n",
    "\n",
    "user_info = instructor_client.messages.create(\n",
    "    \n",
    "    response_model=UserInfo,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Extract the user name: 'John is 20 years old'\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Name: {user_info.name}, Age: {user_info.age}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using Watsonx and Pydantic\n",
    "\n",
    "This tutorial showcases how to implement text classification tasks—specifically, single-label and multi-label classifications—using the Watsonx API and Pydantic models. If you want to see full examples check out the hub examples for single classification and multi classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-Label Classification\n",
    "### Defining the Structures\n",
    "\n",
    "For single-label classification, we define a Pydantic model with a Literal field for the possible labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "class ClassificationResponse(BaseModel):\n",
    "    \"\"\"\n",
    "    A few-shot example of text classification:\n",
    "\n",
    "    Examples:\n",
    "    - \"Buy cheap watches now!\": SPAM\n",
    "    - \"Meeting at 3 PM in the conference room\": NOT_SPAM\n",
    "    - \"You've won a free iPhone! Click here\": SPAM\n",
    "    - \"Can you pick up some milk on your way home?\": NOT_SPAM\n",
    "    - \"Increase your followers by 10000 overnight!\": SPAM\n",
    "    \"\"\"\n",
    "\n",
    "    chain_of_thought: str = Field(\n",
    "        ...,\n",
    "        description=\"The chain of thought that led to the prediction.\",\n",
    "    )\n",
    "    label: Literal[\"SPAM\", \"NOT_SPAM\"] = Field(\n",
    "        ...,\n",
    "        description=\"The predicted class label.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying Text\n",
    "\n",
    "The function classify will perform the single-label classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(data: str) -> ClassificationResponse:\n",
    "    \"\"\"Perform single-label classification on the input text.\"\"\"\n",
    "    return instructor_client.messages.create(\n",
    "        response_model=ClassificationResponse,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Classify the following text: <text>{data}</text>\",\n",
    "            },\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and Evaluation\n",
    "\n",
    "Let's run examples to see if it correctly identifies spam and non-spam messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    for text, label in [\n",
    "        (\"Hey Jason! You're awesome\", \"NOT_SPAM\"),\n",
    "        (\"I am a nigerian prince and I need your help.\", \"SPAM\"),\n",
    "    ]:\n",
    "        prediction = classify(text)\n",
    "        assert prediction.label == label\n",
    "        print(f\"Text: {text}, Predicted Label: {prediction.label}\")\n",
    "        #> Text: Hey Jason! You're awesome, Predicted Label: NOT_SPAM\n",
    "        #> Text: I am a nigerian prince and I need your help., Predicted Label: SPAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Label Classification\n",
    "### Defining the Structures\n",
    "\n",
    "For multi-label classification, we'll update our approach to use Literals instead of enums, and include few-shot examples in the model's docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "class MultiClassPrediction(BaseModel):\n",
    "    \"\"\"\n",
    "    Class for a multi-class label prediction.\n",
    "\n",
    "    Examples:\n",
    "    - \"My account is locked\": [\"TECH_ISSUE\"]\n",
    "    - \"I can't access my billing info\": [\"TECH_ISSUE\", \"BILLING\"]\n",
    "    - \"When do you close for holidays?\": [\"GENERAL_QUERY\"]\n",
    "    - \"My payment didn't go through and now I can't log in\": [\"BILLING\", \"TECH_ISSUE\"]\n",
    "    \"\"\"\n",
    "\n",
    "    chain_of_thought: str = Field(\n",
    "        ...,\n",
    "        description=\"The chain of thought that led to the prediction.\",\n",
    "    )\n",
    "\n",
    "    class_labels: list[Literal[\"TECH_ISSUE\", \"BILLING\", \"GENERAL_QUERY\"]] = Field(\n",
    "        ...,\n",
    "        description=\"The predicted class labels for the support ticket.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying Text\n",
    "\n",
    "The function multi_classify is responsible for multi-label classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_classify(data: str) -> MultiClassPrediction:\n",
    "    \"\"\"Perform multi-label classification on the input text.\"\"\"\n",
    "    return instructor_client.messages.create(\n",
    "        response_model=MultiClassPrediction,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Classify the following support ticket: <ticket>{data}</ticket>\",\n",
    "            },\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and Evaluation\n",
    "\n",
    "Finally, we test the multi-label classification function using a sample support ticket.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-label classification\n",
    "ticket = \"My account is locked and I can't access my billing info.\"\n",
    "prediction = multi_classify(ticket)\n",
    "assert \"TECH_ISSUE\" in prediction.class_labels\n",
    "assert \"BILLING\" in prediction.class_labels\n",
    "print(f\"Ticket: {ticket}\")\n",
    "#> Ticket: My account is locked and I can't access my billing info.\n",
    "print(f\"Predicted Labels: {prediction.class_labels}\")\n",
    "#> Predicted Labels: ['TECH_ISSUE', 'BILLING']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using Literals and including few-shot examples, we've improved both the single-label and multi-label classification implementations. These changes enhance type safety and provide better guidance for the AI model, potentially leading to more accurate classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answering Questions with Validated Citations\n",
    "\n",
    "For the full code example, check out examples/citation_fuzzy_match.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This example shows how to use Instructor with validators to not only add citations to answers generated but also prevent hallucinations by ensuring that every statement made by the LLM is backed up by a direct quote from the context provided, and that those quotes exist!\n",
    "Two Python classes, Fact and QuestionAnswer, are defined to encapsulate the information of individual facts and the entire answer, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures\n",
    "### The Fact Class\n",
    "\n",
    "The Fact class encapsulates a single statement or fact. It contains two fields:\n",
    "\n",
    "    fact: A string representing the body of the fact or statement.\n",
    "    substring_quote: A list of strings. Each string is a direct quote from the context that supports the fact.\n",
    "\n",
    "#### Validation Method: validate_sources\n",
    "\n",
    "This method validates the sources (substring_quote) in the context. It utilizes regex to find the span of each substring quote in the given context. If the span is not found, the quote is removed from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field, BaseModel, model_validator, ValidationInfo\n",
    "import re\n",
    "\n",
    "class Fact(BaseModel):\n",
    "    fact: str = Field(...)\n",
    "    substring_quote: list[str] = Field(...)\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_sources(self, info: ValidationInfo) -> \"Fact\":\n",
    "        text_chunks = info.context.get(\"text_chunk\", None)\n",
    "        spans = list(self.get_spans(text_chunks))\n",
    "        self.substring_quote = [text_chunks[span[0] : span[1]] for span in spans]\n",
    "        return self\n",
    "\n",
    "    def get_spans(self, context):\n",
    "        for quote in self.substring_quote:\n",
    "            yield from self._get_span(quote, context)\n",
    "\n",
    "    def _get_span(self, quote, context):\n",
    "        for match in re.finditer(re.escape(quote), context):\n",
    "            yield match.span()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The QuestionAnswer Class¶\n",
    "\n",
    "This class encapsulates the question and its corresponding answer. It contains two fields:\n",
    "\n",
    "    question: The question asked.\n",
    "    answer: A list of Fact objects that make up the answer.\n",
    "\n",
    "#### Validation Method: validate_sources\n",
    "\n",
    "This method checks that each Fact object in the answer list has at least one valid source. If a Fact object has no valid sources, it is removed from the answer list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, model_validator\n",
    "\n",
    "class QuestionAnswer(BaseModel):\n",
    "    question: str = Field(...)\n",
    "    answer: list[Fact] = Field(...)\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def validate_sources(self) -> \"QuestionAnswer\":\n",
    "        self.answer = [fact for fact in self.answer if len(fact.substring_quote) > 0]\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Ask AI a Question¶\n",
    "## The ask_ai Function\n",
    "\n",
    "This function takes a string question and a string context and returns a QuestionAnswer object. It uses the Watsonx API to fetch the answer and then validates the sources using the defined classes.\n",
    "\n",
    "To understand the validation context work from pydantic check out pydantic's docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tool_choice_option=\"auto\",\n",
    "def ask_ai(question: str, context: str) -> QuestionAnswer:\n",
    "    \n",
    "    return instructor_client.messages.create(\n",
    "        response_model=QuestionAnswer,\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'system', \n",
    "                'content': 'You are a world class assistant to answer questions with correct and exact citations based on a given CONTEXT.'\n",
    "            }, \n",
    "            {\n",
    "                'role': 'user', \n",
    "                'content': 'You are a world class assistant to answer questions with correct and exact citations based on a given CONTEXT.'\n",
    "            }, \n",
    "            {\n",
    "                'role': 'assistant', \n",
    "                'content': 'I have the following context uppon which I have to base my answers.\\n' + f\"CONTEXT:{context}\\n\" \n",
    "            }, \n",
    "            {\n",
    "                'role': 'user', \n",
    "                'content': 'Question: ' + f'{question}'\n",
    "            }\n",
    "            \n",
    "            \n",
    "        ],\n",
    "        validation_context={\"text_chunk\": context},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Here's an example of using these classes and functions to ask a question and validate the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What did the author do during college?\"\n",
    "context = \"\"\"My name is Jason Liu, and I grew up in Toronto Canada but I was born in China. I went to an arts high school but in university I studied Computational Mathematics and physics. As part of coop I worked at many companies including Stitchfix, Facebook. I also started the Data Science club at the University of Waterloo and I was the president of the club for 2 years.'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging to DEBUG\n",
    "#logging.basicConfig(level=logging.DEBUG)\n",
    "print(ask_ai(question=question,context=context))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Resolution and Visualization for Legal Documents\n",
    "\n",
    "In this guide, we demonstrate how to extract and resolve entities from a sample legal contract. Then, we visualize these entities and their dependencies as an entity graph. This approach can be invaluable for legal tech applications, aiding in the understanding of complex documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Data Structures\n",
    "\n",
    "The Entity and Property classes model extracted entities and their attributes. DocumentExtraction encapsulates a list of these entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Property(BaseModel):\n",
    "    key: str\n",
    "    value: str\n",
    "    resolved_absolute_value: str\n",
    "\n",
    "\n",
    "class Entity(BaseModel):\n",
    "    id: int = Field(\n",
    "        ...,\n",
    "        description=\"Unique identifier for the entity, used for deduplication, design a scheme allows multiple entities\",\n",
    "    )\n",
    "    subquote_string: list[str] = Field(\n",
    "        ...,\n",
    "        description=\"Correctly resolved value of the entity, if the entity is a reference to another entity, this should be the id of the referenced entity, include a few more words before and after the value to allow for some context to be used in the resolution\",\n",
    "    )\n",
    "    entity_title: str\n",
    "    properties: list[Property] = Field(\n",
    "        ..., description=\"List of properties of the entity\"\n",
    "    )\n",
    "    dependencies: list[int] = Field(\n",
    "        ...,\n",
    "        description=\"List of entity ids that this entity depends  or relies on to resolve it\",\n",
    "    )\n",
    "\n",
    "\n",
    "class DocumentExtraction(BaseModel):\n",
    "    entities: list[Entity] = Field(\n",
    "        ...,\n",
    "        description=\"Body of the answer, each fact should be a separate object with a body and a list of sources\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Extraction and Resolution\n",
    "\n",
    "The ask_ai function utilizes Watsonx API to extract and resolve entities from the input content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from openai import OpenAI\n",
    "\n",
    "# Apply the patch to the OpenAI client\n",
    "# enables response_model keyword\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "\n",
    "def ask_ai(content) -> DocumentExtraction:\n",
    "    return instructor_client.messages.create(\n",
    "        #tool_choice_option=\"auto\",\n",
    "        response_model=DocumentExtraction,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Extract and resolve a list of entities from the following document:\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": content,\n",
    "            },\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Visualization\n",
    "\n",
    "generate_graph takes the extracted entities and visualizes them using Graphviz. It creates nodes for each entity and edges for their dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def generate_html_label(entity: Entity) -> str:\n",
    "    rows = [\n",
    "        f\"<tr><td>{prop.key}</td><td>{prop.resolved_absolute_value}</td></tr>\"\n",
    "        for prop in entity.properties\n",
    "    ]\n",
    "    table_rows = \"\".join(rows)\n",
    "    return f\"<<table border='0' cellborder='1' cellspacing='0'><tr><td colspan='2'><b>{entity.entity_title}</b></td></tr>{table_rows}</table>>\"\n",
    "\n",
    "\n",
    "def generate_graph(data: DocumentExtraction):\n",
    "    dot = Digraph(comment=\"Entity Graph\", node_attr={\"shape\": \"plaintext\"})\n",
    "\n",
    "    for entity in data.entities:\n",
    "        label = generate_html_label(entity)\n",
    "        dot.node(str(entity.id), label)\n",
    "\n",
    "    for entity in data.entities:\n",
    "        for dep_id in entity.dependencies:\n",
    "            dot.edge(str(entity.id), str(dep_id))\n",
    "\n",
    "    dot.render(\"entity.gv\", view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "\n",
    "Finally, execute the code to visualize the entity graph for the sample legal contract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\"\"\n",
    "Sample Legal Contract\n",
    "Agreement Contract\n",
    "\n",
    "This Agreement is made and entered into on 2020-01-01 by and between Company A (\"the Client\") and Company B (\"the Service Provider\").\n",
    "\n",
    "Article 1: Scope of Work\n",
    "\n",
    "The Service Provider will deliver the software product to the Client 30 days after the agreement date.\n",
    "\n",
    "Article 2: Payment Terms\n",
    "\n",
    "The total payment for the service is $50,000.\n",
    "An initial payment of $10,000 will be made within 7 days of the the signed date.\n",
    "The final payment will be due 45 days after [SignDate].\n",
    "\n",
    "Article 3: Confidentiality\n",
    "\n",
    "The parties agree not to disclose any confidential information received from the other party for 3 months after the final payment date.\n",
    "\n",
    "Article 4: Termination\n",
    "\n",
    "The contract can be terminated with a 30-day notice, unless there are outstanding obligations that must be fulfilled after the [DeliveryDate].\n",
    "\"\"\"  # Your legal contract here\n",
    "model = ask_ai(content)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_graph(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PII Data Extraction and Scrubbing\n",
    "## Overview\n",
    "\n",
    "This example demonstrates the usage of OpenAI's ChatCompletion model for the extraction and scrubbing of Personally Identifiable Information (PII) from a document. The code defines Pydantic models to manage the PII data and offers methods for both extraction and sanitation.\n",
    "\n",
    "## Defining the Structures\n",
    "\n",
    "First, Pydantic models are defined to represent the PII data and the overall structure for PII data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "# Define Schemas for PII data\n",
    "class Data(BaseModel):\n",
    "    index: int\n",
    "    data_type: str\n",
    "    pii_value: str\n",
    "\n",
    "\n",
    "class PIIDataExtraction(BaseModel):\n",
    "    \"\"\"\n",
    "    Extracted PII data from a document, all data_types should try to have consistent property names\n",
    "    \"\"\"\n",
    "\n",
    "    private_data: list[Data]\n",
    "\n",
    "    def scrub_data(self, content: str) -> str:\n",
    "        \"\"\"\n",
    "        Iterates over the private data and replaces the value with a placeholder in the form of\n",
    "        <{data_type}_{i}>\n",
    "        \"\"\"\n",
    "        for i, data in enumerate(self.private_data):\n",
    "            content = content.replace(data.pii_value, f\"<{data.data_type}_{i}>\")\n",
    "        return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting PII Data\n",
    "\n",
    "The Watsonx API is utilized to extract PII information from a given document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_DOCUMENT = \"\"\"\n",
    "# Fake Document with PII for Testing PII Scrubbing Model\n",
    "# (The content here)\n",
    "\"\"\"\n",
    "\n",
    "pii_data = instructor_client.messages.create(\n",
    "    response_model=PIIDataExtraction,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a world class PII scrubbing model, Extract the PII data from the following document\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": EXAMPLE_DOCUMENT,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Extracted PII Data:\")\n",
    "#> Extracted PII Data:\n",
    "print(pii_data.model_dump_json())\n",
    "\"\"\"\n",
    "{\"private_data\":[{\"index\":0,\"data_type\":\"Name\",\"pii_value\":\"John Doe\"},{\"index\":1,\"data_type\":\"Email\",\"pii_value\":\"john.doe@example.com\"},{\"index\":2,\"data_type\":\"Phone Number\",\"pii_value\":\"(555) 123-4567\"},{\"index\":3,\"data_type\":\"Address\",\"pii_value\":\"123 Main St, Anytown, USA\"},{\"index\":4,\"data_type\":\"Social Security Number\",\"pii_value\":\"123-45-6789\"}]}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrubbing PII Data\n",
    "\n",
    "After extracting the PII data, the scrub_data method is used to sanitize the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scrubbed Document:\")\n",
    "#> Scrubbed Document:\n",
    "print(pii_data.scrub_data(EXAMPLE_DOCUMENT))\n",
    "\"\"\"\n",
    "# Fake Document with PII for Testing PII Scrubbing Model\n",
    "# He was born on <date_0>. His social security number is <ssn_1>. He has been using the email address <email_2> for years, and he can always be reached at <phone_3>.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Tables using Watsonx.ai\n",
    "\n",
    "This post demonstrates how to use Python's type annotations and Watsonx new vision model to extract tables from images and convert them into markdown format. This method is particularly useful for data analysis and automation tasks.\n",
    "\n",
    "The full code is available on GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Custom Type for Markdown Tables\n",
    "\n",
    "First, we define a custom type, MarkdownDataFrame, to handle pandas DataFrames formatted in markdown. This type uses Python's Annotated and InstanceOf types, along with decorators BeforeValidator and PlainSerializer, to process and serialize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from typing import Annotated, Any\n",
    "from pydantic import BeforeValidator, PlainSerializer, InstanceOf, WithJsonSchema\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def md_to_df(data: Any) -> Any:\n",
    "    # Convert markdown to DataFrame\n",
    "    if isinstance(data, str):\n",
    "        return (\n",
    "            pd.read_csv(\n",
    "                StringIO(data),  # Process data\n",
    "                sep=\"|\",\n",
    "                index_col=1,\n",
    "            )\n",
    "            .dropna(axis=1, how=\"all\")\n",
    "            .iloc[1:]\n",
    "            .applymap(lambda x: x.strip())\n",
    "        )\n",
    "    return data\n",
    "\n",
    "\n",
    "MarkdownDataFrame = Annotated[\n",
    "    InstanceOf[pd.DataFrame],\n",
    "    BeforeValidator(md_to_df),\n",
    "    PlainSerializer(lambda df: df.to_markdown()),\n",
    "    WithJsonSchema(\n",
    "        {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The markdown representation of the table, each one should be tidy, do not try to join tables that should be seperate\",\n",
    "        }\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Table Class¶\n",
    "\n",
    "The Table class is essential for organizing the extracted data. It includes a caption and a dataframe, processed as a markdown table. Since most of the complexity is handled by the MarkdownDataFrame type, the Table class is straightforward!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Table(BaseModel):\n",
    "    caption: str\n",
    "    dataframe: MarkdownDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Tables from Images\n",
    "\n",
    "The extract_table function uses Watsonx vision model to process an image URL and extract tables in markdown format. We utilize the instructor library to patch the OpenAI client for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "import base64\n",
    "import requests\n",
    "from collections.abc import Iterable\n",
    "from ibm_watsonx_ai.foundation_models.schema import TextChatParameters\n",
    "\n",
    "# Apply the patch to the client to support response_model\n",
    "# Also use MD_JSON mode since the vision model does not support any special structured output mode\n",
    "\n",
    "params = TextChatParameters(\n",
    "    temperature=1,\n",
    "    max_tokens=2000,\n",
    "    time_limit=600000\n",
    ")\n",
    "\n",
    "client = Model(\n",
    "    model_id='meta-llama/llama-3-2-90b-vision-instruct',\n",
    "    params=params,\n",
    "    credentials=Credentials(\n",
    "        api_key = os.environ.get(\"WATSONX_API_KEY\"),\n",
    "        url = os.environ.get(\"WATSONX_URL\")),\n",
    "    project_id= os.environ.get(\"WATSONX_PROJECT_ID\")\n",
    ")\n",
    "\n",
    "# Patch the client with Watsonx\n",
    "instructor_client=instructor.from_watsonx(\n",
    "    client=client,\n",
    "    mode=instructor.Mode.WATSONX_MD_JSON\n",
    ")\n",
    "\n",
    "\n",
    "def extract_table(url: str) -> Iterable[Table]:\n",
    "    url = url\n",
    "    response = requests.get(url)\n",
    "    encoded_string = base64.b64encode(response.content).decode('utf-8')\n",
    "\n",
    "    return instructor_client.messages.create(\n",
    "        response_model=Iterable[Table],\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Extract table from image.\"},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                        \"url\": \"data:image/jpeg;base64,\" + encoded_string,\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Example\n",
    "\n",
    "In this example, we apply the method to extract data from an image showing the top grossing apps in Ireland for October 2023.\n",
    "\n",
    "<img src=\"https://a.storyblok.com/f/47007/2400x2000/bf383abc3c/231031_uk-ireland-in-three-charts_table_v01_b.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://a.storyblok.com/f/47007/2400x2000/bf383abc3c/231031_uk-ireland-in-three-charts_table_v01_b.png\"\n",
    "# Set logging to DEBUG\n",
    "#logging.basicConfig(level=logging.DEBUG)\n",
    "tables = extract_table(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in tables:\n",
    "\n",
    "    print(table.dataframe)\n",
    "    \"\"\"\n",
    "           Android                                      ... Category\n",
    "     Rank                                               ...\n",
    "    1                                       Google One  ...      Social networking\n",
    "    2                                          Disney+  ...          Entertainment\n",
    "    3                    TikTok - Videos, Music & LIVE  ...          Entertainment\n",
    "    4                                 Candy Crush Saga  ...          Entertainment\n",
    "    5                   Tinder: Dating, Chat & Friends  ...                  Games\n",
    "    6                                      Coin Master  ...          Entertainment\n",
    "    7                                           Roblox  ...                 Dating\n",
    "    8                   Bumble - Dating & Make Friends  ...                  Games\n",
    "    9                                      Royal Match  ...               Business\n",
    "    10                     Spotify: Music and Podcasts  ...              Education\n",
    "\n",
    "    [10 rows x 4 columns]\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
