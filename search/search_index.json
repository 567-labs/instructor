{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Instructor, Generating Structured Outputs with LLMs","text":"<p>Structured outputs powered by llms. Designed for simplicity, transparency, and control.</p> <p> </p> <p>Instructor makes it easy to get structured data like JSON from LLMs like GPT-3.5, GPT-4, GPT-4-Vision, and open-source models including Mistral/Mixtral, Anyscale, Ollama, and llama-cpp-python.</p> <p>It stands out for its simplicity, transparency, and user-centric design, built on top of Pydantic. Instructor helps you manage validation context, retries with Tenacity, and streaming Lists and Partial responses.</p> <p> Star the Repo  Cookbooks  Prompting Guide</p>"},{"location":"#why-use-instructor","title":"Why use Instructor?","text":"<ul> <li> <p> Simple API with Full Prompt Control</p> <p>Instructor provides a straightforward API that gives you complete ownership and control over your prompts. This allows for fine-tuned customization and optimization of your LLM interactions.</p> <p> Explore Concepts</p> </li> <li> <p> Multi-Language Support</p> <p>Simplify structured data extraction from LLMs with type hints and validation.</p> <p> Python \u00b7  TypeScript \u00b7  Ruby \u00b7  Go \u00b7  Elixir \u00b7  Rust</p> </li> <li> <p> Reasking and Validation</p> <p>Automatically reask the model when validation fails, ensuring high-quality outputs. Leverage Pydantic's validation for robust error handling.</p> <p> Learn about Reasking</p> </li> <li> <p> Streaming Support</p> <p>Stream partial results and iterables with ease, allowing for real-time processing and improved responsiveness in your applications.</p> <p> Learn about Streaming</p> </li> <li> <p> Powered by Type Hints</p> <p>Leverage Pydantic for schema validation, prompting control, less code, and IDE integration. </p> <p> Learn more</p> </li> <li> <p> Simplified LLM Interactions</p> <p>Support for OpenAI, Anthropic, Google, Vertex AI, Mistral/Mixtral, Anyscale, Ollama, llama-cpp-python, Cohere, LiteLLM. </p> <p> See Hub</p> </li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<pre><code>pip install -U instructor\n</code></pre> <p>If you ever get stuck, you can always run <code>instructor docs</code> to open the documentation in your browser. It even supports searching for specific topics.</p> <pre><code>instructor docs [QUERY]\n</code></pre> <p>You can also check out our cookbooks and concepts to learn more about how to use Instructor.</p> Make sure you've installed the dependencies for your specific client <p>To keep the bundle size small, <code>instructor</code> only ships with the OpenAI client. Before using the other clients and their respective <code>from_xx</code> method, make sure you've installed the dependencies following the instructions below.</p> <ol> <li>Anthropic : <code>pip install \"instructor[anthropic]\"</code></li> <li>Google Generative AI: <code>pip install \"instructor[google-generativeai]\"</code></li> <li>Vertex AI: <code>pip install \"instructor[vertexai]\"</code></li> <li>Cohere: <code>pip install \"instructor[cohere]\"</code></li> <li>Litellm: <code>pip install \"instructor[litellm]\"</code></li> <li>Mistral: <code>pip install \"instructor[mistralai]\"</code></li> </ol> <p>Now, let's see Instructor in action with a simple example:</p>"},{"location":"#using-openai","title":"Using OpenAI","text":"Want to use OpenAI's Structured Output Response? <p>We've added support for OpenAI's structured output response. With this, you'll get all the benefits of instructor you like with the constrained sampling from OpenAI.</p> <pre><code>from openai import OpenAI\nfrom instructor import from_openai, Mode\nfrom pydantic import BaseModel\n\nclient = from_openai(OpenAI(), mode=Mode.TOOLS_STRICT)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nresp = client.chat.completions.create(\n    response_model=User,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Jason is 25 years old.\",\n        }\n    ],\n    model=\"gpt-4o\",\n)\n</code></pre> <pre><code>import instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\n\n# Define your desired output structure\nclass UserInfo(BaseModel):\n    name: str\n    age: int\n\n\n# Patch the OpenAI client\nclient = instructor.from_openai(OpenAI())\n\n# Extract structured data from natural language\nuser_info = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserInfo,\n    messages=[{\"role\": \"user\", \"content\": \"John Doe is 30 years old.\"}],\n)\n\nprint(user_info.name)\n#&gt; John Doe\nprint(user_info.age)\n#&gt; 30\n</code></pre>"},{"location":"#using-anthropic","title":"Using Anthropic","text":"<pre><code>import instructor\nfrom anthropic import Anthropic\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclient = instructor.from_anthropic(Anthropic())\n\n# note that client.chat.completions.create will also work\nresp = client.messages.create(\n    model=\"claude-3-opus-20240229\",\n    max_tokens=1024,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Jason is 25 years old.\",\n        }\n    ],\n    response_model=User,\n)\n\nassert isinstance(resp, User)\nassert resp.name == \"Jason\"\nassert resp.age == 25\n</code></pre>"},{"location":"#using-gemini","title":"Using Gemini","text":""},{"location":"#google-ai","title":"Google AI","text":"<pre><code>import instructor\nimport google.generativeai as genai\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclient = instructor.from_gemini(\n    client=genai.GenerativeModel(\n        model_name=\"models/gemini-1.5-flash-latest\",\n    ),\n    mode=instructor.Mode.GEMINI_JSON,\n)\n\n# note that client.chat.completions.create will also work\nresp = client.messages.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Jason is 25 years old.\",\n        }\n    ],\n    response_model=User,\n)\n\nassert isinstance(resp, User)\nassert resp.name == \"Jason\"\nassert resp.age == 25\n</code></pre>"},{"location":"#vertex-ai","title":"Vertex AI","text":"<p>Note: Gemini Tool Calling is still in preview, and there are some limitations. You can learn more about them in the Vertex AI examples notebook.</p> <pre><code>import instructor\nimport vertexai  # type: ignore\nfrom vertexai.generative_models import GenerativeModel  # type: ignore\nfrom pydantic import BaseModel\n\nvertexai.init()\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclient = instructor.from_vertexai(\n    client=GenerativeModel(\"gemini-1.5-pro-preview-0409\"),\n    mode=instructor.Mode.VERTEXAI_TOOLS,\n)\n\n# note that client.chat.completions.create will also work\nresp = client.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Jason is 25 years old.\",\n        }\n    ],\n    response_model=User,\n)\n\nassert isinstance(resp, User)\nassert resp.name == \"Jason\"\nassert resp.age == 25\n</code></pre> Want to use Gemini's multi-part formats? <p>Instructor supports both the gemini and the vertexai libraries. We've most recently added support for multi-part file formats using google's <code>gm.Part</code> objects. This allows you to pass in additional information to the LLM about the data you'd like to see.</p> <p>Here are two examples of how to use multi-part formats with Instructor.</p> <p>We can combine multiple <code>gm.Part</code> objects into a single list and combine them into a single message to be sent to the LLM. Under the hood, we'll convert them into the appropriate format for Gemini.</p> <pre><code>import instructor\nimport vertexai.generative_models as gm  # type: ignore\nfrom pydantic import BaseModel, Field\nimport requests\n\nclient = instructor.from_vertexai(gm.GenerativeModel(\"gemini-1.5-pro-001\"))\ncontent = [\n    \"Order Details:\",\n    gm.Part.from_text(\"Customer: Alice\"),\n    gm.Part.from_text(\"Items:\"),\n    \"Name: Laptop, Price: 999.99\",\n    \"Name: Mouse, Price: 29.99\",\n]\n\n\nclass Item(BaseModel):\n    name: str\n    price: float\n\n\nclass Order(BaseModel):\n    items: list[Item] = Field(..., default_factory=list)\n    customer: str\n\n\nresp = client.create(\n    response_model=Order,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": content,\n        },\n    ],\n)\n\nprint(resp)\n#&gt; items=[Item(name='Laptop', price=999.99), Item(name='Mouse', price=29.99)] customer='Alice'\n</code></pre> <p>This is also the same for multi-modal responses when we want to work with images. In this example, we'll ask the LLM to describe an image and pass in the image as a <code>gm.Part</code> object.</p> <pre><code>import instructor\nimport vertexai.generative_models as gm  # type: ignore\nfrom pydantic import BaseModel\nimport requests\n\nclient = instructor.from_vertexai(\n    gm.GenerativeModel(\"gemini-1.5-pro-001\"), mode=instructor.Mode.VERTEXAI_JSON\n)\ncontent = [\n    gm.Part.from_text(\"Count the number of objects in the image.\"),\n    gm.Part.from_data(\n        bytes(\n            requests.get(\n                \"https://img.taste.com.au/Oq97xT-Q/taste/2016/11/blueberry-scones-75492-1.jpeg\"\n            ).content\n        ),\n        \"image/jpeg\",\n    ),\n]\n\n\nclass Description(BaseModel):\n    description: str\n\n\nresp = client.create(\n    response_model=Description,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": content,\n        },\n    ],\n)\n\nprint(resp)\n#&gt; description='Seven blueberry scones sit inside a metal pie plate.'\n</code></pre>"},{"location":"#using-litellm","title":"Using Litellm","text":"<pre><code>import instructor\nfrom litellm import completion\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclient = instructor.from_litellm(completion)\n\nresp = client.chat.completions.create(\n    model=\"claude-3-opus-20240229\",\n    max_tokens=1024,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Jason is 25 years old.\",\n        }\n    ],\n    response_model=User,\n)\n\nassert isinstance(resp, User)\nassert resp.name == \"Jason\"\nassert resp.age == 25\n</code></pre>"},{"location":"#using-cohere","title":"Using Cohere","text":"<p>We also support users who want to use the Cohere models using the <code>from_cohere</code> method.</p> Want to get the original Cohere response? <p>If you want to get the original response object from the LLM instead of a structured output, you can pass <code>response_model=None</code> to the <code>create</code> method. This will return the raw response from the underlying API.</p> <pre><code># This will return the original Cohere response object\nraw_response = client.chat.completions.create(\n    response_model=None,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Jason is 25 years old.\",\n        }\n    ],\n)\n</code></pre> <p>This can be useful when you need access to additional metadata or want to handle the raw response yourself.</p> <pre><code>import instructor\nfrom pydantic import BaseModel\nfrom cohere import Client\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclient = instructor.from_cohere(Client())\n\nresp = client.chat.completions.create(\n    response_model=User,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Jason is 25 years old.\",\n        }\n    ],\n)\n\nassert resp.name == \"Jason\"\nassert resp.age == 25\n</code></pre>"},{"location":"#correct-typing","title":"Correct Typing","text":"<p>This was the dream of instructor but due to the patching of openai, it wasnt possible for me to get typing to work well. Now, with the new client, we can get typing to work well! We've also added a few <code>create_*</code> methods to make it easier to create iterables and partials, and to access the original completion.</p>"},{"location":"#calling-create","title":"Calling <code>create</code>","text":"<pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\nuser = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Create a user\"},\n    ],\n    response_model=User,\n)\n</code></pre> <p>Now if you use a IDE, you can see the type is correctly infered.</p> <p></p>"},{"location":"#handling-async-await-create","title":"Handling async: <code>await create</code>","text":"<p>This will also work correctly with asynchronous clients.</p> <pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclient = instructor.from_openai(openai.AsyncOpenAI())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nasync def extract():\n    return await client.chat.completions.create(\n        model=\"gpt-4-turbo-preview\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"Create a user\"},\n        ],\n        response_model=User,\n    )\n</code></pre> <p>Notice that simply because we return the <code>create</code> method, the <code>extract()</code> function will return the correct user type.</p> <p></p>"},{"location":"#returning-the-original-completion-create_with_completion","title":"Returning the original completion: <code>create_with_completion</code>","text":"<p>You can also return the original completion object</p> <pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nuser, completion = client.chat.completions.create_with_completion(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Create a user\"},\n    ],\n    response_model=User,\n)\n</code></pre> <p></p>"},{"location":"#streaming-partial-objects-create_partial","title":"Streaming Partial Objects: <code>create_partial</code>","text":"<p>In order to handle streams, we still support <code>Iterable[T]</code> and <code>Partial[T]</code> but to simply the type inference, we've added <code>create_iterable</code> and <code>create_partial</code> methods as well!</p> <pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nuser_stream = client.chat.completions.create_partial(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Create a user\"},\n    ],\n    response_model=User,\n)\n\nfor user in user_stream:\n    print(user)\n    #&gt; name=None age=None\n    #&gt; name=None age=None\n    #&gt; name=None age=None\n    #&gt; name=None age=None\n    #&gt; name=None age=25\n    #&gt; name=None age=25\n    #&gt; name=None age=25\n    #&gt; name=None age=25\n    #&gt; name=None age=25\n    #&gt; name=None age=25\n    #&gt; name='John Doe' age=25\n    # name=None age=None\n    # name='' age=None\n    # name='John' age=None\n    # name='John Doe' age=None\n    # name='John Doe' age=30\n</code></pre> <p>Notice now that the type infered is <code>Generator[User, None]</code></p> <p></p>"},{"location":"#streaming-iterables-create_iterable","title":"Streaming Iterables: <code>create_iterable</code>","text":"<p>We get an iterable of objects when we want to extract multiple objects.</p> <pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nusers = client.chat.completions.create_iterable(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Create 2 users\"},\n    ],\n    response_model=User,\n)\n\nfor user in users:\n    print(user)\n    #&gt; name='John Doe' age=30\n    #&gt; name='Jane Doe' age=28\n    # User(name='John Doe', age=30)\n    # User(name='Jane Smith', age=25)\n</code></pre> <p></p>"},{"location":"#validation","title":"Validation","text":"<p>You can also use Pydantic to validate your outputs and get the llm to retry on failure. Check out our docs on retrying and validation context.</p>"},{"location":"#more-examples","title":"More Examples","text":"<p>If you'd like to see more check out our cookbook.</p> <p>Installing Instructor is a breeze. Just run <code>pip install instructor</code>.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>If you want to help out, checkout some of the issues marked as <code>good-first-issue</code> or <code>help-wanted</code>. Found here. They could be anything from code improvements, a guest blog post, or a new cook book.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the MIT License.</p>"},{"location":"api/","title":"API Reference","text":"Source code in <code>instructor/client.py</code> <pre><code>def from_openai(\n    client: openai.OpenAI | openai.AsyncOpenAI,\n    mode: instructor.Mode = instructor.Mode.TOOLS,\n    **kwargs: Any,\n) -&gt; Instructor | AsyncInstructor:\n    if hasattr(client, \"base_url\"):\n        provider = get_provider(str(client.base_url))\n    else:\n        provider = Provider.OPENAI\n\n    if not isinstance(client, (openai.OpenAI, openai.AsyncOpenAI)):\n        import warnings\n\n        warnings.warn(\n            \"Client should be an instance of openai.OpenAI or openai.AsyncOpenAI. Unexpected behavior may occur with other client types.\",\n            stacklevel=2,\n        )\n\n    if provider in {Provider.ANYSCALE, Provider.TOGETHER}:\n        assert mode in {\n            instructor.Mode.TOOLS,\n            instructor.Mode.JSON,\n            instructor.Mode.JSON_SCHEMA,\n            instructor.Mode.MD_JSON,\n        }\n\n    if provider in {Provider.DATABRICKS}:\n        assert mode in {\n            instructor.Mode.MD_JSON\n        }, \"Databricks provider only supports `MD_JSON` mode.\"\n\n    if provider in {Provider.OPENAI}:\n        assert mode in {\n            instructor.Mode.TOOLS,\n            instructor.Mode.JSON,\n            instructor.Mode.FUNCTIONS,\n            instructor.Mode.PARALLEL_TOOLS,\n            instructor.Mode.MD_JSON,\n            instructor.Mode.TOOLS_STRICT,\n            instructor.Mode.JSON_O1,\n        }\n\n    if isinstance(client, openai.OpenAI):\n        return Instructor(\n            client=client,\n            create=instructor.patch(create=client.chat.completions.create, mode=mode),\n            mode=mode,\n            provider=provider,\n            **kwargs,\n        )\n\n    if isinstance(client, openai.AsyncOpenAI):\n        return AsyncInstructor(\n            client=client,\n            create=instructor.patch(create=client.chat.completions.create, mode=mode),\n            mode=mode,\n            provider=provider,\n            **kwargs,\n        )\n</code></pre>"},{"location":"api/#instructor.dsl.validators.Validator","title":"<code>Validator</code>","text":"<p>               Bases: <code>OpenAISchema</code></p> <p>Validate if an attribute is correct and if not, return a new value with an error message</p> Source code in <code>instructor/dsl/validators.py</code> <pre><code>class Validator(OpenAISchema):\n    \"\"\"\n    Validate if an attribute is correct and if not,\n    return a new value with an error message\n    \"\"\"\n\n    is_valid: bool = Field(\n        default=True,\n        description=\"Whether the attribute is valid based on the requirements\",\n    )\n    reason: Optional[str] = Field(\n        default=None,\n        description=\"The error message if the attribute is not valid, otherwise None\",\n    )\n    fixed_value: Optional[str] = Field(\n        default=None,\n        description=\"If the attribute is not valid, suggest a new value for the attribute\",\n    )\n</code></pre>"},{"location":"api/#instructor.dsl.validators.llm_validator","title":"<code>llm_validator(statement, client, allow_override=False, model='gpt-3.5-turbo', temperature=0)</code>","text":"<p>Create a validator that uses the LLM to validate an attribute</p>"},{"location":"api/#instructor.dsl.validators.llm_validator--usage","title":"Usage","text":"<pre><code>from instructor import llm_validator\nfrom pydantic import BaseModel, Field, field_validator\n\nclass User(BaseModel):\n    name: str = Annotated[str, llm_validator(\"The name must be a full name all lowercase\")\n    age: int = Field(description=\"The age of the person\")\n\ntry:\n    user = User(name=\"Jason Liu\", age=20)\nexcept ValidationError as e:\n    print(e)\n</code></pre> <pre><code>1 validation error for User\nname\n    The name is valid but not all lowercase (type=value_error.llm_validator)\n</code></pre> <p>Note that there, the error message is written by the LLM, and the error type is <code>value_error.llm_validator</code>.</p> <p>Parameters:</p> Name Type Description Default <code>statement</code> <code>str</code> <p>The statement to validate</p> required <code>model</code> <code>str</code> <p>The LLM to use for validation (default: \"gpt-3.5-turbo-0613\")</p> <code>'gpt-3.5-turbo'</code> <code>temperature</code> <code>float</code> <p>The temperature to use for the LLM (default: 0)</p> <code>0</code> <code>openai_client</code> <code>OpenAI</code> <p>The OpenAI client to use (default: None)</p> required Source code in <code>instructor/dsl/validators.py</code> <pre><code>def llm_validator(\n    statement: str,\n    client: Instructor,\n    allow_override: bool = False,\n    model: str = \"gpt-3.5-turbo\",\n    temperature: float = 0,\n) -&gt; Callable[[str], str]:\n    \"\"\"\n    Create a validator that uses the LLM to validate an attribute\n\n    ## Usage\n\n    ```python\n    from instructor import llm_validator\n    from pydantic import BaseModel, Field, field_validator\n\n    class User(BaseModel):\n        name: str = Annotated[str, llm_validator(\"The name must be a full name all lowercase\")\n        age: int = Field(description=\"The age of the person\")\n\n    try:\n        user = User(name=\"Jason Liu\", age=20)\n    except ValidationError as e:\n        print(e)\n    ```\n\n    ```\n    1 validation error for User\n    name\n        The name is valid but not all lowercase (type=value_error.llm_validator)\n    ```\n\n    Note that there, the error message is written by the LLM, and the error type is `value_error.llm_validator`.\n\n    Parameters:\n        statement (str): The statement to validate\n        model (str): The LLM to use for validation (default: \"gpt-3.5-turbo-0613\")\n        temperature (float): The temperature to use for the LLM (default: 0)\n        openai_client (OpenAI): The OpenAI client to use (default: None)\n    \"\"\"\n\n    def llm(v: str) -&gt; str:\n        resp = client.chat.completions.create(\n            response_model=Validator,\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a world class validation model. Capable to determine if the following value is valid for the statement, if it is not, explain why and suggest a new value.\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Does `{v}` follow the rules: {statement}\",\n                },\n            ],\n            model=model,\n            temperature=temperature,\n        )\n\n        # If the response is  not valid, return the reason, this could be used in\n        # the future to generate a better response, via reasking mechanism.\n        assert resp.is_valid, resp.reason\n\n        if allow_override and not resp.is_valid and resp.fixed_value is not None:\n            # If the value is not valid, but we allow override, return the fixed value\n            return resp.fixed_value\n        return v\n\n    return llm\n</code></pre>"},{"location":"api/#instructor.dsl.validators.openai_moderation","title":"<code>openai_moderation(client)</code>","text":"<p>Validates a message using OpenAI moderation model.</p> <p>Should only be used for monitoring inputs and outputs of OpenAI APIs Other use cases are disallowed as per: https://platform.openai.com/docs/guides/moderation/overview</p> <p>Example: <pre><code>from instructor import OpenAIModeration\n\nclass Response(BaseModel):\n    message: Annotated[str, AfterValidator(OpenAIModeration(openai_client=client))]\n\nResponse(message=\"I hate you\")\n</code></pre></p> <pre><code> ValidationError: 1 validation error for Response\n message\nValue error, `I hate you.` was flagged for ['harassment'] [type=value_error, input_value='I hate you.', input_type=str]\n</code></pre> <p>client (OpenAI): The OpenAI client to use, must be sync (default: None)</p> Source code in <code>instructor/dsl/validators.py</code> <pre><code>def openai_moderation(client: OpenAI) -&gt; Callable[[str], str]:\n    \"\"\"\n    Validates a message using OpenAI moderation model.\n\n    Should only be used for monitoring inputs and outputs of OpenAI APIs\n    Other use cases are disallowed as per:\n    https://platform.openai.com/docs/guides/moderation/overview\n\n    Example:\n    ```python\n    from instructor import OpenAIModeration\n\n    class Response(BaseModel):\n        message: Annotated[str, AfterValidator(OpenAIModeration(openai_client=client))]\n\n    Response(message=\"I hate you\")\n    ```\n\n    ```\n     ValidationError: 1 validation error for Response\n     message\n    Value error, `I hate you.` was flagged for ['harassment'] [type=value_error, input_value='I hate you.', input_type=str]\n    ```\n\n    client (OpenAI): The OpenAI client to use, must be sync (default: None)\n    \"\"\"\n\n    def validate_message_with_openai_mod(v: str) -&gt; str:\n        response = client.moderations.create(input=v)\n        out = response.results[0]\n        cats = out.categories.model_dump()\n        if out.flagged:\n            raise ValueError(\n                f\"`{v}` was flagged for {', '.join(cat for cat in cats if cats[cat])}\"\n            )\n\n        return v\n\n    return validate_message_with_openai_mod\n</code></pre>"},{"location":"api/#instructor.dsl.iterable.IterableModel","title":"<code>IterableModel(subtask_class, name=None, description=None)</code>","text":"<p>Dynamically create a IterableModel OpenAISchema that can be used to segment multiple tasks given a base class. This creates class that can be used to create a toolkit for a specific task, names and descriptions are automatically generated. However they can be overridden.</p>"},{"location":"api/#instructor.dsl.iterable.IterableModel--usage","title":"Usage","text":"<pre><code>from pydantic import BaseModel, Field\nfrom instructor import IterableModel\n\nclass User(BaseModel):\n    name: str = Field(description=\"The name of the person\")\n    age: int = Field(description=\"The age of the person\")\n    role: str = Field(description=\"The role of the person\")\n\nMultiUser = IterableModel(User)\n</code></pre>"},{"location":"api/#instructor.dsl.iterable.IterableModel--result","title":"Result","text":"<pre><code>class MultiUser(OpenAISchema, MultiTaskBase):\n    tasks: List[User] = Field(\n        default_factory=list,\n        repr=False,\n        description=\"Correctly segmented list of `User` tasks\",\n    )\n\n    @classmethod\n    def from_streaming_response(cls, completion) -&gt; Generator[User]:\n        '''\n        Parse the streaming response from OpenAI and yield a `User` object\n        for each task in the response\n        '''\n        json_chunks = cls.extract_json(completion)\n        yield from cls.tasks_from_chunks(json_chunks)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>subtask_class</code> <code>Type[OpenAISchema]</code> <p>The base class to use for the MultiTask</p> required <code>name</code> <code>Optional[str]</code> <p>The name of the MultiTask class, if None then the name of the subtask class is used as <code>Multi{subtask_class.__name__}</code></p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>The description of the MultiTask class, if None then the description is set to <code>Correct segmentation of</code>{subtask_class.name}<code>tasks</code></p> <code>None</code> <p>Returns:</p> Name Type Description <code>schema</code> <code>OpenAISchema</code> <p>A new class that can be used to segment multiple tasks</p> Source code in <code>instructor/dsl/iterable.py</code> <pre><code>def IterableModel(\n    subtask_class: type[BaseModel],\n    name: Optional[str] = None,\n    description: Optional[str] = None,\n) -&gt; type[BaseModel]:\n    \"\"\"\n    Dynamically create a IterableModel OpenAISchema that can be used to segment multiple\n    tasks given a base class. This creates class that can be used to create a toolkit\n    for a specific task, names and descriptions are automatically generated. However\n    they can be overridden.\n\n    ## Usage\n\n    ```python\n    from pydantic import BaseModel, Field\n    from instructor import IterableModel\n\n    class User(BaseModel):\n        name: str = Field(description=\"The name of the person\")\n        age: int = Field(description=\"The age of the person\")\n        role: str = Field(description=\"The role of the person\")\n\n    MultiUser = IterableModel(User)\n    ```\n\n    ## Result\n\n    ```python\n    class MultiUser(OpenAISchema, MultiTaskBase):\n        tasks: List[User] = Field(\n            default_factory=list,\n            repr=False,\n            description=\"Correctly segmented list of `User` tasks\",\n        )\n\n        @classmethod\n        def from_streaming_response(cls, completion) -&gt; Generator[User]:\n            '''\n            Parse the streaming response from OpenAI and yield a `User` object\n            for each task in the response\n            '''\n            json_chunks = cls.extract_json(completion)\n            yield from cls.tasks_from_chunks(json_chunks)\n    ```\n\n    Parameters:\n        subtask_class (Type[OpenAISchema]): The base class to use for the MultiTask\n        name (Optional[str]): The name of the MultiTask class, if None then the name\n            of the subtask class is used as `Multi{subtask_class.__name__}`\n        description (Optional[str]): The description of the MultiTask class, if None\n            then the description is set to `Correct segmentation of `{subtask_class.__name__}` tasks`\n\n    Returns:\n        schema (OpenAISchema): A new class that can be used to segment multiple tasks\n    \"\"\"\n    task_name = subtask_class.__name__ if name is None else name\n\n    name = f\"Iterable{task_name}\"\n\n    list_tasks = (\n        list[subtask_class],\n        Field(\n            default_factory=list,\n            repr=False,\n            description=f\"Correctly segmented list of `{task_name}` tasks\",\n        ),\n    )\n\n    base_models = cast(tuple[type[BaseModel], ...], (OpenAISchema, IterableBase))\n    new_cls = create_model(\n        name,\n        tasks=list_tasks,\n        __base__=base_models,\n    )\n    new_cls = cast(type[IterableBase], new_cls)\n\n    # set the class constructor BaseModel\n    new_cls.task_type = subtask_class\n\n    new_cls.__doc__ = (\n        f\"Correct segmentation of `{task_name}` tasks\"\n        if description is None\n        else description\n    )\n    assert issubclass(\n        new_cls, OpenAISchema\n    ), \"The new class should be a subclass of OpenAISchema\"\n    return new_cls\n</code></pre>"},{"location":"api/#instructor.dsl.partial.Partial","title":"<code>Partial</code>","text":"<p>               Bases: <code>Generic[T_Model]</code></p> <p>Generate a new class which has PartialBase as a base class.</p> Notes <p>This will enable partial validation of the model while streaming.</p> Example <p>Partial[SomeModel]</p> Source code in <code>instructor/dsl/partial.py</code> <pre><code>class Partial(Generic[T_Model]):\n    \"\"\"Generate a new class which has PartialBase as a base class.\n\n    Notes:\n        This will enable partial validation of the model while streaming.\n\n    Example:\n        Partial[SomeModel]\n    \"\"\"\n\n    def __new__(\n        cls,\n        *args: object,  # noqa :ARG003\n        **kwargs: object,  # noqa :ARG003\n    ) -&gt; Partial[T_Model]:\n        \"\"\"Cannot instantiate.\n\n        Raises:\n            TypeError: Direct instantiation not allowed.\n        \"\"\"\n        raise TypeError(\"Cannot instantiate abstract Partial class.\")\n\n    def __init_subclass__(\n        cls,\n        *args: object,\n        **kwargs: object,\n    ) -&gt; NoReturn:\n        \"\"\"Cannot subclass.\n\n        Raises:\n           TypeError: Subclassing not allowed.\n        \"\"\"\n        raise TypeError(f\"Cannot subclass {cls.__module__}.Partial\")\n\n    def __class_getitem__(\n        cls,\n        wrapped_class: type[T_Model] | tuple[type[T_Model], type[MakeFieldsOptional]],\n    ) -&gt; type[T_Model]:\n        \"\"\"Convert model to one that inherits from PartialBase.\n\n        We don't make the fields optional at this point, we just wrap them with `Partial` so the names of the nested models will be\n        `Partial{ModelName}`. We want the output of `model_json_schema()` to\n        reflect the name change, but everything else should be the same as the\n        original model. During validation, we'll generate a true partial model\n        to support partially defined fields.\n\n        \"\"\"\n\n        make_fields_optional = None\n        if isinstance(wrapped_class, tuple):\n            wrapped_class, make_fields_optional = wrapped_class\n\n        def _wrap_models(field: FieldInfo) -&gt; tuple[object, FieldInfo]:\n            tmp_field = deepcopy(field)\n\n            annotation = field.annotation\n\n            # Handle generics (like List, Dict, etc.)\n            if get_origin(annotation) is not None:\n                # Get the generic base (like List, Dict) and its arguments (like User in List[User])\n                generic_base = get_origin(annotation)\n                generic_args = get_args(annotation)\n\n                # Recursively apply Partial to each of the generic arguments\n                modified_args = tuple(\n                    (\n                        Partial[arg]\n                        if isinstance(arg, type) and issubclass(arg, BaseModel)\n                        else arg\n                    )\n                    for arg in generic_args\n                )\n\n                # Reconstruct the generic type with modified arguments\n                tmp_field.annotation = (\n                    generic_base[modified_args] if generic_base else None\n                )\n            # If the field is a BaseModel, then recursively convert it's\n            # attributes to optionals.\n            elif isinstance(annotation, type) and issubclass(annotation, BaseModel):\n                tmp_field.annotation = Partial[annotation]\n            return tmp_field.annotation, tmp_field\n\n        model_name = (\n            wrapped_class.__name__\n            if wrapped_class.__name__.startswith(\"Partial\")\n            else f\"Partial{wrapped_class.__name__}\"\n        )\n\n        return create_model(\n            model_name,\n            __base__=(wrapped_class, PartialBase),  # type: ignore\n            __module__=wrapped_class.__module__,\n            **{\n                field_name: (\n                    _make_field_optional(field_info)\n                    if make_fields_optional is not None\n                    else _wrap_models(field_info)\n                )\n                for field_name, field_info in wrapped_class.model_fields.items()\n            },  # type: ignore\n        )\n</code></pre>"},{"location":"api/#instructor.dsl.partial.Partial.__class_getitem__","title":"<code>__class_getitem__(wrapped_class)</code>","text":"<p>Convert model to one that inherits from PartialBase.</p> <p>We don't make the fields optional at this point, we just wrap them with <code>Partial</code> so the names of the nested models will be <code>Partial{ModelName}</code>. We want the output of <code>model_json_schema()</code> to reflect the name change, but everything else should be the same as the original model. During validation, we'll generate a true partial model to support partially defined fields.</p> Source code in <code>instructor/dsl/partial.py</code> <pre><code>def __class_getitem__(\n    cls,\n    wrapped_class: type[T_Model] | tuple[type[T_Model], type[MakeFieldsOptional]],\n) -&gt; type[T_Model]:\n    \"\"\"Convert model to one that inherits from PartialBase.\n\n    We don't make the fields optional at this point, we just wrap them with `Partial` so the names of the nested models will be\n    `Partial{ModelName}`. We want the output of `model_json_schema()` to\n    reflect the name change, but everything else should be the same as the\n    original model. During validation, we'll generate a true partial model\n    to support partially defined fields.\n\n    \"\"\"\n\n    make_fields_optional = None\n    if isinstance(wrapped_class, tuple):\n        wrapped_class, make_fields_optional = wrapped_class\n\n    def _wrap_models(field: FieldInfo) -&gt; tuple[object, FieldInfo]:\n        tmp_field = deepcopy(field)\n\n        annotation = field.annotation\n\n        # Handle generics (like List, Dict, etc.)\n        if get_origin(annotation) is not None:\n            # Get the generic base (like List, Dict) and its arguments (like User in List[User])\n            generic_base = get_origin(annotation)\n            generic_args = get_args(annotation)\n\n            # Recursively apply Partial to each of the generic arguments\n            modified_args = tuple(\n                (\n                    Partial[arg]\n                    if isinstance(arg, type) and issubclass(arg, BaseModel)\n                    else arg\n                )\n                for arg in generic_args\n            )\n\n            # Reconstruct the generic type with modified arguments\n            tmp_field.annotation = (\n                generic_base[modified_args] if generic_base else None\n            )\n        # If the field is a BaseModel, then recursively convert it's\n        # attributes to optionals.\n        elif isinstance(annotation, type) and issubclass(annotation, BaseModel):\n            tmp_field.annotation = Partial[annotation]\n        return tmp_field.annotation, tmp_field\n\n    model_name = (\n        wrapped_class.__name__\n        if wrapped_class.__name__.startswith(\"Partial\")\n        else f\"Partial{wrapped_class.__name__}\"\n    )\n\n    return create_model(\n        model_name,\n        __base__=(wrapped_class, PartialBase),  # type: ignore\n        __module__=wrapped_class.__module__,\n        **{\n            field_name: (\n                _make_field_optional(field_info)\n                if make_fields_optional is not None\n                else _wrap_models(field_info)\n            )\n            for field_name, field_info in wrapped_class.model_fields.items()\n        },  # type: ignore\n    )\n</code></pre>"},{"location":"api/#instructor.dsl.partial.Partial.__init_subclass__","title":"<code>__init_subclass__(*args, **kwargs)</code>","text":"<p>Cannot subclass.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>Subclassing not allowed.</p> Source code in <code>instructor/dsl/partial.py</code> <pre><code>def __init_subclass__(\n    cls,\n    *args: object,\n    **kwargs: object,\n) -&gt; NoReturn:\n    \"\"\"Cannot subclass.\n\n    Raises:\n       TypeError: Subclassing not allowed.\n    \"\"\"\n    raise TypeError(f\"Cannot subclass {cls.__module__}.Partial\")\n</code></pre>"},{"location":"api/#instructor.dsl.partial.Partial.__new__","title":"<code>__new__(*args, **kwargs)</code>","text":"<p>Cannot instantiate.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>Direct instantiation not allowed.</p> Source code in <code>instructor/dsl/partial.py</code> <pre><code>def __new__(\n    cls,\n    *args: object,  # noqa :ARG003\n    **kwargs: object,  # noqa :ARG003\n) -&gt; Partial[T_Model]:\n    \"\"\"Cannot instantiate.\n\n    Raises:\n        TypeError: Direct instantiation not allowed.\n    \"\"\"\n    raise TypeError(\"Cannot instantiate abstract Partial class.\")\n</code></pre>"},{"location":"api/#instructor.dsl.partial.PartialBase","title":"<code>PartialBase</code>","text":"<p>               Bases: <code>Generic[T_Model]</code></p> Source code in <code>instructor/dsl/partial.py</code> <pre><code>class PartialBase(Generic[T_Model]):\n    @classmethod\n    @cache\n    def get_partial_model(cls) -&gt; type[T_Model]:\n        \"\"\"Return a partial model we can use to validate partial results.\"\"\"\n        assert issubclass(\n            cls, BaseModel\n        ), f\"{cls.__name__} must be a subclass of BaseModel\"\n\n        model_name = (\n            cls.__name__\n            if cls.__name__.startswith(\"Partial\")\n            else f\"Partial{cls.__name__}\"\n        )\n\n        return create_model(\n            model_name,\n            __base__=cls,\n            __module__=cls.__module__,\n            **{\n                field_name: _make_field_optional(field_info)\n                for field_name, field_info in cls.model_fields.items()\n            },  # type: ignore[all]\n        )\n\n    @classmethod\n    def from_streaming_response(\n        cls, completion: Iterable[Any], mode: Mode, **kwargs: Any\n    ) -&gt; Generator[T_Model, None, None]:\n        json_chunks = cls.extract_json(completion, mode)\n\n        if mode in {Mode.MD_JSON, Mode.GEMINI_TOOLS}:\n            json_chunks = extract_json_from_stream(json_chunks)\n\n        yield from cls.model_from_chunks(json_chunks, **kwargs)\n\n    @classmethod\n    async def from_streaming_response_async(\n        cls, completion: AsyncGenerator[Any, None], mode: Mode, **kwargs: Any\n    ) -&gt; AsyncGenerator[T_Model, None]:\n        json_chunks = cls.extract_json_async(completion, mode)\n\n        if mode == Mode.MD_JSON:\n            json_chunks = extract_json_from_stream_async(json_chunks)\n\n        return cls.model_from_chunks_async(json_chunks, **kwargs)\n\n    @classmethod\n    def model_from_chunks(\n        cls, json_chunks: Iterable[Any], **kwargs: Any\n    ) -&gt; Generator[T_Model, None, None]:\n        potential_object = \"\"\n        partial_model = cls.get_partial_model()\n        for chunk in json_chunks:\n            potential_object += chunk\n            obj = from_json((potential_object.strip() or \"{}\").encode(), partial_mode=\"on\")\n            obj = partial_model.model_validate(obj, strict=None, **kwargs)\n            yield obj\n\n    @classmethod\n    async def model_from_chunks_async(\n        cls, json_chunks: AsyncGenerator[str, None], **kwargs: Any\n    ) -&gt; AsyncGenerator[T_Model, None]:\n        potential_object = \"\"\n        partial_model = cls.get_partial_model()\n        async for chunk in json_chunks:\n            potential_object += chunk\n            obj = from_json((potential_object.strip() or \"{}\").encode(), partial_mode=\"on\")\n            obj = partial_model.model_validate(obj, strict=None, **kwargs)\n            yield obj\n\n    @staticmethod\n    def extract_json(\n        completion: Iterable[Any], mode: Mode\n    ) -&gt; Generator[str, None, None]:\n        for chunk in completion:\n            try:\n                if mode == Mode.ANTHROPIC_JSON:\n                    if json_chunk := chunk.delta.text:\n                        yield json_chunk\n                if mode == Mode.ANTHROPIC_TOOLS:\n                    yield chunk.delta.partial_json\n                if mode == Mode.GEMINI_JSON:\n                    yield chunk.text\n                if mode == Mode.GEMINI_TOOLS:\n                    # Gemini seems to return the entire function_call and not a chunk?\n                    import json\n\n                    resp = chunk.candidates[0].content.parts[0].function_call\n                    yield json.dumps(type(resp).to_dict(resp)[\"args\"])  # type:ignore\n                elif chunk.choices:\n                    if mode == Mode.FUNCTIONS:\n                        Mode.warn_mode_functions_deprecation()\n                        if json_chunk := chunk.choices[0].delta.function_call.arguments:\n                            yield json_chunk\n                    elif mode in {Mode.JSON, Mode.MD_JSON, Mode.JSON_SCHEMA}:\n                        if json_chunk := chunk.choices[0].delta.content:\n                            yield json_chunk\n                    elif mode in {Mode.TOOLS, Mode.TOOLS_STRICT}:\n                        if json_chunk := chunk.choices[0].delta.tool_calls:\n                            yield json_chunk[0].function.arguments\n                    else:\n                        raise NotImplementedError(\n                            f\"Mode {mode} is not supported for MultiTask streaming\"\n                        )\n            except AttributeError:\n                pass\n\n    @staticmethod\n    async def extract_json_async(\n        completion: AsyncGenerator[Any, None], mode: Mode\n    ) -&gt; AsyncGenerator[str, None]:\n        async for chunk in completion:\n            try:\n                if mode == Mode.ANTHROPIC_JSON:\n                    if json_chunk := chunk.delta.text:\n                        yield json_chunk\n                if mode == Mode.ANTHROPIC_TOOLS:\n                    yield chunk.delta.partial_json\n                elif chunk.choices:\n                    if mode == Mode.FUNCTIONS:\n                        Mode.warn_mode_functions_deprecation()\n                        if json_chunk := chunk.choices[0].delta.function_call.arguments:\n                            yield json_chunk\n                    elif mode in {Mode.JSON, Mode.MD_JSON, Mode.JSON_SCHEMA}:\n                        if json_chunk := chunk.choices[0].delta.content:\n                            yield json_chunk\n                    elif mode in {Mode.TOOLS, Mode.TOOLS_STRICT}:\n                        if json_chunk := chunk.choices[0].delta.tool_calls:\n                            yield json_chunk[0].function.arguments\n                    else:\n                        raise NotImplementedError(\n                            f\"Mode {mode} is not supported for MultiTask streaming\"\n                        )\n            except AttributeError:\n                pass\n</code></pre>"},{"location":"api/#instructor.dsl.partial.PartialBase.get_partial_model","title":"<code>get_partial_model()</code>  <code>cached</code> <code>classmethod</code>","text":"<p>Return a partial model we can use to validate partial results.</p> Source code in <code>instructor/dsl/partial.py</code> <pre><code>@classmethod\n@cache\ndef get_partial_model(cls) -&gt; type[T_Model]:\n    \"\"\"Return a partial model we can use to validate partial results.\"\"\"\n    assert issubclass(\n        cls, BaseModel\n    ), f\"{cls.__name__} must be a subclass of BaseModel\"\n\n    model_name = (\n        cls.__name__\n        if cls.__name__.startswith(\"Partial\")\n        else f\"Partial{cls.__name__}\"\n    )\n\n    return create_model(\n        model_name,\n        __base__=cls,\n        __module__=cls.__module__,\n        **{\n            field_name: _make_field_optional(field_info)\n            for field_name, field_info in cls.model_fields.items()\n        },  # type: ignore[all]\n    )\n</code></pre>"},{"location":"api/#instructor.dsl.maybe.MaybeBase","title":"<code>MaybeBase</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[T]</code></p> <p>Extract a result from a model, if any, otherwise set the error and message fields.</p> Source code in <code>instructor/dsl/maybe.py</code> <pre><code>class MaybeBase(BaseModel, Generic[T]):\n    \"\"\"\n    Extract a result from a model, if any, otherwise set the error and message fields.\n    \"\"\"\n\n    result: Optional[T]\n    error: bool = Field(default=False)\n    message: Optional[str]\n\n    def __bool__(self) -&gt; bool:\n        return self.result is not None\n</code></pre>"},{"location":"api/#instructor.dsl.maybe.Maybe","title":"<code>Maybe(model)</code>","text":"<p>Create a Maybe model for a given Pydantic model. This allows you to return a model that includes fields for <code>result</code>, <code>error</code>, and <code>message</code> for sitatations where the data may not be present in the context.</p>"},{"location":"api/#instructor.dsl.maybe.Maybe--usage","title":"Usage","text":"<pre><code>from pydantic import BaseModel, Field\nfrom instructor import Maybe\n\nclass User(BaseModel):\n    name: str = Field(description=\"The name of the person\")\n    age: int = Field(description=\"The age of the person\")\n    role: str = Field(description=\"The role of the person\")\n\nMaybeUser = Maybe(User)\n</code></pre>"},{"location":"api/#instructor.dsl.maybe.Maybe--result","title":"Result","text":"<pre><code>class MaybeUser(BaseModel):\n    result: Optional[User]\n    error: bool = Field(default=False)\n    message: Optional[str]\n\n    def __bool__(self):\n        return self.result is not None\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Type[BaseModel]</code> <p>The Pydantic model to wrap with Maybe.</p> required <p>Returns:</p> Name Type Description <code>MaybeModel</code> <code>Type[BaseModel]</code> <p>A new Pydantic model that includes fields for <code>result</code>, <code>error</code>, and <code>message</code>.</p> Source code in <code>instructor/dsl/maybe.py</code> <pre><code>def Maybe(model: type[T]) -&gt; type[MaybeBase[T]]:\n    \"\"\"\n    Create a Maybe model for a given Pydantic model. This allows you to return a model that includes fields for `result`, `error`, and `message` for sitatations where the data may not be present in the context.\n\n    ## Usage\n\n    ```python\n    from pydantic import BaseModel, Field\n    from instructor import Maybe\n\n    class User(BaseModel):\n        name: str = Field(description=\"The name of the person\")\n        age: int = Field(description=\"The age of the person\")\n        role: str = Field(description=\"The role of the person\")\n\n    MaybeUser = Maybe(User)\n    ```\n\n    ## Result\n\n    ```python\n    class MaybeUser(BaseModel):\n        result: Optional[User]\n        error: bool = Field(default=False)\n        message: Optional[str]\n\n        def __bool__(self):\n            return self.result is not None\n    ```\n\n    Parameters:\n        model (Type[BaseModel]): The Pydantic model to wrap with Maybe.\n\n    Returns:\n        MaybeModel (Type[BaseModel]): A new Pydantic model that includes fields for `result`, `error`, and `message`.\n    \"\"\"\n    return create_model(\n        f\"Maybe{model.__name__}\",\n        __base__=MaybeBase,\n        result=(\n            Optional[model],\n            Field(\n                default=None,\n                description=\"Correctly extracted result from the model, if any, otherwise None\",\n            ),\n        ),\n        error=(bool, Field(default=False)),\n        message=(\n            Optional[str],\n            Field(\n                default=None,\n                description=\"Error message if no result was found, should be short and concise\",\n            ),\n        ),\n    )\n</code></pre>"},{"location":"api/#instructor.function_calls.OpenAISchema","title":"<code>OpenAISchema</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>instructor/function_calls.py</code> <pre><code>class OpenAISchema(BaseModel):\n    # Ignore classproperty, since Pydantic doesn't understand it like it would a normal property.\n    model_config = ConfigDict(ignored_types=(classproperty,))\n\n    @classproperty\n    def openai_schema(cls) -&gt; dict[str, Any]:\n        \"\"\"\n        Return the schema in the format of OpenAI's schema as jsonschema\n\n        Note:\n            Its important to add a docstring to describe how to best use this class, it will be included in the description attribute and be part of the prompt.\n\n        Returns:\n            model_json_schema (dict): A dictionary in the format of OpenAI's schema as jsonschema\n        \"\"\"\n        schema = cls.model_json_schema()\n        docstring = parse(cls.__doc__ or \"\")\n        parameters = {\n            k: v for k, v in schema.items() if k not in (\"title\", \"description\")\n        }\n        for param in docstring.params:\n            if (name := param.arg_name) in parameters[\"properties\"] and (\n                description := param.description\n            ):\n                if \"description\" not in parameters[\"properties\"][name]:\n                    parameters[\"properties\"][name][\"description\"] = description\n\n        parameters[\"required\"] = sorted(\n            k for k, v in parameters[\"properties\"].items() if \"default\" not in v\n        )\n\n        if \"description\" not in schema:\n            if docstring.short_description:\n                schema[\"description\"] = docstring.short_description\n            else:\n                schema[\"description\"] = (\n                    f\"Correctly extracted `{cls.__name__}` with all \"\n                    f\"the required parameters with correct types\"\n                )\n\n        return {\n            \"name\": schema[\"title\"],\n            \"description\": schema[\"description\"],\n            \"parameters\": parameters,\n        }\n\n    @classproperty\n    def anthropic_schema(cls) -&gt; dict[str, Any]:\n        return {\n            \"name\": cls.openai_schema[\"name\"],\n            \"description\": cls.openai_schema[\"description\"],\n            \"input_schema\": cls.model_json_schema(),\n        }\n\n    @classproperty\n    def gemini_schema(cls) -&gt; Any:\n        import google.generativeai.types as genai_types\n\n        function = genai_types.FunctionDeclaration(\n            name=cls.openai_schema[\"name\"],\n            description=cls.openai_schema[\"description\"],\n            parameters=map_to_gemini_function_schema(cls.openai_schema[\"parameters\"]),\n        )\n        return function\n\n    @classmethod\n    def from_response(\n        cls,\n        completion: ChatCompletion,\n        validation_context: Optional[dict[str, Any]] = None,\n        strict: Optional[bool] = None,\n        mode: Mode = Mode.TOOLS,\n    ) -&gt; BaseModel:\n        \"\"\"Execute the function from the response of an openai chat completion\n\n        Parameters:\n            completion (openai.ChatCompletion): The response from an openai chat completion\n            throw_error (bool): Whether to throw an error if the function call is not detected\n            validation_context (dict): The validation context to use for validating the response\n            strict (bool): Whether to use strict json parsing\n            mode (Mode): The openai completion mode\n\n        Returns:\n            cls (OpenAISchema): An instance of the class\n        \"\"\"\n        if mode == Mode.ANTHROPIC_TOOLS:\n            return cls.parse_anthropic_tools(completion, validation_context, strict)\n\n        if mode == Mode.ANTHROPIC_JSON:\n            return cls.parse_anthropic_json(completion, validation_context, strict)\n\n        if mode in {Mode.VERTEXAI_TOOLS, Mode.GEMINI_TOOLS}:\n            return cls.parse_vertexai_tools(completion, validation_context)\n\n        if mode == Mode.VERTEXAI_JSON:\n            return cls.parse_vertexai_json(completion, validation_context, strict)\n\n        if mode == Mode.COHERE_TOOLS:\n            return cls.parse_cohere_tools(completion, validation_context, strict)\n\n        if mode == Mode.GEMINI_JSON:\n            return cls.parse_gemini_json(completion, validation_context, strict)\n\n        if mode == Mode.GEMINI_TOOLS:\n            return cls.parse_gemini_tools(completion, validation_context, strict)\n\n        if mode == Mode.COHERE_JSON_SCHEMA:\n            return cls.parse_cohere_json_schema(completion, validation_context, strict)\n\n        if completion.choices[0].finish_reason == \"length\":\n            raise IncompleteOutputException(last_completion=completion)\n\n        if mode == Mode.FUNCTIONS:\n            Mode.warn_mode_functions_deprecation()\n            return cls.parse_functions(completion, validation_context, strict)\n\n        if mode in {Mode.TOOLS, Mode.MISTRAL_TOOLS, Mode.TOOLS_STRICT}:\n            return cls.parse_tools(completion, validation_context, strict)\n\n        if mode in {Mode.JSON, Mode.JSON_SCHEMA, Mode.MD_JSON, Mode.JSON_O1}:\n            return cls.parse_json(completion, validation_context, strict)\n\n        raise ValueError(f\"Invalid patch mode: {mode}\")\n\n    @classmethod\n    def parse_cohere_json_schema(\n        cls: type[BaseModel],\n        completion: ChatCompletion,\n        validation_context: Optional[dict[str, Any]] = None,\n        strict: Optional[bool] = None,\n    ):\n        assert hasattr(\n            completion, \"text\"\n        ), \"Completion is not of type NonStreamedChatResponse\"\n        return cls.model_validate_json(\n            completion.text, context=validation_context, strict=strict\n        )\n\n    @classmethod\n    def parse_anthropic_tools(\n        cls: type[BaseModel],\n        completion: ChatCompletion,\n        validation_context: Optional[dict[str, Any]] = None,\n        strict: Optional[bool] = None,\n    ) -&gt; BaseModel:\n        from anthropic.types import Message\n\n        if isinstance(completion, Message) and completion.stop_reason == \"max_tokens\":\n            raise IncompleteOutputException(last_completion=completion)\n\n        # Anthropic returns arguments as a dict, dump to json for model validation below\n        tool_calls = [\n            json.dumps(c.input) for c in completion.content if c.type == \"tool_use\"\n        ]  # TODO update with anthropic specific types\n\n        tool_calls_validator = TypeAdapter(\n            Annotated[list[Any], Field(min_length=1, max_length=1)]\n        )\n        tool_call = tool_calls_validator.validate_python(tool_calls)[0]\n\n        return cls.model_validate_json(\n            tool_call, context=validation_context, strict=strict\n        )\n\n    @classmethod\n    def parse_anthropic_json(\n        cls: type[BaseModel],\n        completion: ChatCompletion,\n        validation_context: Optional[dict[str, Any]] = None,\n        strict: Optional[bool] = None,\n    ) -&gt; BaseModel:\n        from anthropic.types import Message\n\n        assert isinstance(completion, Message)\n\n        if completion.stop_reason == \"max_tokens\":\n            raise IncompleteOutputException(last_completion=completion)\n\n        text = completion.content[0].text\n        extra_text = extract_json_from_codeblock(text)\n\n        if strict:\n            return cls.model_validate_json(\n                extra_text, context=validation_context, strict=True\n            )\n        else:\n            # Allow control characters.\n            parsed = json.loads(extra_text, strict=False)\n            # Pydantic non-strict: https://docs.pydantic.dev/latest/concepts/strict_mode/\n            return cls.model_validate(parsed, context=validation_context, strict=False)\n\n    @classmethod\n    def parse_gemini_json(\n        cls: type[BaseModel],\n        completion: Any,\n        validation_context: Optional[dict[str, Any]] = None,\n        strict: Optional[bool] = None,\n    ) -&gt; BaseModel:\n        try:\n            text = completion.text\n        except ValueError:\n            logger.debug(\n                f\"Error response: {completion.result.candidates[0].finish_reason}\\n\\n{completion.result.candidates[0].safety_ratings}\"\n            )\n\n        try:\n            extra_text = extract_json_from_codeblock(text)  # type: ignore\n        except UnboundLocalError:\n            raise ValueError(\"Unable to extract JSON from completion text\") from None\n\n        if strict:\n            return cls.model_validate_json(\n                extra_text, context=validation_context, strict=True\n            )\n        else:\n            # Allow control characters.\n            parsed = json.loads(extra_text, strict=False)\n            # Pydantic non-strict: https://docs.pydantic.dev/latest/concepts/strict_mode/\n            return cls.model_validate(parsed, context=validation_context, strict=False)\n\n    @classmethod\n    def parse_vertexai_tools(\n        cls: type[BaseModel],\n        completion: ChatCompletion,\n        validation_context: Optional[dict[str, Any]] = None,\n    ) -&gt; BaseModel:\n        tool_call = completion.candidates[0].content.parts[0].function_call.args  # type: ignore\n        model = {}\n        for field in tool_call:  # type: ignore\n            model[field] = tool_call[field]\n        # We enable strict=False because the conversion from protobuf -&gt; dict often results in types like ints being cast to floats, as a result in order for model.validate to work we need to disable strict mode.\n        return cls.model_validate(model, context=validation_context, strict=False)\n\n    @classmethod\n    def parse_vertexai_json(\n        cls: type[BaseModel],\n        completion: ChatCompletion,\n        validation_context: Optional[dict[str, Any]] = None,\n        strict: Optional[bool] = None,\n    ) -&gt; BaseModel:\n        return cls.model_validate_json(\n            completion.text, context=validation_context, strict=strict\n        )\n\n    @classmethod\n    def parse_cohere_tools(\n        cls: type[BaseModel],\n        completion: ChatCompletion,\n        validation_context: Optional[dict[str, Any]] = None,\n        strict: Optional[bool] = None,\n    ) -&gt; BaseModel:\n        text = cast(str, completion.text)  # type: ignore - TODO update with cohere specific types\n        extra_text = extract_json_from_codeblock(text)\n        return cls.model_validate_json(\n            extra_text, context=validation_context, strict=strict\n        )\n\n    @classmethod\n    def parse_functions(\n        cls: type[BaseModel],\n        completion: ChatCompletion,\n        validation_context: Optional[dict[str, Any]] = None,\n        strict: Optional[bool] = None,\n    ) -&gt; BaseModel:\n        message = completion.choices[0].message\n        assert (\n            message.function_call.name == cls.openai_schema[\"name\"]  # type: ignore[index]\n        ), \"Function name does not match\"\n        return cls.model_validate_json(\n            message.function_call.arguments,  # type: ignore[attr-defined]\n            context=validation_context,\n            strict=strict,\n        )\n\n    @classmethod\n    def parse_tools(\n        cls: type[BaseModel],\n        completion: ChatCompletion,\n        validation_context: Optional[dict[str, Any]] = None,\n        strict: Optional[bool] = None,\n    ) -&gt; BaseModel:\n        message = completion.choices[0].message\n        # this field seems to be missing when using instructor with some other tools (e.g. litellm)\n        # trying to fix this by adding a check\n        if hasattr(message, \"refusal\"):\n            assert (\n                message.refusal is None\n            ), f\"Unable to generate a response due to {message.refusal}\"\n        assert (\n            len(message.tool_calls or []) == 1\n        ), \"Instructor does not support multiple tool calls, use List[Model] instead.\"\n        tool_call = message.tool_calls[0]  # type: ignore\n        assert (\n            tool_call.function.name == cls.openai_schema[\"name\"]  # type: ignore[index]\n        ), \"Tool name does not match\"\n        return cls.model_validate_json(\n            tool_call.function.arguments,  # type: ignore\n            context=validation_context,\n            strict=strict,\n        )\n\n    @classmethod\n    def parse_json(\n        cls: type[BaseModel],\n        completion: ChatCompletion,\n        validation_context: Optional[dict[str, Any]] = None,\n        strict: Optional[bool] = None,\n    ) -&gt; BaseModel:\n        message = completion.choices[0].message.content or \"\"\n        message = extract_json_from_codeblock(message)\n\n        return cls.model_validate_json(\n            message,\n            context=validation_context,\n            strict=strict,\n        )\n</code></pre>"},{"location":"api/#instructor.function_calls.OpenAISchema.from_response","title":"<code>from_response(completion, validation_context=None, strict=None, mode=Mode.TOOLS)</code>  <code>classmethod</code>","text":"<p>Execute the function from the response of an openai chat completion</p> <p>Parameters:</p> Name Type Description Default <code>completion</code> <code>ChatCompletion</code> <p>The response from an openai chat completion</p> required <code>throw_error</code> <code>bool</code> <p>Whether to throw an error if the function call is not detected</p> required <code>validation_context</code> <code>dict</code> <p>The validation context to use for validating the response</p> <code>None</code> <code>strict</code> <code>bool</code> <p>Whether to use strict json parsing</p> <code>None</code> <code>mode</code> <code>Mode</code> <p>The openai completion mode</p> <code>TOOLS</code> <p>Returns:</p> Name Type Description <code>cls</code> <code>OpenAISchema</code> <p>An instance of the class</p> Source code in <code>instructor/function_calls.py</code> <pre><code>@classmethod\ndef from_response(\n    cls,\n    completion: ChatCompletion,\n    validation_context: Optional[dict[str, Any]] = None,\n    strict: Optional[bool] = None,\n    mode: Mode = Mode.TOOLS,\n) -&gt; BaseModel:\n    \"\"\"Execute the function from the response of an openai chat completion\n\n    Parameters:\n        completion (openai.ChatCompletion): The response from an openai chat completion\n        throw_error (bool): Whether to throw an error if the function call is not detected\n        validation_context (dict): The validation context to use for validating the response\n        strict (bool): Whether to use strict json parsing\n        mode (Mode): The openai completion mode\n\n    Returns:\n        cls (OpenAISchema): An instance of the class\n    \"\"\"\n    if mode == Mode.ANTHROPIC_TOOLS:\n        return cls.parse_anthropic_tools(completion, validation_context, strict)\n\n    if mode == Mode.ANTHROPIC_JSON:\n        return cls.parse_anthropic_json(completion, validation_context, strict)\n\n    if mode in {Mode.VERTEXAI_TOOLS, Mode.GEMINI_TOOLS}:\n        return cls.parse_vertexai_tools(completion, validation_context)\n\n    if mode == Mode.VERTEXAI_JSON:\n        return cls.parse_vertexai_json(completion, validation_context, strict)\n\n    if mode == Mode.COHERE_TOOLS:\n        return cls.parse_cohere_tools(completion, validation_context, strict)\n\n    if mode == Mode.GEMINI_JSON:\n        return cls.parse_gemini_json(completion, validation_context, strict)\n\n    if mode == Mode.GEMINI_TOOLS:\n        return cls.parse_gemini_tools(completion, validation_context, strict)\n\n    if mode == Mode.COHERE_JSON_SCHEMA:\n        return cls.parse_cohere_json_schema(completion, validation_context, strict)\n\n    if completion.choices[0].finish_reason == \"length\":\n        raise IncompleteOutputException(last_completion=completion)\n\n    if mode == Mode.FUNCTIONS:\n        Mode.warn_mode_functions_deprecation()\n        return cls.parse_functions(completion, validation_context, strict)\n\n    if mode in {Mode.TOOLS, Mode.MISTRAL_TOOLS, Mode.TOOLS_STRICT}:\n        return cls.parse_tools(completion, validation_context, strict)\n\n    if mode in {Mode.JSON, Mode.JSON_SCHEMA, Mode.MD_JSON, Mode.JSON_O1}:\n        return cls.parse_json(completion, validation_context, strict)\n\n    raise ValueError(f\"Invalid patch mode: {mode}\")\n</code></pre>"},{"location":"api/#instructor.function_calls.OpenAISchema.openai_schema","title":"<code>openai_schema()</code>","text":"<p>Return the schema in the format of OpenAI's schema as jsonschema</p> Note <p>Its important to add a docstring to describe how to best use this class, it will be included in the description attribute and be part of the prompt.</p> <p>Returns:</p> Name Type Description <code>model_json_schema</code> <code>dict</code> <p>A dictionary in the format of OpenAI's schema as jsonschema</p> Source code in <code>instructor/function_calls.py</code> <pre><code>@classproperty\ndef openai_schema(cls) -&gt; dict[str, Any]:\n    \"\"\"\n    Return the schema in the format of OpenAI's schema as jsonschema\n\n    Note:\n        Its important to add a docstring to describe how to best use this class, it will be included in the description attribute and be part of the prompt.\n\n    Returns:\n        model_json_schema (dict): A dictionary in the format of OpenAI's schema as jsonschema\n    \"\"\"\n    schema = cls.model_json_schema()\n    docstring = parse(cls.__doc__ or \"\")\n    parameters = {\n        k: v for k, v in schema.items() if k not in (\"title\", \"description\")\n    }\n    for param in docstring.params:\n        if (name := param.arg_name) in parameters[\"properties\"] and (\n            description := param.description\n        ):\n            if \"description\" not in parameters[\"properties\"][name]:\n                parameters[\"properties\"][name][\"description\"] = description\n\n    parameters[\"required\"] = sorted(\n        k for k, v in parameters[\"properties\"].items() if \"default\" not in v\n    )\n\n    if \"description\" not in schema:\n        if docstring.short_description:\n            schema[\"description\"] = docstring.short_description\n        else:\n            schema[\"description\"] = (\n                f\"Correctly extracted `{cls.__name__}` with all \"\n                f\"the required parameters with correct types\"\n            )\n\n    return {\n        \"name\": schema[\"title\"],\n        \"description\": schema[\"description\"],\n        \"parameters\": parameters,\n    }\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>We would love for you to contribute to <code>Instructor</code>.</p>"},{"location":"contributing/#evals","title":"Evals","text":"<p>We invite you to contribute evals in pytest as a way to monitor the quality of the openai models and the instructor library. To get started check out the jxnl/instructor/tests/llm/test_openai/evals and contribute your own evals in the form of pytest tests. These evals will be run once a week and the results will be posted.</p>"},{"location":"contributing/#issues","title":"Issues","text":"<p>If you find a bug, please file an issue on our issue tracker on GitHub.</p> <p>To help us reproduce the bug, please provide a minimal reproducible example, including a code snippet and the full error message.</p> <ol> <li>The <code>response_model</code> you are using.</li> <li>The <code>messages</code> you are using.</li> <li>The <code>model</code> you are using.</li> </ol>"},{"location":"contributing/#pull-requests","title":"Pull Requests","text":"<p>We welcome pull requests! There is plenty to do, and we are happy to discuss any contributions you would like to make.</p> <p>If it is not a small change, please start by filing an issue first.</p> <p>If you need ideas, you can check out the help wanted or good first issue labels.</p> <p>Grit is used to enforce best practices. You can run <code>grit check</code> to check your code before submitting a pull request.</p>"},{"location":"contributing/#contributors","title":"Contributors","text":""},{"location":"contributing/#additional-resources","title":"Additional Resources","text":"<p>To enhance your understanding of the documentation, here are some useful references:</p> <p>Learn more in the mkdocs serve documentation and make sure to install the necessary dependencies listed in <code>requirements-doc.txt</code> before running the command.</p> <ul> <li> <p>mkdocs serve: The <code>mkdocs serve</code> command is used to preview your documentation locally during the development phase. When you run this command in your terminal, MkDocs starts a development server, allowing you to view and interact with your documentation in a web browser. This is helpful for checking how your changes look before publishing the documentation.</p> </li> <li> <p>hl_lines in Code Blocks: The <code>hl_lines</code> feature in code blocks allows you to highlight specific lines within the code block. This is useful for drawing attention to particular lines of code when explaining examples or providing instructions. You can specify the lines to highlight using the <code>hl_lines</code> option in your code block configuration. For more details and examples, you can refer to the hl_lines documentation.</p> </li> <li> <p>Admonitions: Admonitions are a way to visually emphasize or call attention to certain pieces of information in your documentation. They come in various styles, such as notes, warnings, tips, etc. Admonitions provide a structured and consistent way to present important content. For usage examples and details on incorporating admonitions into your documentation, you can refer to the admonitions documentation.</p> </li> </ul> <p>For more details about the documentation structure and features, refer to the MkDocs Material documentation.</p> <p>Thank you for your contributions, and happy coding!</p>"},{"location":"help/","title":"Getting help with Instructor","text":"<p>If you need help getting started with Instructor or with advanced usage, the following sources may be useful.</p>"},{"location":"help/#material-discord-discord","title":":material-discord: Discord","text":"<p>The Discord is a great place to ask questions and get help from the community.</p>"},{"location":"help/#concepts","title":"Concepts","text":"<p>The concepts section explains the core concepts of Instructor and how to prompt with models.</p>"},{"location":"help/#cookbooks","title":"Cookbooks","text":"<p>The cookbooks are a great place to start. They contain a variety of examples that demonstrate how to use Instructor in different scenarios.</p>"},{"location":"help/#blog","title":"Blog","text":"<p>The blog contains articles that explain how to use Instructor in different scenarios.</p>"},{"location":"help/#github-discussions","title":"GitHub Discussions","text":"<p>GitHub discussions are useful for asking questions, your question and the answer will help everyone.</p>"},{"location":"help/#github-issues","title":"GitHub Issues","text":"<p>GitHub issues are useful for reporting bugs or requesting new features.</p>"},{"location":"help/#twitter","title":"Twitter","text":"<p>You can also reach out to me on Twitter if you have any questions or ideas.</p>"},{"location":"installation/","title":"Installation","text":"<p>Installation is as simple as:</p> <pre><code>pip install instructor\n</code></pre> <p>Instructor has a few dependencies:</p> <ul> <li><code>openai</code>: OpenAI's Python client.</li> <li><code>typer</code>: Build great CLIs. Easy to code. Based on Python type hints.</li> <li><code>docstring-parser</code>: A parser for Python docstrings, to improve the experience of working with docstrings in jsonschema.</li> <li><code>pydantic</code>: Data validation and settings management using python type annotations.</li> </ul> <p>If you've got Python 3.9+ and <code>pip</code> installed, you're good to go.</p>"},{"location":"why/","title":"Why use Instructor?","text":"<p>This is a letter from the author Jason Liu of Instructor. I'm a big fan of Pydantic and I think it's the best way to handle data validation in Python. I've been using it for years and I'm excited to bring it to the OpenAI API.</p> Why use Pydantic? <p>Its hard to answer the question of why use Instructor without first answering why use Pydantic.:</p> <ul> <li> <p>Powered by type hints \u2014 with Pydantic, schema validation and serialization are controlled by type annotations; less to learn, less code to write, and integration with your IDE and static analysis tools.</p> </li> <li> <p>Speed \u2014 Pydantic's core validation logic is written in Rust. As a result, Pydantic is among the fastest data validation libraries for Python.</p> </li> <li> <p>JSON Schema \u2014 Pydantic models can emit JSON Schema, allowing for easy integration with other tools. [Learn more\u2026]</p> </li> <li> <p>Customisation \u2014 Pydantic allows custom validators and serializers to alter how data is processed in many powerful ways.</p> </li> <li> <p>Ecosystem \u2014 around 8,000 packages on PyPI use Pydantic, including massively popular libraries like FastAPI, huggingface, Django Ninja, SQLModel, &amp; LangChain.</p> </li> <li> <p>Battle tested \u2014 Pydantic is downloaded over 70M times/month and is used by all FAANG companies and 20 of the 25 largest companies on NASDAQ. If you're trying to do something with Pydantic, someone else has probably already done it.</p> </li> </ul>"},{"location":"why/#no-new-standards","title":"No New standards","text":"<p>Instructor is built on top of Pydantic and OpenAI, which will be familiar to many developers already. But, since many llm providers support the OpenAI API spec, you can use many closed source and open source providers like Anyscale, Together, Groq, Ollama, and Llama-cpp-python.</p> <p>All we do is augment the <code>create</code> such that</p> <pre><code>def create(response_model=Type[T]) -&gt; T:\n</code></pre> <p>Check out how we connect with open source</p>"},{"location":"why/#pydantic-over-raw-schema","title":"Pydantic over Raw Schema","text":"<p>I find many prompt building tools to be overly complex and difficult to use, they might be simple to get started with a trivial examples but once you need more control, you have to wish they were simpler. Instructor does the least amount of work to get the job done.</p> PydanticJson Schema <p>Pydantic is more readable and definitions and reference values are handled automatically. This is a big win for Instructor, as it allows us to focus on the data extraction and not the schema.</p> <pre><code>from typing import List, Literal\nfrom pydantic import BaseModel, Field\n\n\nclass Property(BaseModel):\n    name: str = Field(description=\"name of property in snake case\")\n    value: str\n\nclass Character(BaseModel):\n    \"\"\"\n    Any character in a fictional story\n    \"\"\"\n    name: str\n    age: int\n    properties: List[Property]\n    role: Literal['protagonist', 'antagonist', 'supporting']\n\nclass AllCharacters(BaseModel):\n    characters: List[Character] = Field(description=\"A list of all characters in the story\")\n</code></pre> <p>Would you Ever prefer to code review this? Where everything is a string, ripe for typos and errors in references? I know I wouldn't.</p> <pre><code>var = {\n    \"$defs\": {\n        \"Character\": {\n            \"description\": \"Any character in a fictional story\",\n            \"properties\": {\n                \"name\": {\"title\": \"Name\", \"type\": \"string\"},\n                \"age\": {\"title\": \"Age\", \"type\": \"integer\"},\n                \"properties\": {\n                    \"type\": \"array\",\n                    \"items\": {\"$ref\": \"#/$defs/Property\"},\n                    \"title\": \"Properties\",\n                },\n                \"role\": {\n                    \"enum\": [\"protagonist\", \"antagonist\", \"supporting\"],\n                    \"title\": \"Role\",\n                    \"type\": \"string\",\n                },\n            },\n            \"required\": [\"name\", \"age\", \"properties\", \"role\"],\n            \"title\": \"Character\",\n            \"type\": \"object\",\n        },\n        \"Property\": {\n            \"properties\": {\n                \"name\": {\n                    \"description\": \"name of property in snake case\",\n                    \"title\": \"Name\",\n                    \"type\": \"string\",\n                },\n                \"value\": {\"title\": \"Value\", \"type\": \"string\"},\n            },\n            \"required\": [\"name\", \"value\"],\n            \"title\": \"Property\",\n            \"type\": \"object\",\n        },\n    },\n    \"properties\": {\n        \"characters\": {\n            \"description\": \"A list of all characters in the story\",\n            \"items\": {\"$ref\": \"#/$defs/Character\"},\n            \"title\": \"Characters\",\n            \"type\": \"array\",\n        }\n    },\n    \"required\": [\"characters\"],\n    \"title\": \"AllCharacters\",\n    \"type\": \"object\",\n}\n</code></pre>"},{"location":"why/#easy-to-try-and-install","title":"Easy to try and install","text":"<p>The minimum viable api just adds <code>response_model</code> to the client, if you dont think you want a model its very easy to remove it and continue building your application </p> InstructorOpenAI <pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n# Patch the OpenAI client with Instructor\nclient = instructor.from_openai(OpenAI())\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n# Function to extract user details\ndef extract_user() -&gt; UserDetail:\n    user = client.chat.completions.create(\n        model=\"gpt-4-turbo-preview\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n        ]\n    )\n    return user\n</code></pre> <pre><code>import openai\nimport json\n\ndef extract_user() -&gt; dict:\n    completion = client.chat.completions.create(\n        model=\"gpt-4-turbo-preview\",\n        tools=[\n            {\n                \"type\": \"function\",\n                \"function\": {\n                    \"name\": \"ExtractUser\",\n                    \"description\": \"Correctly extracted `ExtractUser` with all the required parameters with correct types\",\n                    \"parameters\": {\n                        \"properties\": {\n                            \"name\": {\"title\": \"Name\", \"type\": \"string\"},\n                            \"age\": {\"title\": \"Age\", \"type\": \"integer\"},\n                        },\n                        \"required\": [\"age\", \"name\"],\n                        \"type\": \"object\",\n                    },\n                },\n            }\n        ],\n        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"ExtractUser\"}},\n        messages=[\n            {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n        ],\n    )  # type: ignore\n\n    user = json_loads(completion.choices[0].message.tool_calls[0].function.arguments)\n    assert \"name\" in user, \"Name is not in the response\"\n    assert \"age\" in user, \"Age is not in the response\"\n    user[\"age\"] = int(user[\"age\"])\n    return user\n</code></pre>"},{"location":"why/#partial-extraction","title":"Partial Extraction","text":"<p>We also support partial extraction, which is useful for streaming in data that is incomplete.</p> <pre><code>import instructor\n\nfrom instructor import Partial\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nfrom typing import List\nfrom rich.console import Console\n\nclient = instructor.from_openai(OpenAI())\n\ntext_block = \"...\"\n\nclass User(BaseModel):\n    name: str\n    email: str\n    twitter: str\n\n\nclass MeetingInfo(BaseModel):\n    users: List[User]\n    date: str\n    location: str\n    budget: int\n    deadline: str\n\n\nextraction_stream = client.chat.completions.create(\n    model=\"gpt-4\",\n    response_model=Partial[MeetingInfo],\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"Get the information about the meeting and the users {text_block}\",\n        },\n    ],\n    stream=True,\n)\n\n\nconsole = Console()\n\nfor extraction in extraction_stream:\n    obj = extraction.model_dump()\n    console.clear()\n    console.print(obj)\n</code></pre> <p>This will output the following:</p> <p></p> <p>As you can see, we've baked in a self correcting mechanism into the model. This is a powerful way to make your models more robust and less brittle without including a lot of extra code or prompts.</p>"},{"location":"why/#iterables-and-lists","title":"Iterables and Lists","text":"<p>We can also generate tasks as the tokens are streamed in by defining an <code>Iterable[T]</code> type.</p> <p>Lets look at an example in action with the same class</p> <pre><code>from typing import Iterable\n\nUsers = Iterable[User]\n\nusers = client.chat.completions.create(\n    model=\"gpt-4\",\n    temperature=0.1,\n    stream=True,\n    response_model=Users,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a perfect entity extraction system\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": (\n                f\"Consider the data below:\\n{input}\"\n                \"Correctly segment it into entitites\"\n                \"Make sure the JSON is correct\"\n            ),\n        },\n    ],\n    max_tokens=1000,\n)\n\nfor user in users:\n    assert isinstance(user, User)\n    print(user)\n\n#&gt; name=\"Jason\" \"age\"=10\n#&gt; name=\"John\" \"age\"=10\n</code></pre>"},{"location":"why/#simple-types","title":"Simple Types","text":"<p>We also support simple types, which are useful for extracting simple values like numbers, strings, and booleans.</p>"},{"location":"why/#self-correcting-on-validation-error","title":"Self Correcting on Validation Error","text":"<p>Due to pydantic's very own validation model, easily add validators to the model to correct the data.  If we run this code, we will get a validation error because the name is not in uppercase. While we could have included a prompt to fix this, we can also just add a field validator to the model. This will result in two API calls, to make sure you do your best to prompt before adding validators.</p> <pre><code>import instructor\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel, field_validator\n\n# Apply the patch to the OpenAI client\nclient = instructor.from_openai(OpenAI())\n\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"name\")\n    @classmethod\n    def validate_name(cls, v):\n        if v.upper() != v:\n            raise ValueError(\"Name must be in uppercase.\")\n        return v\n\n\nmodel = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetails,\n    max_retries=2,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nassert model.name == \"JASON\"\n</code></pre>"},{"location":"blog/","title":"Welcome to the Instructor Blog","text":"<p>The goal of the blog is to capture some content that does not neatly fit within documentation or the cookbooks. I also want to share some of my thoughts and experiences with the community. If you want to check out my personal blog, you can here</p>"},{"location":"blog/#advanced-topics","title":"Advanced Topics","text":"<ol> <li>What is Query Understanding, how does it go beyond embeddings?</li> <li>How can one achieve GPT-4 level summaries using GPT-3.5-turbo?</li> <li>What are the basics of Guardrails and Validation in AI models?</li> <li>How does one validate citations in AI-generated content?</li> <li>What are the methods and benefits of fine-tuning and distillation in AI models?</li> </ol>"},{"location":"blog/#learning-python","title":"Learning Python","text":"<ul> <li>How can I effectively cache my functions in Python?</li> <li>What are the fundamentals of batch processing with async in Python?</li> <li>How can I stream models to improve latency?</li> </ul>"},{"location":"blog/#integrations","title":"Integrations","text":"<ul> <li>Ollama</li> <li>llama-cpp-python</li> <li>Anyscale</li> <li>Together Compute</li> </ul>"},{"location":"blog/#media","title":"Media","text":"<ul> <li>Course: Structured Outputs w/ Instructor</li> <li>Keynote: Pydantic is all you need</li> </ul>"},{"location":"blog/2023/11/02/ai-engineer-keynote-pydantic-is-all-you-need/","title":"AI Engineer Keynote: Pydantic is all you need","text":"<p>Click here to watch the full talk</p> <p>Last month, I ventured back onto the speaking circuit at the inaugural AI Engineer Summit, sharing insights on leveraging Pydantic for effective prompt engineering. I dove deep into what is covered in our documentation and standard blog posts,</p> <p>I'd genuinely appreciate any feedback on the talk \u2013 every bit helps in refining the art. So, take a moment to check out the full talk here, and let's continue pushing the boundaries of what's possible.</p>","tags":["python","talks","prompt engineering","video"]},{"location":"blog/2024/09/03/structured-outputs-for-gemini-now-supported/","title":"Structured Outputs for Gemini now supported","text":"<p>We're excited to announce that <code>instructor</code> now supports structured outputs using tool calling for both the Gemini SDK and the VertexAI SDK.</p> <p>A special shoutout to Sonal for his contributions to the Gemini Tool Calling support.</p> <p>Let's walk through a simple example of how to use these new features</p>"},{"location":"blog/2024/09/03/structured-outputs-for-gemini-now-supported/#installation","title":"Installation","text":"<p>To get started, install the latest version of <code>instructor</code>. Depending on whether you're using Gemini or VertexAI, you should install the following:</p> GeminiVertexAI <pre><code>pip install \"instructor[google-generativeai]\"\n</code></pre> <pre><code>pip install \"instructor[vertexai]\"\n</code></pre> <p>This ensures that you have the necessary dependencies to use the Gemini or VertexAI SDKs with instructor.</p> <p>We recommend using the Gemini SDK over the VertexAI SDK for two main reasons.</p> <ol> <li>Compared to the VertexAI SDK, the Gemini SDK comes with a free daily quota of 1.5 billion tokens to use for developers.</li> <li>The Gemini SDK is significantly easier to setup, all you need is a <code>GOOGLE_API_KEY</code> that you can generate in your GCP console. THe VertexAI SDK on the other hand requires a credentials.json file or an OAuth integration to use.</li> </ol>"},{"location":"blog/2024/09/03/structured-outputs-for-gemini-now-supported/#getting-started","title":"Getting Started","text":"<p>With our provider agnostic API, you can use the same interface to interact with both SDKs, the only thing that changes here is how we initialise the client itself.</p> <p>Before running the following code, you'll need to make sure that you have your Gemini API Key set in your shell under the alias <code>GOOGLE_API_KEY</code>.</p> <pre><code>import instructor\nimport google.generativeai as genai\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclient = instructor.from_gemini(\n    client=genai.GenerativeModel(\n        model_name=\"models/gemini-1.5-flash-latest\", # (1)!\n    )\n)\n\nresp = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Jason is 25 years old.\",\n        }\n    ],\n    response_model=User,\n)\n\nprint(resp)\n#&gt; name='Jason' age=25\n</code></pre> <ol> <li>Current Gemini models that support tool calling are <code>gemini-1.5-flash-latest</code> and <code>gemini-1.5-pro-latest</code>.</li> </ol> <p>We can achieve a similar thing with the VertexAI SDK. For this to work, you'll need to authenticate to VertexAI.</p> <p>There are some instructions here but the easiest way I found was to simply download the GCloud cli and run <code>gcloud auth application-default login</code>.</p> <pre><code>import instructor\nimport vertexai  # type: ignore\nfrom vertexai.generative_models import GenerativeModel  # type: ignore\nfrom pydantic import BaseModel\n\nvertexai.init()\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclient = instructor.from_vertexai(\n    client=GenerativeModel(\"gemini-1.5-pro-preview-0409\"), # (1)!\n)\n\n\nresp = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Jason is 25 years old.\",\n        }\n    ],\n    response_model=User,\n)\n\nprint(resp)\n#&gt; name='Jason' age=25\n</code></pre> <ol> <li>Current Gemini models that support tool calling are <code>gemini-1.5-flash-latest</code> and <code>gemini-1.5-pro-latest</code>.</li> </ol>"},{"location":"blog/2024/03/20/announcing-anthropic-support/","title":"Announcing Anthropic Support","text":"<p>A special shoutout to Shreya for her contributions to the anthropic support. As of now, all features are operational with the exception of streaming support.</p> <p>For those eager to experiment, simply patch the client with <code>ANTHROPIC_JSON</code>, which will enable you to leverage the <code>anthropic</code> client for making requests.</p> <pre><code>pip install instructor[anthropic]\n</code></pre> <p>Missing Features</p> <p>Just want to acknowledge that we know that we are missing partial streaming and some better re-asking support for XML. We are working on it and will have it soon.</p> <pre><code>from pydantic import BaseModel\nfrom typing import List\nimport anthropic\nimport instructor\n\n# Patching the Anthropics client with the instructor for enhanced capabilities\nanthropic_client = instructor.from_openai(\n    create=anthropic.Anthropic().messages.create,\n    mode=instructor.Mode.ANTHROPIC_JSON\n)\n\nclass Properties(BaseModel):\n    name: str\n    value: str\n\nclass User(BaseModel):\n    name: str\n    age: int\n    properties: List[Properties]\n\nuser_response = anthropic_client(\n    model=\"claude-3-haiku-20240307\",\n    max_tokens=1024,\n    max_retries=0,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Create a user for a model with a name, age, and properties.\",\n        }\n    ],\n    response_model=User,\n)  # type: ignore\n\nprint(user_response.model_dump_json(indent=2))\n\"\"\"\n{\n    \"name\": \"John\",\n    \"age\": 25,\n    \"properties\": [\n        {\n            \"key\": \"favorite_color\",\n            \"value\": \"blue\"\n        }\n    ]\n}\n</code></pre> <p>We're encountering challenges with deeply nested types and eagerly invite the community to test, provide feedback, and suggest necessary improvements as we enhance the anthropic client's support.</p>"},{"location":"blog/2024/03/05/zero-cost-abstractions/","title":"Why Instructor is the Best Library for Structured LLM Outputs","text":"<p>Large language models (LLMs) like GPTs are incredibly powerful, but working with their open-ended text outputs can be challenging. This is where the Instructor library shines - it allows you to easily map LLM outputs to structured data using Python type annotations.</p> <p>The core idea behind Instructor is incredibly simple: it's just a patch over the OpenAI Python SDK that adds a response_model parameter. This parameter lets you pass in a Pydantic model that describes the structure you want the LLM output mapped to. Pydantic models are defined using standard Python type hints, so there's zero new syntax to learn.</p> <p>Here's an example of extracting structured user data from an LLM:</p> <pre><code>from pydantic import BaseModel\nimport instructor\n\nclass User(BaseModel):\n    name: str \n    age: int\n\nclient = instructor.from_openai(openai.OpenAI())\n\nuser = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=User, # (1)!\n    messages=[\n        {\n            \"role\": \"user\", \n            \"content\": \"Extract the user's name and age from this: John is 25 years old\"\n        }\n    ]\n)\n\nprint(user) # (2)!\n# &gt; User(name='John', age=25)\n</code></pre> <ol> <li>Notice that now we have a new response_model parameter that we pass in to the completions.create method. This parameter lets us specify the structure we want the LLM output to be mapped to. In this case, we're using a Pydantic model called User that describes a user's name and age.</li> <li>The output of the completions.create method is a User object that matches the structure we specified in the response_model parameter, rather than a ChatCompletion.</li> </ol>","tags":["python","llms"]},{"location":"blog/2024/03/05/zero-cost-abstractions/#other-features","title":"Other Features","text":"<p>Other features on instructor, in and out of the llibrary are:</p> <ol> <li>Ability to use Tenacity in retrying logic</li> <li>Ability to use Pydantic's validation context</li> <li>Parallel Tool Calling with correct types</li> <li>Streaming Partial and Iterable data.</li> <li>Returning Primitive Types and Unions as well! </li> <li>Lots, and Lots of Cookbooks, Tutorials, Documentation and even instructor hub</li> </ol>","tags":["python","llms"]},{"location":"blog/2024/03/05/zero-cost-abstractions/#instructors-broad-applicability","title":"Instructor's Broad Applicability","text":"<p>One of the key strengths of Instructor is that it's designed as a lightweight patch over the official OpenAI Python SDK. This means it can be easily integrated not just with OpenAI's hosted API service, but with any provider or platform that exposes an interface compatible with the OpenAI SDK.</p> <p>For example, providers like Anyscale, Together, Ollama, Groq, and llama-cpp-python all either use or mimic the OpenAI Python SDK under the hood. With Instructor's zero-overhead patching approach, teams can immediately start deriving structured data outputs from any of these providers. There's no need for custom integration work.</p>","tags":["python","llms"]},{"location":"blog/2024/03/05/zero-cost-abstractions/#direct-access-to-the-messages-array","title":"Direct access to the messages array","text":"<p>Unlike other libraries that abstract away the <code>messages=[...]</code> parameter, Instructor provides direct access. This direct approach facilitates intricate prompt engineering, ensuring compatibility with OpenAI's evolving message types, including future support for images, audio, or video, without the constraints of string formatting.</p>","tags":["python","llms"]},{"location":"blog/2024/03/05/zero-cost-abstractions/#low-abstraction","title":"Low Abstraction","text":"<p>What makes Instructor so powerful is how seamlessly it integrates with existing OpenAI SDK code. To use it, you literally just call instructor.from_openai() on your OpenAI client instance, then use response_model going forward. There's no complicated refactoring or new abstractions to wrap your head around.</p> <p>This incremental, zero-overhead adoption path makes Instructor perfect for sprinkling structured LLM outputs into an existing OpenAI-based application. You can start extracting data models from simple prompts, then incrementally expand to more complex hierarchical models, streaming outputs, and custom validations.</p> <p>And if you decide Instructor isn't a good fit after all, removing it is as simple as not applying the patch! The familiarity and flexibility of working directly with the OpenAI SDK is a core strength.</p> <p>Instructor solves the \"string hellll\" of unstructured LLM outputs. It allows teams to easily realize the full potential of tools like GPTs by mapping their text to type-safe, validated data structures. If you're looking to get more structured value out of LLMs, give Instructor a try!</p>","tags":["python","llms"]},{"location":"blog/2023/11/26/python-caching/","title":"Introduction to Caching in Python","text":"<p>Instructor makes working with language models easy, but they are still computationally expensive.</p> <p>Today, we're diving into optimizing instructor code while maintaining the excellent DX offered by Pydantic models. We'll tackle the challenges of caching Pydantic models, typically incompatible with <code>pickle</code>, and explore solutions that use <code>decorators</code> like <code>functools.cache</code>. Then, we'll craft custom decorators with <code>diskcache</code> and <code>redis</code> to support persistent caching and distributed systems.</p> <p>Let's first consider our canonical example, using the <code>OpenAI</code> Python client to extract user details.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n# Enables `response_model`\nclient = instructor.from_openai(OpenAI())\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\ndef extract(data) -&gt; UserDetail:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ],\n    )\n</code></pre> <p>Now imagine batch processing data, running tests or experiments, or simply calling <code>extract</code> multiple times over a workflow. We'll quickly run into performance issues, as the function may be called repeatedly, and the same data will be processed over and over again, costing us time and money.</p>","tags":["caching","functools","redis","diskcache","python"]},{"location":"blog/2023/11/26/python-caching/#1-functoolscache-for-simple-in-memory-caching","title":"1. <code>functools.cache</code> for Simple In-Memory Caching","text":"<p>When to Use: Ideal for functions with immutable arguments, called repeatedly with the same parameters in small to medium-sized applications. This makes sense when we might be reusing the same data within a single session or in an application where we don't need to persist the cache between sessions.</p> <pre><code>import functools\n\n\n@functools.cache\ndef extract(data):\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ],\n    )\n</code></pre> <p>Changing the Model does not Invalidate the Cache</p> <p>Note that changing the model does not invalidate the cache. This is because the cache key is based on the function's name and arguments, not the model. This means that if we change the model, the cache will still return the old result.</p> <p>Now we can call <code>extract</code> multiple times with the same argument, and the result will be cached in memory for faster access.</p> <pre><code>import time\n\nstart = time.perf_counter()  # (1)\nmodel = extract(\"Extract jason is 25 years old\")\nprint(f\"Time taken: {time.perf_counter() - start}\")\n\nstart = time.perf_counter()\nmodel = extract(\"Extract jason is 25 years old\")  # (2)\nprint(f\"Time taken: {time.perf_counter() - start}\")\n\n#&gt; Time taken: 0.92\n#&gt; Time taken: 1.20e-06 # (3)\n</code></pre> <ol> <li>Using <code>time.perf_counter()</code> to measure the time taken to run the function is better than using <code>time.time()</code> because it's more accurate and less susceptible to system clock changes.</li> <li>The second time we call <code>extract</code>, the result is returned from the cache, and the function is not called.</li> <li>The second call to <code>extract</code> is much faster because the result is returned from the cache!</li> </ol> <p>Benefits: Easy to implement, provides fast access due to in-memory storage, and requires no additional libraries.</p> What is a decorator? <p>A decorator is a function that takes another function and extends the behavior of the latter function without explicitly modifying it. In Python, decorators are functions that take a function as an argument and return a closure.</p> <pre><code>def decorator(func):\n    def wrapper(*args, **kwargs):\n        print(\"Do something before\")  # (1)\n        result = func(*args, **kwargs)\n        print(\"Do something after\")  # (2)\n        return result\n\n    return wrapper\n\n\n@decorator\ndef say_hello():\n    print(\"Hello!\")\n\n\nsay_hello()\n#&gt; \"Do something before\"\n#&gt; \"Hello!\"\n#&gt; \"Do something after\"\n</code></pre> <ol> <li>The code is executed before the function is called</li> <li>The code is executed after the function is called</li> </ol>","tags":["caching","functools","redis","diskcache","python"]},{"location":"blog/2023/11/26/python-caching/#2-diskcache-for-persistent-large-data-caching","title":"2. <code>diskcache</code> for Persistent, Large Data Caching","text":"Copy Caching Code <p>We'll be using the same <code>instructor_cache</code> decorator for both <code>diskcache</code> and <code>redis</code> caching. You can copy the code below and use it for both examples.</p> <pre><code>import functools\nimport inspect\nimport diskcache\n\ncache = diskcache.Cache('./my_cache_directory')  # (1)\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):  # (2)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n</code></pre> <ol> <li>We create a new <code>diskcache.Cache</code> instance to store the cached data. This will create a new directory called <code>my_cache_directory</code> in the current working directory.</li> <li>We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic in this example code</li> </ol> <p>Remember that you can change this code to support non-Pydantic models, or to use a different caching backend. More over, don't forget that this cache does not invalidate when the model changes, so you might want to encode the <code>Model.model_json_schema()</code> as part of the key.</p> <p>When to Use: Suitable for applications needing cache persistence between sessions or dealing with large datasets. This is useful when we want to reuse the same data across multiple sessions, or when we need to store large amounts of data!</p> <pre><code>import functools\nimport inspect\nimport instructor\nimport diskcache\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI())\ncache = diskcache.Cache('./my_cache_directory')\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation  # (4)\n    if not issubclass(return_type, BaseModel):  # (1)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = (\n            f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"  #  (2)\n        )\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type (3)\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n@instructor_cache\ndef extract(data) -&gt; UserDetail:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ],\n    )\n</code></pre> <ol> <li>We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic</li> <li>We use functool's <code>_make_key</code> to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately.</li> <li>We use Pydantic's <code>model_validate_json</code> to deserialize the cached result into a Pydantic model.</li> <li>We use <code>inspect.signature</code> to get the function's return type annotation, which we use to validate the cached result.</li> </ol> <p>Benefits: Reduces computation time for heavy data processing, provides disk-based caching for persistence.</p>","tags":["caching","functools","redis","diskcache","python"]},{"location":"blog/2023/11/26/python-caching/#2-redis-caching-decorator-for-distributed-systems","title":"2. Redis Caching Decorator for Distributed Systems","text":"Copy Caching Code <p>We'll be using the same <code>instructor_cache</code> decorator for both <code>diskcache</code> and <code>redis</code> caching. You can copy the code below and use it for both examples.</p> <pre><code>import functools\nimport inspect\nimport redis\n\ncache = redis.Redis(\"localhost\")\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n</code></pre> <p>Remember that you can change this code to support non-Pydantic models, or to use a different caching backend. More over, don't forget that this cache does not invalidate when the model changes, so you might want to encode the <code>Model.model_json_schema()</code> as part of the key.</p> <p>When to Use: Recommended for distributed systems where multiple processes need to access the cached data, or for applications requiring fast read/write access and handling complex data structures.</p> <pre><code>import redis\nimport functools\nimport inspect\nimport instructor\n\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\ncache = redis.Redis(\"localhost\")\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):  # (1)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"  # (2)\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n@instructor_cache\ndef extract(data) -&gt; UserDetail:\n    # Assuming client.chat.completions.create returns a UserDetail instance\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ],\n    )\n</code></pre> <ol> <li>We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic</li> <li>We use functool's <code>_make_key</code> to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately.</li> </ol> <p>Benefits: Scalable for large-scale systems, supports fast in-memory data storage and retrieval, and is versatile for various data types.</p> <p>Looking carefully</p> <p>If you look carefully at the code above you'll notice that we're using the same <code>instructor_cache</code> decorator as before. The implementation is the same, but we're using a different caching backend!</p>","tags":["caching","functools","redis","diskcache","python"]},{"location":"blog/2023/11/26/python-caching/#conclusion","title":"Conclusion","text":"<p>Choosing the right caching strategy depends on your application's specific needs, such as the size and type of data, the need for persistence, and the system's architecture. Whether it's optimizing a function's performance in a small application or managing large datasets in a distributed environment, Python offers robust solutions to improve efficiency and reduce computational overhead.</p> <p>If you'd like to use this code, try to send it over to ChatGPT to understand it more, and to add additional features that might matter for you, for example, the cache isn't invalidated when your BaseModel changes, so you might want to encode the <code>Model.model_json_schema()</code> as part of the key.</p> <p>If you like the content check out our GitHub as give us a star and checkout the library.</p>","tags":["caching","functools","redis","diskcache","python"]},{"location":"blog/2023/11/05/chain-of-density/","title":"Smarter Summaries w/ Finetuning GPT-3.5 and Chain of Density","text":"<p>Discover how to distil an iterative method like Chain Of Density into a single finetuned model using Instructor</p> <p>In this article, we'll guide you through implementing the original Chain of Density method using Instructor, then show how to distile a GPT 3.5 model to match GPT-4's iterative summarization capabilities. Using these methods were able to decrease latency by 20x, reduce costs by 50x and maintain entity density.</p> <p>By the end you'll end up with a GPT 3.5 model, (fine-tuned using Instructor's great tooling), capable of producing summaries that rival the effectiveness of Chain of Density [Adams et al. (2023)]. As always, all code is readily available in our <code>examples/chain-of-density</code> folder in our repo for your reference.</p> Datasets and Colab Notebook <p>We've also uploaded all our generated data to Hugging Face here for you to use if you'd like to try reproducing these experiments. We've also added a Colab Instance for you to check our generated values.</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#part-1-chain-of-density","title":"Part 1) Chain of Density","text":"<p>Summarizing extensive texts with AI can be challenging, often relying on inconsistent techniques. Their novel method, Chain Of Density prompting, enhances AI-based text summarization, outperforming human-generated summaries.</p> <p>Initially, an AI produces a summary, then refines it through multiple iterations, adding missing article entities. Each iteration adds new article entities to the summary, keeping length consistent, leading to an entity-dense, informative summary called Chain Of Density.</p> <p>First introduced in the paper - From Sparse to Dense: GPT-4 Summarization with Chain of Density Prompting. The team has found that this method is able to consistently beats similar summaries written by human annotators.</p> Implementation Details <p>Note that our implementation uses a validator to ensure that the rewritten summary has a minimum length rather than a prompt. We also perform just 3 and not 5 rounds of rewrites, resulting in a lower final entity density.</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#original-prompt","title":"Original Prompt","text":"<p>We can break down the original process into smaller api calls. This allows us to introduce validation at each step to ensure that we're getting the results that we want.</p> Original Chain of Density Prompt <pre><code>Article: {{ARTICLE}}\n\nYou will generate increasingly concise, entity-dense summaries of the\nabove Article.\n\nRepeat the following 2 steps 5 times.\n\nStep 1. Identify 1-3 informative Entities (\";\" delimited) from the\nArticle which are missing from the previously generated summary.\nStep 2. Write a new, denser summary of identical length which covers\nevery entity and detail from the previous summary plus the Missing\nEntities.\n\nA Missing Entity is:\n- Relevant: to the main story.\n- Specific: descriptive yet concise (5 words or fewer).\n- Novel; not in the previous summary.\n- Faithful: present in the Article.\n- Anywhere: located anywhere in the Article.\n\nGuidelines:\n- The first summary should be long (4-5 sentences, -80 words) yet\nhighly non-specific, containing little information beyond the\nentities marked as missing. Use overly verbose language and fillers\n(e.g., \"this article discusses\") to reach -80 words.\n- Make every word count: re-write the previous summary to improve\nflow and make space for additional entities.\n- Make space with fusion, compression, and removal of uninformative\nphrases like \"the article discusses\"\n- The summaries should become highly dense and concise yet\nself-contained, e.g., easily understood without the Article.\n- Missing entities can appear anywhere in the new summary.\n- Never drop entities from the previous summary. If space cannot be\nmade, add fewer new entities.\n\nRemember, use the exact same number of words for each summary.\n\nAnswer in JSON. The JSON should be a list (length 5) of dictionaries\nwhose keys are \"Missing_Entities\" and \"Denser_Summary\"\n</code></pre> <p> </p> Improved process with Instructor","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#data-modelling","title":"Data Modelling","text":"<p>Before we begin modelling the data, let's make sure we install all of our dependencies</p> <pre><code>pip install instructor aiohttp rich\n</code></pre>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#initial-summary","title":"Initial Summary","text":"<p>Let's start by walking through some of the data models that we'll be using as the <code>response_model</code> for our open ai function calls</p> <p>Firstly, we'll need a data model for the initial summary that we will be generating. We'll take the description of this class straight from the original prompt. It's important to note that these docstrings serve a purpose, they are directly used by the LLM when generating the outputs.</p> A quick note on Docstrings <p>Under the hood, Instructor parses the <code>response_model</code> that you give us into a function call for OpenAI to execute. This means that the final output will be closely linked to the Pydantic model you specify.</p> <p>For instance, this simple model that we later use in fine-tuning.</p> <pre><code>class GeneratedSummary(BaseModel):\n    \"\"\"\n    This represents a highly concise summary that includes as many entities as possible from the original source article.\n\n    An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\n\n    Guidelines\n    - Make every word count\n    - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.\n    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\"\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This represents the final summary generated that captures the meaning of the original article which is as concise as possible. \",\n    )\n</code></pre> <p>We eventually transform it into an OpenAI function call as seen below.</p> <pre><code>{\n\"functions\": [\n    {\n    \"name\": \"GeneratedSummary\",\n    \"description\": \"This represents a highly concise summary that includes as many entities as possible from the original source article.\\n\\nAn Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\\n\\nGuidelines\\n- Make every word count\\n- The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.\\n- Make space with fusion, compression, and removal of uninformative phrases like \\\"the article discusses\\\"\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n        \"summary\": {\n            \"description\": \"This represents the final summary generated that captures the meaning of the original article which is as concise as possible. \",\n            \"title\": \"Summary\",\n            \"type\": \"string\"\n        }\n        },\n        \"required\": [\n        \"summary\"\n        ]\n\n    }\n    }\n]\n}\n}\n</code></pre> <p>Therefore this means that the more elaborate and detailed your descriptions are, the better the outputs you will be able to get back. But we don't just stop there, since it's all Pydantic under the hood, you can validate and parse the resulting output to make sure it is exactly what you specify. It's all python all the way down.</p> <pre><code>class InitialSummary(BaseModel):\n    \"\"\"\n    This is an initial summary which should be long ( 4-5 sentences, ~80 words)\n    yet highly non-specific, containing little information beyond the entities marked as missing.\n    Use overly verbose languages and fillers (Eg. This article discusses) to reach ~80 words.\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This is a summary of the article provided which is overly verbose and uses fillers. It should be roughly 80 words in length\",\n    )\n</code></pre>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#rewritten-summary","title":"Rewritten Summary","text":"<p>We'll also need one additional class to help model the rewritten schema</p> <pre><code>class RewrittenSummary(BaseModel):\n    \"\"\"\n    This is a new, denser summary of identical length which covers every entity\n    and detail from the previous summary plus the Missing Entities.\n\n    Guidelines\n    - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities\n    - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n    - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.\n    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\"\n    - Missing entities can appear anywhere in the new summary\n\n    An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\",\n    )\n    absent: List[str] = Field(\n        ...,\n        default_factory=list,\n        description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\",\n    )\n    missing: List[str] = Field(\n        default_factory=list,\n        description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\",\n    )\n</code></pre> <p>Using Pydantic Validators with Instructor</p> <p>For a more in-depth walkthrough on how to use <code>Pydantic</code> validators with the <code>Instructor</code> library, we recommend checking out our previous article on LLM validation - Good LLM Validation is just Good Validation</p> <p>Ideally, we'd like for <code>Missing</code> to have a length between 1 and 3, <code>Absent</code> to be an empty list and for our rewritten summaries to keep a minimum entity density. With <code>Instructor</code>, we can implement this logic using native <code>Pydantic</code> validators that are simply declared as part of the class itself.</p> <pre><code>import nltk\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\n@field_validator(\"summary\")\ndef min_length(cls, v: str):\n    tokens = nltk.word_tokenize(v) #(1)!\n    num_tokens = len(tokens)\n    if num_tokens &lt; 60:\n        raise ValueError(\n            \"The current summary is too short. Please make sure that you generate a new summary that is around 80 words long.\"\n        )\n    return v\n\n@field_validator(\"missing\")\ndef has_missing_entities(cls, missing_entities: List[str]):\n    if len(missing_entities) == 0:\n        raise ValueError(\n            \"You must identify 1-3 informative Entities from the Article which are missing from the previously generated summary to be used in a new summary\"\n        )\n    return missing_entities\n\n@field_validator(\"absent\")\ndef has_no_absent_entities(cls, absent_entities: List[str]):\n    absent_entity_string = \",\".join(absent_entities)\n    if len(absent_entities) &gt; 0:\n        print(f\"Detected absent entities of {absent_entity_string}\")\n        raise ValueError(\n            f\"Do not omit the following Entities {absent_entity_string} from the new summary\"\n        )\n    return absent_entities\n\n@field_validator(\"summary\")\ndef min_entity_density(cls, v: str):\n    tokens = nltk.word_tokenize(v)\n    num_tokens = len(tokens)\n\n    # Extract Entities\n    doc = nlp(v) #(2)!\n    num_entities = len(doc.ents)\n\n    density = num_entities / num_tokens\n    if density &lt; 0.08: #(3)!\n        raise ValueError(\n            f\"The summary of {v} has too few entities. Please regenerate a new summary with more new entities added to it. Remember that new entities can be added at any point of the summary.\"\n        )\n\n    return v\n</code></pre> <ol> <li> <p>Similar to the original paper, we utilize the <code>NLTK</code> word tokenizer to count the number of tokens within our generated sentences.     We aim for at least 60 tokens in our generated summary so that we don't lose information.</p> </li> <li> <p>We also use the spaCy library to calculate the entity density of the generated summary.</p> </li> <li> <p>We also implement a minimum entity density so that we stay within a given range. 0.08 is arbitrarily chosen in this case</p> </li> </ol>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#putting-it-all-together","title":"Putting it all Together","text":"<p>Now that we have our models and the rough flow figured out, let's implement a function to summarize a piece of text using <code>Chain Of Density</code> summarization.</p> <pre><code>from openai import OpenAI\nimport instructor\n\nclient = instructor.from_openai(OpenAI()) #(1)!\n\ndef summarize_article(article: str, summary_steps: int = 3):\n    summary_chain = []\n    # We first generate an initial summary\n    summary: InitialSummary = client.chat.completions.create(  # (2)!\n        model=\"gpt-4-0613\",\n        response_model=InitialSummary,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Write a summary about the article that is long (4-5 sentences) yet highly non-specific. Use overly, verbose language and fillers(eg.,'this article discusses') to reach ~80 words\",\n            },\n            {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"},\n            {\n                \"role\": \"user\",\n                \"content\": \"The generated summary should be about 80 words.\",\n            },\n        ],\n        max_retries=2,\n    )\n    prev_summary = None\n    summary_chain.append(summary.summary)\n    for i in range(summary_steps):\n        missing_entity_message = (\n            []\n            if prev_summary is None\n            else [\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Please include these Missing Entities: {','.join(prev_summary.missing)}\",\n                },\n            ]\n        )\n        new_summary: RewrittenSummary = client.chat.completions.create( # (3)!\n            model=\"gpt-4-0613\",\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"\n                You are going to generate an increasingly concise,entity-dense summary of the following article.\n\n                Perform the following two tasks\n                - Identify 1-3 informative entities from the following article which is missing from the previous summary\n                - Write a new denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities\n\n                Guidelines\n                - Make every word count: re-write the previous summary to improve flow and make space for additional entities\n                - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n                - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.\n                - Missing entities can appear anywhere in the new summary\n                - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n                \"\"\",\n                },\n                {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"},\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Here is the previous summary: {summary_chain[-1]}\",\n                },\n                *missing_entity_message,\n            ],\n            max_retries=3, #(4)!\n            max_tokens=1000,\n            response_model=RewrittenSummary,\n        )\n        summary_chain.append(new_summary.summary)\n        prev_summary = new_summary\n\n    return summary_chain\n</code></pre> <ol> <li> <p>We need to apply a <code>patch</code> function on the <code>OpenAI</code> client for us to get all     of the benefits that <code>Instructor</code> provides. With a simple <code>patch</code>, we can get     automatic type coercion of our outputs and automatic retries for invalid outputs     out of the box!</p> </li> <li> <p>We first generate an initial summary. Note here that we explictly ask for a summary that has     80 words and is lengthy with overly verbose fillers in the system prompt</p> </li> <li> <p>We slightly modify the original system prompt used in the original paper to perform a rewrite of the summary.     Using <code>Instructor</code>, we also get validation of the generated output with our <code>field_validator</code>s that we defined above</p> </li> <li> <p>If you've chosen a value that is larger than 0.08, make sure to increase this value in case you need to do multiple rewrites</p> </li> </ol> <p>This summarization function yields a result which triples the number of entities while maintaining the same number of tokens. We can also see that stylistically, the summary is a lot more natural.</p> <p>First Iteration</p> <p>This article discusses the highly-anticipated boxing match between Manny Pacquiao and Floyd Mayweather. The article revolves around Manny Pacquiao's statements about his upcoming fight and his preparations for the same. A portion of the article provides details about the financial stipulations of the match and its significance in the sporting arena. Quotes from Pacquiao illustrating his determination and his battle strategy are highlighted. The tone of the article is largely centered around creating a build-up to the upcoming mega event.</p> <p>Final Iteration</p> <p>Manny Pacquiao, the Filipino boxer, anticipates the forthcoming May 2 showdown at the MGM Grand as the fight of his life, against the undefeated American Floyd Mayweather, in a $300m bout. Despite being seen as the underdog in this high-stakes Las Vegas match, Pacquiao is confident, promising a warrior's spirit and assuring the fans who have been awaiting this encounter for a decade, that it will indeed be the biggest sporting spectacle in history worthy of their anticipation</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#part-2-fine-tuning","title":"Part 2) Fine-Tuning","text":"<p>In this section, we'll look into how to fine-tune a GPT 3.5 model so that it is able to perform at an equivalent level as a GPT-4 model. We'll then compare the performance of our model against that of <code>GPT-4</code> to see how it stacks up.</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#creating-a-training-set","title":"Creating a Training Set","text":"<p>In order to prevent any contamination of data during testing, we randomly sampled 120 articles from the <code>griffin/chain-of-density</code> dataset and split these articles into a <code>train.csv</code> and a <code>test.csv</code> file which we uploaded to Hugging Face. Now, we just neeed to import the <code>Instructions</code> module from the <code>Instructor</code> package which allows you to generate a nicely formatted <code>.jsonl</code> file to be used for fine-tuning</p> <pre><code>from typing import List\nfrom chain_of_density import summarize_article #(1)!\nimport csv\nimport logging\nimport instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI()) # (2)!\n\nlogging.basicConfig(level=logging.INFO) #(3)!\n\ninstructions = instructor.Instructions( #(4)!\n    name=\"Chain Of Density\",\n    finetune_format=\"messages\",\n    # log handler is used to save the data to a file\n    # you can imagine saving it to a database or other storage\n    # based on your needs!\n    log_handlers=[logging.FileHandler(\"generated.jsonl\")],\n    openai_client=client,\n)\n\nclass GeneratedSummary(BaseModel):\n    \"\"\"\n    This represents a highly concise summary that includes as many entities as possible from the original source article.\n\n    An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\n\n    Guidelines\n    - Make every word count\n    - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.\n    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\"\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This represents the final summary generated that captures the meaning of the original article which is as concise as possible. \",\n    )\n\n@instructions.distil #(4)!\ndef distil_summarization(text: str) -&gt; GeneratedSummary:\n    summary_chain: List[str] = summarize_article(text)\n    return GeneratedSummary(summary=summary_chain[-1]) #(5)!\n\nwith open(\"train.csv\", \"r\") as file:\n    reader = csv.reader(file)\n    next(reader)  # Skip the header\n    for article, summary in reader:\n        # Run Distillisation to generate the values\n        distil_summarization(article)\n</code></pre> <ol> <li> <p>In this example, we're using the summarize_article that we defined up above. We saved it in a local file called <code>chain_of_density.py</code>,     hence the import</p> </li> <li> <p>We patch the default OpenAI client so that we can use the Instructor library with it</p> </li> <li> <p>We also need to configure logging at the <code>INFO</code> level. This is very important, if this is not configured, your output will not be generated.</p> </li> <li> <p>We instantiate a <code>Instruction</code> object which will help us handle the conversion of our function calls into a valid <code>.jsonl</code> file. We also define     the name of the <code>.jsonl</code> file in the <code>log_handlers</code> parameter</p> </li> <li> <p>We add in an <code>instructions.distil</code> annotation so that we automatically capture the input and output of the function we'd like to     fine-tune our model to output</p> </li> <li> <p>We return a <code>Pydantic</code> object which matches the annotation that we use on our function. Note that we must specify a <code>Pydantic</code> object to     be returned when using the <code>instructions.distil</code> annotation</p> </li> </ol> <p>Rate Limiting</p> <p>We recommend running this script on a small subset of the dataset first to test you've got everything configured nicely. Don't forget to add in rate limiting error handling with <code>tenacity</code> and set the <code>OPENAI_API_KEY</code> shell environment variable before running any subsequent commands</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#creating-fine-tuning-jobs","title":"Creating Fine-Tuning Jobs","text":"<p>Once we run this script, we'll have a new file called <code>generated.jsonl</code> in our local repository. Now all that's left is to run the command below to start fine-tuning your first model!</p> <pre><code>instructor jobs create-from-file generated.jsonl\n</code></pre> Finetuning Reference <p>Checking out our Finetuning CLI to learn about other hyperparameters that you can tune to improve your model's performance.</p> <p>Once the job is complete, all we need to do is to then change the annotation in the function call to <code>distil_summarization</code> in our original file above to start using our new model.</p> <pre><code>@instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\")  # (1)!\ndef distil_summarization(text: str) -&gt; GeneratedSummary:\n    summary_chain: List[str] = summarize_article(text)\n    return GeneratedSummary(summary=summary_chain[-1])\n</code></pre> <ol> <li>Don't forget to replace this with your new model id. OpenAI identifies fine tuned models with an id of    ft:gpt-3.5-turbo-0613:personal:: under their Fine-tuning tab on their dashboard <p>With that, you've now got your own fine-tuned model ready to go and serve data in production. We've seen how Instructor can make your life easier, from fine-tuning to distillation.</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#results-and-benchmarks","title":"Results and Benchmarks","text":"<p>We'll be comparing the following models in 3 ways using 20 articles that were not used for fine-tuning.</p> <ul> <li>Entity Density : This is entities per token, the higher the better for density.</li> <li>Latency : Time to last token generated in seconds</li> <li>Costs : Total cost to generate outputs - we break down the cost into training and inference costs for easy reference</li> </ul> <code>3.5 Finetuned (n)</code> <p>This is a GPT 3.5 model that we fine-tuned on <code>n</code> examples. Each model was finetuned for 4-5 epochs ( This was automatically decided by the OpenAI scheduler )</p> <code>GPT-4 (COD)</code> <p>This is a GPT4 model which we applied 3 rounds of Chain Of Density rewrites to generate a summary with using the methodology above</p> <code>GPT-3.5 (Vanilla)</code> <p>This is a GPT 3.5 model that we asked to generate entity-dense summaries which were concise. Summaries were generated in a single pass targetting about 80-90 tokens.</p> Model Mean Latency (s) Mean Entity Density 3.5 Finetuned (20) 2.1 0.15 3.5 Finetuned (50) 2.1 0.14 3.5 Finetuned (76) 2.1 0.14 GPT-3.5 (Vanilla) 16.8 0.12 GPT-4 (COD) 49.5 0.15 Finetuning Datasets <p>For our finetuned models, we did a few optimisations to raise the performance.</p> <p>We only included summaries that had a minimum density of 0.15 in the dataset, took the summary in the entire chain with the highest density as the final one, forced every regenerated summary to have a minimum density of 0.12 and regenerated summaries up to three times if they didn't meet the summaries. This is a much more expensive strategy and can cost up to 2.5x or more what we do in this tutorial</p> <p>This resulted in the total cost of $63.46 to generate just 75 examples due to the stringent requirements, translating to about $0.85 per generated summary example.</p> <p>Using the OpenAI Usage Dashboard, we can calculate the cost of generating 20 summaries as seen below.</p> Model Training Cost ($) Inference Cost ($) Tokens Used Total Cost ($) GPT-3.5 (Vanilla) - 0.20 51,162 0.2 3.5 Finetuned (20) 0.7 0.20 56,573 0.8 3.5 Finetuned (50) 1.4 0.17 49,057 1.3 3.5 Finetuned (76) 1.8 0.17 51,583 2.5 GPT-4 (COD) - 12.9 409,062 12.9 <p>Here, we can see that <code>GPT-4</code> has an approximate inference cost of <code>0.65</code> per summary while our finetuned models have an inference cost of <code>0.0091</code> per summary which is ~ <code>72x</code> cheaper.</p> <p>Interestingly, the model finetuned with the least examples seems to outperform the others. While the reason for this is unknown, a few potential reasons could be that either we didn't train for sufficient epochs ( We chose the default 5 epochs ) or that the models started learning to imitate other behaviour such as more abstract writing styles from the larger variety of samples, resulting in a decrease in entity density.</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/05/chain-of-density/#conclusions","title":"Conclusions","text":"<p>Finetuning this iterative method was 20-40x faster while improving overall performance, resulting in massive efficiency gains by finetuning and distilling capabilities into specialized models.</p> <p>We've seen how <code>Instructor</code> can make your life easier, from data modeling to distillation and finetuning. If you enjoy the content or want to try out <code>instructor</code> check out the github and don't forget to give us a star!</p>","tags":["pydantic","validation","chain of density","finetuneing","gpt-3.5-turbo","distillation"]},{"location":"blog/2023/11/18/validate-citations/","title":"Verifying LLM Citations with Pydantic","text":"<p>Ensuring the accuracy of information is crucial. This blog post explores how Pydantic's powerful and flexible validators can enhance data accuracy through citation verification.</p> <p>We'll start with using a simple substring check to verify citations. Then we'll use <code>instructor</code> itself to power an LLM to verify citations and align answers with the given citations. Finally, we'll explore how we can use these techniques to generate a dataset of accurate responses.</p>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#example-1-simple-substring-check","title":"Example 1: Simple Substring Check","text":"<p>In this example, we use the <code>Statements</code> class to verify if a given substring quote exists within a text chunk. If the substring is not found, an error is raised.</p>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#code-example","title":"Code Example:","text":"<pre><code>from typing import List\nfrom openai import OpenAI\nfrom pydantic import BaseModel, ValidationInfo, field_validator\nimport instructor\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass Statements(BaseModel):\n    body: str\n    substring_quote: str\n\n    @field_validator(\"substring_quote\")\n    @classmethod\n    def substring_quote_exists(cls, v: str, info: ValidationInfo):\n        context = info.context.get(\"text_chunks\", None)\n\n        for text_chunk in context.values():\n            if v in text_chunk:  # (1)\n                return v\n        raise ValueError(\"Could not find substring_quote `{v}` in contexts\")\n\n\nclass AnswerWithCitaton(BaseModel):\n    question: str\n    answer: List[Statements]\n</code></pre> <ol> <li>While we use a simple substring check in this example, we can use more complex techniques like regex or Levenshtein distance.</li> </ol> <p>Once the class is defined, we can use it to validate the context and raise an error if the substring is not found.</p> <pre><code>try:\n    AnswerWithCitaton.model_validate(\n        {\n            \"question\": \"What is the capital of France?\",\n            \"answer\": [\n                {\"body\": \"Paris\", \"substring_quote\": \"Paris is the capital of France\"},\n            ],\n        },\n        context={\n            \"text_chunks\": {\n                1: \"Jason is a pirate\",\n                2: \"Paris is not the capital of France\",\n                3: \"Irrelevant data\",\n            }\n        },\n    )\nexcept ValidationError as e:\n    print(e)\n</code></pre>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#error-message-example","title":"Error Message Example:","text":"<pre><code>answer.0.substring_quote\n  Value error, Could not find substring_quote `Paris is the capital of France` in contexts [type=value_error, input_value='Paris is the capital of France', input_type=str]\n    For further information visit [https://errors.pydantic.dev/2.4/v/value_error](https://errors.pydantic.dev/2.4/v/value_error)\n</code></pre> <p>Pydantic raises a validation error when the <code>substring_quote</code> attribute does not exist in the context. This approach can be used to validate more complex data using techniques like regex or Levenshtein distance.</p>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#example-2-using-llm-for-verification","title":"Example 2: Using LLM for Verification","text":"<p>This approach leverages OpenAI's LLM to validate citations. If the citation does not exist in the context, the LLM returns an error message.</p>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#code-example_1","title":"Code Example:","text":"<pre><code>class Validation(BaseModel):\n    is_valid: bool\n    error_messages: Optional[str] = Field(None, description=\"Error messages if any\")\n\n\nclass Statements(BaseModel):\n    body: str\n    substring_quote: str\n\n    @model_validator(mode=\"after\")\n    def substring_quote_exists(self, info: ValidationInfo):\n        context = info.context.get(\"text_chunks\", None)\n\n        resp: Validation = client.chat.completions.create(\n            response_model=Validation,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Does the following citation exist in the following context?\\n\\nCitation: {self.substring_quote}\\n\\nContext: {context}\",\n                }\n            ],\n            model=\"gpt-3.5-turbo\",\n        )\n\n        if resp.is_valid:\n            return self\n\n        raise ValueError(resp.error_messages)\n\n\nclass AnswerWithCitaton(BaseModel):\n    question: str\n    answer: List[Statements]\n</code></pre> <p>Now when we use a correct citation, the LLM returns a valid response.</p> <pre><code>resp = AnswerWithCitaton.model_validate(\n    {\n        \"question\": \"What is the capital of France?\",\n        \"answer\": [\n            {\"body\": \"Paris\", \"substring_quote\": \"Paris is the capital of France\"},\n        ],\n    },\n    context={\n        \"text_chunks\": {\n            1: \"Jason is a pirate\",\n            2: \"Paris is the capital of France\",\n            3: \"Irrelevant data\",\n        }\n    },\n)\nprint(resp.model_dump_json(indent=2))\n</code></pre>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#result","title":"Result:","text":"<pre><code>{\n  \"question\": \"What is the capital of France?\",\n  \"answer\": [\n    {\n      \"body\": \"Paris\",\n      \"substring_quote\": \"Paris is the capital of France\"\n    }\n  ]\n}\n</code></pre> <p>When we have citations that don't exist in the context, the LLM returns an error message.</p> <pre><code>try:\n    AnswerWithCitaton.model_validate(\n        {\n            \"question\": \"What is the capital of France?\",\n            \"answer\": [\n                {\"body\": \"Paris\", \"substring_quote\": \"Paris is the capital of France\"},\n            ],\n        },\n        context={\n            \"text_chunks\": {\n                1: \"Jason is a pirate\",\n                2: \"Paris is not the capital of France\",\n                3: \"Irrelevant data\",\n            }\n        },\n    )\nexcept ValidationError as e:\n    print(e)\n</code></pre>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#error-message-example_1","title":"Error Message Example:","text":"<pre><code>1 validation error for AnswerWithCitaton\nanswer.0\n  Value error, Citation not found in context [type=value_error, input_value={'body': 'Paris', 'substr... the capital of France'}, input_type=dict]\n    For further information visit [https://errors.pydantic.dev/2.4/v/value_error](https://errors.pydantic.dev/2.4/v/value_error)\n</code></pre>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#example-3-aligning-citations-and-answers","title":"Example 3: Aligning Citations and Answers","text":"<p>In this example, we ensure that the provided answers are aligned with the given citations and context. The LLM is used to verify the alignment.</p> <p>We use the same <code>Statements</code> model as above, but we add a new model for the answer that also verifies the alignment of citations.</p>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#code-example_2","title":"Code Example:","text":"<pre><code>class AnswerWithCitaton(BaseModel):\n    question: str\n    answer: List[Statements]\n\n    @model_validator(mode=\"after\")\n    def validate_answer(self, info: ValidationInfo):\n        context = info.context.get(\"text_chunks\", None)\n\n        resp: Validation = client.chat.completions.create(\n            response_model=Validation,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Does the following answers match the question and the context?\\n\\nQuestion: {self.question}\\n\\nAnswer: {self.answer}\\n\\nContext: {context}\",\n                }\n            ],\n            model=\"gpt-3.5-turbo\",\n        )\n\n        if resp.is_valid:\n            return self\n\n        raise ValueError(resp.error_messages)\n</code></pre> <p>When we have a mismatch between the answer and the citation, the LLM returns an error message.</p> <pre><code>try:\n    AnswerWithCitaton.model_validate(\n        {\n            \"question\": \"What is the capital of France?\",\n            \"answer\": [\n                {\"body\": \"Texas\", \"substring_quote\": \"Paris is the capital of France\"},\n            ],\n        },\n        context={\n            \"text_chunks\": {\n                1: \"Jason is a pirate\",\n                2: \"Paris is the capital of France\",\n                3: \"Irrelevant data\",\n            }\n        },\n    )\nexcept ValidationError as e:\n    print(e)\n</code></pre>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#error-message-example_2","title":"Error Message Example:","text":"<pre><code>1 validation error for AnswerWithCitaton\n  Value error, The answer does not match the question and context [type=value_error, input_value={'question': 'What is the...he capital of France'}]}, input_type=dict]\n    For further information visit [https://errors.pydantic.dev/2.4/v/value_error](https://errors.pydantic.dev/2.4/v/value_error)\n</code></pre>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2023/11/18/validate-citations/#conclusion","title":"Conclusion","text":"<p>These examples demonstrate the potential of using Pydantic and OpenAI to enhance data accuracy through citation verification. While the LLM-based approach may not be efficient for runtime operations, it has exciting implications for generating a dataset of accurate responses. By leveraging this method during data generation, we can fine-tune a model that excels in citation accuracy. Similar to our last post on finetuning a better summarizer.</p> <p>If you like the content check out our GitHub as give us a star and checkout the library.</p>","tags":["pydantic","validation","finetuneing","citations","hallucination"]},{"location":"blog/2024/02/14/weights-and-biases-course/","title":"Free course on Weights and Biases","text":"<p>I just released a free course on wits and biases. It goes over the material from tutorial. Check it out at wandb.courses its free and open to everyone and just under an hour long!</p> <p></p> <p>Click the image to access the course</p>","tags":["open source"]},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/","title":"Enhancing Python Functions with Instructor: A Guide to Fine-Tuning and Distillation","text":""},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#introduction","title":"Introduction","text":"<p>Get ready to dive deep into the world of fine-tuning task specific language models with Python functions. We'll explore how the <code>instructor.instructions</code> streamlines this process, making the task you want to distil more efficient and powerful while preserving its original functionality and backwards compatibility.</p> <p>If you want to see the full example checkout examples/distillation</p>"},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#why-use-instructor","title":"Why use Instructor?","text":"<p>Imagine you're developing a backend service that uses a mix old and new school ML practises, it may involve pipelines with multiple function calls, validations, and data processing. Sounds cumbersome, right? That's where <code>Instructor</code> comes in. It simplifies complex procedures, making them more efficient and easier to manage by adding a decorator to your function that will automatically generate a dataset for fine-tuning and help you swap out the function implementation.</p>"},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#quick-start-how-to-use-instructors-distillation-feature","title":"Quick Start: How to Use Instructor's Distillation Feature","text":"<p>Before we dig into the nitty-gritty, let's look at how easy it is to use Instructor's distillation feature to use function calling finetuning to export the data to a JSONL file.</p> <pre><code>import logging\nimport random\nfrom pydantic import BaseModel\nfrom instructor import Instructions  # pip install instructor\n\n# Logging setup\nlogging.basicConfig(level=logging.INFO)\n\ninstructions = Instructions(\n    name=\"three_digit_multiply\",\n    finetune_format=\"messages\",\n    # log handler is used to save the data to a file\n    # you can imagine saving it to a database or other storage\n    # based on your needs!\n    log_handlers=[logging.FileHandler(\"math_finetunes.jsonl\")],\n)\n\n\nclass Multiply(BaseModel):\n    a: int\n    b: int\n    result: int\n\n\n# Define a function with distillation\n# The decorator will automatically generate a dataset for fine-tuning\n# They must return a pydantic model to leverage function calling\n@instructions.distil\ndef fn(a: int, b: int) -&gt; Multiply:\n    resp = a * b\n    return Multiply(a=a, b=b, result=resp)\n\n\n# Generate some data\nfor _ in range(10):\n    a = random.randint(100, 999)\n    b = random.randint(100, 999)\n    print(fn(a, b))\n    #&gt; a=873 b=234 result=204282\n    #&gt; a=902 b=203 result=183106\n    #&gt; a=962 b=284 result=273208\n    #&gt; a=491 b=739 result=362849\n    #&gt; a=193 b=400 result=77200\n    #&gt; a=300 b=448 result=134400\n    #&gt; a=952 b=528 result=502656\n    #&gt; a=574 b=797 result=457478\n    #&gt; a=482 b=204 result=98328\n    #&gt; a=781 b=278 result=217118\n</code></pre>"},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#the-intricacies-of-fine-tuning-language-models","title":"The Intricacies of Fine-tuning Language Models","text":"<p>Fine-tuning isn't just about writing a function like <code>def f(a, b): return a * b</code>. It requires detailed data preparation and logging. However, Instructor provides a built-in logging feature and structured outputs to simplify this.</p>"},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#why-instructor-and-distillation-are-game-changers","title":"Why Instructor and Distillation are Game Changers","text":"<p>The library offers two main benefits:</p> <ol> <li>Efficiency: Streamlines functions, distilling requirements into model weights and a few lines of code.</li> <li>Integration: Eases combining classical machine learning and language models by providing a simple interface that wraps existing functions.</li> </ol>"},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#role-of-instructor-in-simplifying-fine-tuning","title":"Role of Instructor in Simplifying Fine-Tuning","text":"<p>The <code>from instructor import Instructions</code> feature is a time saver. It auto-generates a fine-tuning dataset, making it a breeze to imitate a function's behavior.</p>"},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#logging-output-and-running-a-finetune","title":"Logging Output and Running a Finetune","text":"<p>Here's how the logging output would look:</p> <pre><code>{\n    \"messages\": [\n        {\"role\": \"system\", \"content\": 'Predict the results of this function: ...'},\n        {\"role\": \"user\", \"content\": 'Return fn(133, b=539)'},\n        {\n            \"role\": \"assistant\",\n            \"function_call\": {\n                \"name\": \"Multiply\",\n                \"arguments\": '{\"a\":133,\"b\":539,\"result\":89509}',\n            },\n        },\n    ],\n    \"functions\": [\n        {\"name\": \"Multiply\", \"description\": \"Correctly extracted `Multiply`...\"}\n    ],\n}\n</code></pre> <p>Run a finetune like this:</p> <p>Don't forget to set your OpenAI Key as an environment variable</p> <p>All of the <code>instructor jobs</code> commands assume you've set an environment variable of <code>OPENAI_API_KEY</code> in your shell. You can set this by running the command <code>export OPENAI_API_KEY=&lt;Insert API Key Here&gt;</code> in your shell</p> <pre><code>instructor jobs create-from-file math_finetunes.jsonl\n</code></pre>"},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#next-steps-and-future-plans","title":"Next Steps and Future Plans","text":"<p>Here's a sneak peek of what I'm planning:</p> <pre><code>from instructor import Instructions, patch\n\npatch()  # (1)!\n\n\nclass Multiply(BaseModel):\n    a: int\n    b: int\n    result: int\n\n\ninstructions = Instructions(\n    name=\"three_digit_multiply\",\n)\n\n\n@instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\")  # (2)!\ndef fn(a: int, b: int) -&gt; Multiply:\n    resp = a + b\n    return Multiply(a=a, b=b, result=resp)\n</code></pre> <ol> <li> <p>Don't forget to run the <code>patch()</code> command that we provide with the <code>Instructor</code> package. This helps     automatically serialize the content back into the `Pydantic`` model that we're looking for.</p> </li> <li> <p>Don't forget to replace this with your new model id. OpenAI identifies fine tuned models with an id     of <code>ft:gpt-3.5-turbo-0613:personal::&lt;id&gt;</code> under their Fine-tuning tab on their dashboard</p> </li> </ol> <p>With this, you can swap the function implementation, making it backward compatible. You can even imagine using the different models for different tasks or validating and runnign evals by using the original function and comparing it to the distillation.</p>"},{"location":"blog/2023/10/17/enhancing-python-functions-with-instructor-a-guide-to-fine-tuning-and-distillation/#conclusion","title":"Conclusion","text":"<p>We've seen how <code>Instructor</code> can make your life easier, from fine-tuning to distillation. Now if you're thinking wow, I'd love a backend service to do this for continously, you're in luck! Please check out the survey at useinstructor.com and let us know who you are.</p> <p>If you enjoy the content or want to try out <code>instructor</code> please check out the github and give us a star!</p>"},{"location":"blog/2024/03/08/simple-synthetic-data-generation/","title":"Simple Synthetic Data Generation","text":"<p>What that people have been using instructor for is to generate synthetic data rather than extracting data itself. We can even use the J-Schemo extra fields to give specific examples to control how we generate data. </p> <p>Consider the example below. We'll likely generate very simple names.</p> <pre><code>from typing import Iterable\nfrom pydantic import BaseModel\nimport instructor\nfrom openai import OpenAI\n\n\n# Define the UserDetail model\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n# Patch the OpenAI client to enable the response_model functionality\nclient = instructor.from_openai(OpenAI())\n\n\ndef generate_fake_users(count: int) -&gt; Iterable[UserDetail]:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Iterable[UserDetail],\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Generate a {count} synthetic users\"},\n        ],\n    )\n\n\nfor user in generate_fake_users(5):\n    print(user)\n    \"\"\"\n    name='Alice' age=25\n    name='Bob' age=30\n    name='Charlie' age=35\n    name='David' age=40\n    name='Eve' age=45\n    \"\"\"\n</code></pre>"},{"location":"blog/2024/03/08/simple-synthetic-data-generation/#leveraging-simple-examples","title":"Leveraging Simple Examples","text":"<p>We might want to set examples as part of the prompt by leveraging Pydantics configuration. We can set examples directly in the JSON scheme itself.</p> <pre><code>from typing import Iterable\nfrom pydantic import BaseModel\nimport instructor\nfrom openai import OpenAI\n\n\n# Define the UserDetail model\nclass UserDetail(BaseModel):\n    name: str = Field(examples=[\"Timothee Chalamet\", \"Zendaya\"])\n    age: int\n\n\n# Patch the OpenAI client to enable the response_model functionality\nclient = instructor.from_openai(OpenAI())\n\n\ndef generate_fake_users(count: int) -&gt; Iterable[UserDetail]:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Iterable[UserDetail],\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Generate a {count} synthetic users\"},\n        ],\n    )\n\n\nfor user in generate_fake_users(5):\n    print(user)\n    \"\"\"\n    name='Timothee Chalamet' age=25\n    name='Zendaya' age=24\n    name='Keanu Reeves' age=56\n    name='Scarlett Johansson' age=36\n    name='Chris Hemsworth' age=37\n    \"\"\"\n</code></pre> <p>By incorporating names of celebrities as examples, we have shifted towards generating synthetic data featuring well-known personalities, moving away from the simplistic, single-word names previously used.</p>"},{"location":"blog/2024/03/08/simple-synthetic-data-generation/#leveraging-complex-example","title":"Leveraging Complex Example","text":"<p>To effectively generate synthetic examples with more nuance, lets upgrade to the \"gpt-4-turbo-preview\" model, use model level examples rather than attribute level examples:</p> <pre><code>import instructor\n\nfrom typing import Iterable\nfrom pydantic import BaseModel, Field, ConfigDict\nfrom openai import OpenAI\n\n\n# Define the UserDetail model\nclass UserDetail(BaseModel):\n    \"\"\"Old Wizards\"\"\"\n    name: str\n    age: int\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"name\": \"Gandalf the Grey\", \"age\": 1000},\n                {\"name\": \"Albus Dumbledore\", \"age\": 150},\n            ]\n        }\n    )\n\n\n# Patch the OpenAI client to enable the response_model functionality\nclient = instructor.from_openai(OpenAI())\n\n\ndef generate_fake_users(count: int) -&gt; Iterable[UserDetail]:\n    return client.chat.completions.create(\n        model=\"gpt-4-turbo-preview\",\n        response_model=Iterable[UserDetail],\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Generate `{count}` synthetic examples\"},\n        ],\n    )\n\n\nfor user in generate_fake_users(5):\n    print(user)\n    \"\"\"\n    name='Merlin' age=196\n    name='Saruman the White' age=543\n    name='Radagast the Brown' age=89\n    name='Morgoth' age=901\n    name='Filius Flitwick' age=105 \n    \"\"\"\n</code></pre>"},{"location":"blog/2024/03/08/simple-synthetic-data-generation/#leveraging-descriptions","title":"Leveraging Descriptions","text":"<p>By adjusting the descriptions within our Pydantic models, we can subtly influence the nature of the synthetic data generated. This method allows for a more nuanced control over the output, ensuring that the generated data aligns more closely with our expectations or requirements. </p> <p>For instance, specifying \"Fancy French sounding names\" as a description for the <code>name</code> field in our <code>UserDetail</code> model directs the generation process to produce names that fit this particular criterion, resulting in a dataset that is both diverse and tailored to specific linguistic characteristics.</p> <pre><code>import instructor\n\nfrom typing import Iterable\nfrom pydantic import BaseModel, Field\nfrom openai import OpenAI\n\n\n# Define the UserDetail model\nclass UserDetail(BaseModel):\n    name: str = Field(description=\"Fancy French sounding names\")\n    age: int\n\n\n# Patch the OpenAI client to enable the response_model functionality\nclient = instructor.from_openai(OpenAI())\n\n\ndef generate_fake_users(count: int) -&gt; Iterable[UserDetail]:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Iterable[UserDetail],\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Generate `{count}` synthetic users\"},\n        ],\n    )\n\n\nfor user in generate_fake_users(5):\n    print(user)\n    \"\"\"\n    name='Jean' age=25\n    name='Claire' age=30\n    name='Pierre' age=22\n    name='Marie' age=27\n    name='Luc' age=35\n    \"\"\"\n</code></pre>"},{"location":"blog/2024/05/03/fastapi-open-telemetry-and-instructor/","title":"Why Logfire is a perfect fit for FastAPI + Instructor","text":"<p>Logfire is a new tool that provides key insight into your application with Open Telemtry. Instead of using ad-hoc print statements, Logfire helps to profile every part of your application and is integrated directly into Pydantic and FastAPI, two popular libraries amongst Instructor users.</p> <p>In short, this is the secret sauce to help you get your application to the finish line and beyond. We'll show you how to easily integrate Logfire into FastAPI, one of the most popular choices amongst users of Instructor using two examples</p> <ol> <li>Data Extraction from a single User Query</li> <li>Using <code>asyncio</code> to process multiple users in parallel</li> <li>Streaming multiple objects using an <code>Iterable</code> so that they're avaliable on demand</li> </ol> <p>As usual, all of the code that we refer to here is provided in examples/logfire-fastapi for you to use in your projects.</p> Configure Logfire <p>Before starting this tutorial, make sure that you've registered for a Logfire account. You'll also need to create a project to track these logs. Lastly, in order to see the request body, you'll also need to configure the default log level to <code>debug</code> instead of the default <code>info</code> on the dashboard console.</p> <p>Make sure to create a virtual environment and install all of the packages inside the <code>requirements.txt</code> file at examples/logfire-fastapi.</p>"},{"location":"blog/2024/05/03/fastapi-open-telemetry-and-instructor/#data-extraction","title":"Data Extraction","text":"<p>Let's start by trying to extract some user information given a user query. We can do so with a simple Pydantic model as seen below.</p> <pre><code>from pydantic import BaseModel\nfrom fastapi import FastAPI\nfrom openai import AsyncOpenAI\nimport instructor\n\n\nclass UserData(BaseModel):\n    query: str\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\napp = FastAPI()\nclient = instructor.from_openai(AsyncOpenAI())\n\n\n@app.post(\"/user\", response_model=UserDetail)\nasync def endpoint_function(data: UserData) -&gt; UserDetail:\n    user_detail = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Extract: `{data.query}`\"},\n        ],\n    )\n\n    return user_detail\n</code></pre> <p>This simple endpoint takes in a user query and extracts out a user from the statement. Let's see how we can add in Logfire into this endpoint with just a few lines of code</p> <pre><code>from pydantic import BaseModel\nfrom fastapi import FastAPI\nfrom openai import AsyncOpenAI\nimport instructor\nimport logfire #(1)!\n\n\nclass UserData(BaseModel):\n    query: str\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\napp = FastAPI()\nopenai_client = AsyncOpenAI() #(2)!\nlogfire.configure(pydantic_plugin=logfire.PydanticPlugin(record=\"all\"))\nlogfire.instrument_openai(openai_client)\nlogfire.instrument_fastapi(app)\nclient = instructor.from_openai(openai_client)\n\n\n@app.post(\"/user\", response_model=UserDetail)\nasync def endpoint_function(data: UserData) -&gt; UserDetail:\n    user_detail = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Extract: `{data.query}`\"},\n        ],\n    )\n\n    return user_detail\n</code></pre> <ol> <li>Import in the logfire package</li> <li>Setup logging using their native integrations with FastAPI and OpenAI</li> </ol> <p>With just those few lines of code, we've got ourselves a working integration with Logfire. When we call our endpoint at <code>/user</code> with the following payload, everything is immediately logged in the console.</p> <pre><code>curl -X 'POST' \\\n  'http://localhost:8000/user' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"query\": \"Daniel is a 24 year man living in New York City\"\n}'\n</code></pre> <p>We can see that Pydantic has nicely logged for us the validation result of our openai call here. Just right above, we also have the result of the OpenAI call.</p> <p></p> <p>We've also got full visibility into the arguments that were passed into the endpoint when we called it. This is extremely useful for users when they eventually want to reproduce errors in production locally.</p> <p></p>"},{"location":"blog/2024/05/03/fastapi-open-telemetry-and-instructor/#using-asyncio","title":"Using Asyncio","text":"<p>Sometimes, we might need to run multiple jobs in parallel. Let's see how we can take advantage of <code>asyncio</code> so that we can speed up our operations. We can do so by adding the following bits of code to our previous file.</p> What is Asyncio? <p>For a deeper guide into how to work with Asycnio, see our previous guide here.</p> New CodeFull File <pre><code>import asyncio\n\nclass MultipleUserData(BaseModel):\n    queries: list[str]\n\n@app.post(\"/many-users\", response_model=list[UserDetail])\nasync def extract_many_users(data: MultipleUserData):\n    async def extract_user(query: str):\n        user_detail = await client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            response_model=UserDetail,\n            messages=[\n                {\"role\": \"user\", \"content\": f\"Extract: `{query}`\"},\n            ],\n        )\n        logfire.info(\"/User returning\", value=user_detail)\n        return user_detail\n\n    coros = [extract_user(query) for query in data.queries]\n    return await asyncio.gather(*coros)\n</code></pre> <pre><code>from pydantic import BaseModel\nfrom fastapi import FastAPI\nfrom openai import AsyncOpenAI\nimport instructor\nimport logfire\nfrom collections.abc import Iterable\nfrom fastapi.responses import StreamingResponse\nimport asyncio\n\n\nclass UserData(BaseModel):\n    query: str\n\n\nclass MultipleUserData(BaseModel):\n    queries: list[str]\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\napp = FastAPI()\nopenai_client = AsyncOpenAI()\nlogfire.configure(pydantic_plugin=logfire.PydanticPlugin(record=\"all\"))\nlogfire.instrument_openai(openai_client)\nlogfire.instrument_fastapi(app)\nclient = instructor.from_openai(openai_client)\n\n\n@app.post(\"/user\", response_model=UserDetail)\nasync def endpoint_function(data: UserData) -&gt; UserDetail:\n    user_detail = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Extract: `{data.query}`\"},\n        ],\n    )\n    logfire.info(\"/User returning\", value=user_detail)\n    return user_detail\n\n\n@app.post(\"/many-users\", response_model=list[UserDetail])\nasync def extract_many_users(data: MultipleUserData):\n    async def extract_user(query: str):\n        user_detail = await client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            response_model=UserDetail,\n            messages=[\n                {\"role\": \"user\", \"content\": f\"Extract: `{query}`\"},\n            ],\n        )\n        logfire.info(\"/User returning\", value=user_detail)\n        return user_detail\n\n    coros = [extract_user(query) for query in data.queries]\n    return await asyncio.gather(*coros)\n</code></pre> <p>We can call this endpoint with a simple <code>curl</code> call</p> <pre><code>curl -X 'POST' \\\n  'http://localhost:8000/many-users' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"queries\": [\n    \"Daniel is a 34 year man in New York City\",\"Sarah is a 20 year old living in Tokyo\", \"Jeffrey is 55 and lives down in Leeds\"\n  ]\n}'\n</code></pre> <p>This is all logged in Logfire as seen below. We have complete visiblity into the performance of our entire application and it's pretty clear that a large chunk of the latency is taken up by the OpenAI Call.</p> <p>We could also potentially separate the logs into more graunular levels by creating a new span for each instance of <code>extract_user</code> created.</p> <p></p>"},{"location":"blog/2024/05/03/fastapi-open-telemetry-and-instructor/#streaming","title":"Streaming","text":"<p>Now let's see how we can take advantage of Instructor's <code>Iterable</code> support to stream multiple instances of an extracted object. This is extremely useful for application where speed is crucial and users want to get the results quickly.</p> <p>Let's add a new endpoint to our server to see how this might work</p> New CodeFull File <pre><code>import asyncio\nfrom collections.abc import Iterable\nfrom fastapi.responses import StreamingResponse\n\nclass MultipleUserData(BaseModel):\n    queries: list[str]\n\n@app.post(\"/extract\", response_class=StreamingResponse)\nasync def extract(data: UserData):\n    supressed_client = AsyncOpenAI()\n    logfire.instrument_openai(supressed_client, suppress_other_instrumentation=False) #(1)!\n    client = instructor.from_openai(supressed_client)\n    users = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Iterable[UserDetail],\n        stream=True,\n        messages=[\n            {\"role\": \"user\", \"content\": data.query},\n        ],\n    )\n\n    async def generate():\n        with logfire.span(\"Generating User Response Objects\"):\n            async for user in users:\n                resp_json = user.model_dump_json()\n                logfire.info(\"Returning user object\", value=resp_json)\n\n                yield resp_json\n\n    return StreamingResponse(generate(), media_type=\"text/event-stream\")\n</code></pre> <ol> <li>Note that we supress instrumentation to print out the stream objects. This has to do with the parsing of partials in Instructor.</li> </ol> <pre><code>from pydantic import BaseModel\nfrom fastapi import FastAPI\nfrom openai import AsyncOpenAI\nimport instructor\nimport logfire\nimport asyncio\nfrom collections.abc import Iterable\nfrom fastapi.responses import StreamingResponse\n\n\nclass UserData(BaseModel):\n    query: str\n\n\nclass MultipleUserData(BaseModel):\n    queries: list[str]\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\napp = FastAPI()\nopenai_client = AsyncOpenAI()\nlogfire.configure(pydantic_plugin=logfire.PydanticPlugin(record=\"all\"))\nlogfire.instrument_fastapi(app)\nlogfire.instrument_openai(openai_client)\nclient = instructor.from_openai(openai_client)\n\n\n@app.post(\"/user\", response_model=UserDetail)\nasync def endpoint_function(data: UserData) -&gt; UserDetail:\n    user_detail = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Extract: `{data.query}`\"},\n        ],\n    )\n    logfire.info(\"/User returning\", value=user_detail)\n    return user_detail\n\n\n@app.post(\"/many-users\", response_model=list[UserDetail])\nasync def extract_many_users(data: MultipleUserData):\n    async def extract_user(query: str):\n        user_detail = await client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            response_model=UserDetail,\n            messages=[\n                {\"role\": \"user\", \"content\": f\"Extract: `{query}`\"},\n            ],\n        )\n        logfire.info(\"/User returning\", value=user_detail)\n        return user_detail\n\n    coros = [extract_user(query) for query in data.queries]\n    return await asyncio.gather(*coros)\n\n\n@app.post(\"/extract\", response_class=StreamingResponse)\nasync def extract(data: UserData):\n    supressed_client = AsyncOpenAI()\n    logfire.instrument_openai(supressed_client, suppress_other_instrumentation=False)\n    client = instructor.from_openai(supressed_client)\n    users = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Iterable[UserDetail],\n        stream=True,\n        messages=[\n            {\"role\": \"user\", \"content\": data.query},\n        ],\n    )\n\n    async def generate():\n        with logfire.span(\"Generating User Response Objects\"):\n            async for user in users:\n                resp_json = user.model_dump_json()\n                logfire.info(\"Returning user object\", value=resp_json)\n\n                yield resp_json\n\n    return StreamingResponse(generate(), media_type=\"text/event-stream\")\n</code></pre> <p>We can call and log out the stream returned using the <code>requests</code> library and using the <code>iter_content</code> method</p> <pre><code>import requests\n\nresponse = requests.post(\n    \"http://127.0.0.1:3000/extract\",\n    json={\n        \"query\": \"Alice and Bob are best friends. They are currently 32 and 43 respectively. \"\n    },\n    stream=True,\n)\n\nfor chunk in response.iter_content(chunk_size=1024):\n    if chunk:\n        print(str(chunk, encoding=\"utf-8\"), end=\"\\n\")\n</code></pre> <p>This gives us the output of</p> <pre><code>{\"name\":\"Alice\",\"age\":32}\n{\"name\":\"Bob\",\"age\":43}\n</code></pre> <p>We can also see the individual stream objects inside the Logfire dashboard as seen below. Note that we've grouped the generated logs inside a span of its own for easy logging.</p> <p></p>"},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/","title":"Generators and LLM Streaming","text":"<p>Latency is crucial, especially in eCommerce and newer chat applications like ChatGPT. Streaming is the solution that enables us to enhance the user experience without the need for faster response times.</p> <p>And what makes streaming possible? Generators!</p> <p>In this post, we're going to dive into the cool world of Python generators \u2014 these tools are more than just a coding syntax trick. We'll explore Python generators from the ground up and then delve into LLM streaming using the Instructor library.</p>"},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#python-generators-an-efficient-approach-to-iterables","title":"Python Generators: An Efficient Approach to Iterables","text":"<p>Generators in Python are a game-changer for handling large data sets and stream processing. They allow functions to yield values one at a time, pausing and resuming their state, which is a faster and more memory-efficient approach compared to traditional collections that store all elements in memory.</p>"},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#the-basics-yielding-values","title":"The Basics: Yielding Values","text":"<p>A generator function in Python uses the <code>yield</code> keyword. It yields values one at a time, allowing the function to pause and resume its state.</p> <pre><code>def count_to_3():\n    yield 1\n    yield 2\n    yield 3\n\n\nfor num in count_to_3():\n    print(num)\n    #&gt; 1\n    #&gt; 2\n    #&gt; 3\n</code></pre> <pre><code>1\n2\n3\n</code></pre>"},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#advantages-over-traditional-collections","title":"Advantages Over Traditional Collections","text":"<ul> <li>Lazy Evaluation &amp; reduced latency: The time to get the first element (or time-to-first-token in LLM land) from a generator is significantly lower. Generators only produce one value at a time, whereas accessing the first element of a collection will require that the whole collection be created first.</li> <li>Memory Efficiency: Only one item is in memory at a time.</li> <li>Maintain State: Automatically maintains state between executions.</li> </ul> <p>Let's see how much faster generators are and where they really shine:</p> <pre><code>import time\n\n\ndef expensive_func(x):\n    \"\"\"Simulate an expensive operation.\"\"\"\n    time.sleep(1)\n    return x**2\n\n\ndef calculate_time_for_first_result_with_list(func_input, func):\n    \"\"\"Calculate using a list comprehension and return the first result with its computation time.\"\"\"\n    start_perf = time.perf_counter()\n    result = [func(x) for x in func_input][0]\n    end_perf = time.perf_counter()\n    print(f\"Time for first result (list): {end_perf - start_perf:.2f} seconds\")\n    #&gt; Time for first result (list): 5.02 seconds\n    return result\n\n\ndef calculate_time_for_first_result_with_generator(func_input, func):\n    \"\"\"Calculate using a generator and return the first result with its computation time.\"\"\"\n    start_perf = time.perf_counter()\n    result = next(func(x) for x in func_input)\n    end_perf = time.perf_counter()\n    print(f\"Time for first result (generator): {end_perf - start_perf:.2f} seconds\")\n    #&gt; Time for first result (generator): 1.01 seconds\n    return result\n\n\n# Prepare inputs for the function\nnumbers = [1, 2, 3, 4, 5]\n\n# Benchmarking\nfirst_result_list = calculate_time_for_first_result_with_list(numbers, expensive_func)\nfirst_result_gen = calculate_time_for_first_result_with_generator(\n    numbers, expensive_func\n)\n</code></pre> <pre><code>Time for first result (list): 5.02 seconds\nTime for first result (generator): 1.01 seconds\n</code></pre> <p>The generator computes one expensive operation and returns the first result immediately, while the list comprehension computes the expensive operation for all elements in the list before returning the first result.</p>"},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#generator-expressions-a-shortcut","title":"Generator Expressions: A Shortcut","text":"<p>Python also allows creating generators in a single line of code, known as generator expressions. They are syntactically similar to list comprehensions but use parentheses.</p> <pre><code>squares = (x * x for x in range(10))\n</code></pre>"},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#use-cases-in-real-world-applications","title":"Use Cases in Real-World Applications","text":"<p>Generators shine in scenarios like reading large files, data streaming (eg. llm token streaming), and pipeline creation for data processing.</p>"},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#llm-streaming","title":"LLM Streaming","text":"<p>If you've used ChatGPT, you'll see that the tokens are streamed out one by one, instead of the full response being shown at the end (can you imagine waiting for the full response??). This is made possible by generators.</p> <p>Here's how a vanilla openai generator looks:</p> <pre><code>from openai import OpenAI\n\n# Set your OpenAI API key\nclient = OpenAI(\n    api_key=\"My API Key\",\n)\n\nresponse_generator = client.chat.completions.create(\n    model='gpt-3.5-turbo',\n    messages=[{'role': 'user', 'content': \"What are some good reasons to smile?\"}],\n    temperature=0,\n    stream=True,\n)\n\nfor chunk in response_generator:\n    print(chunk.choices[0].delta.content, end=\"\")\n</code></pre> <p>This is great, but what if we want to do some structured extraction on this stream? For instance, we might want to render frontend components based on product rankings that are streamed out by an LLM.</p> <p>Should we wait for the entire stream to finish before extracting &amp; validating the list of components or can we extract &amp; validate the components in real time as they are streamed?</p> <p>In e-commerce, every millisecond matters so the time-to-first-render can differentiate a successful and not-so-successful e commerce store (and i know how a failing e commerce store feels :/ ).</p> <p>Let's see how we can use Instructor to handle extraction from this real time stream!</p>"},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#e-commerce-product-ranking","title":"E-commerce Product Ranking","text":""},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#scenario","title":"Scenario","text":"<p>Imagine an e-commerce platform where we have:</p> <p>\u2022 a customer profile: this includes a detailed history of purchases, browsing behavior, product ratings, preferences in various categories, search history, and even responses to previous recommendations. This extensive data is crucial for generating highly personalized and relevant product suggestions.</p> <p>\u2022 a list of candidate products: these could be some shortlisted products we think the customer would like.</p> <p>Our goal is to re-rerank these candidate products for the best conversion and we'll use an LLM!</p>"},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#stream-processing","title":"Stream Processing","text":"<p>User Data:</p> <p>Let's assume we have the following user profile:</p> <pre><code>profile_data = \"\"\"\nCustomer ID: 12345\nRecent Purchases: [Laptop, Wireless Headphones, Smart Watch]\nFrequently Browsed Categories: [Electronics, Books, Fitness Equipment]\nProduct Ratings: {Laptop: 5 stars, Wireless Headphones: 4 stars}\nRecent Search History: [best budget laptops 2023, latest sci-fi books, yoga mats]\nPreferred Brands: [Apple, AllBirds, Bench]\nResponses to Previous Recommendations: {Philips: Not Interested, Adidas: Not Interested}\nLoyalty Program Status: Gold Member\nAverage Monthly Spend: $500\nPreferred Shopping Times: Weekend Evenings\n...\n\"\"\"\n</code></pre> <p>We want to rank the following products for this user:</p> <pre><code>products = [\n    {\n        \"product_id\": 1,\n        \"product_name\": \"Apple MacBook Air (2023) - Latest model, high performance, portable\",\n    },\n    {\n        \"product_id\": 2,\n        \"product_name\": \"Sony WH-1000XM4 Wireless Headphones - Noise-canceling, long battery life\",\n    },\n    {\n        \"product_id\": 3,\n        \"product_name\": \"Apple Watch Series 7 - Advanced fitness tracking, seamless integration with Apple ecosystem\",\n    },\n    {\n        \"product_id\": 4,\n        \"product_name\": \"Kindle Oasis - Premium e-reader with adjustable warm light\",\n    },\n    {\n        \"product_id\": 5,\n        \"product_name\": \"AllBirds Wool Runners - Comfortable, eco-friendly sneakers\",\n    },\n    {\n        \"product_id\": 6,\n        \"product_name\": \"Manduka PRO Yoga Mat - High-quality, durable, eco-friendly\",\n    },\n    {\n        \"product_id\": 7,\n        \"product_name\": \"Bench Hooded Jacket - Stylish, durable, suitable for outdoor activities\",\n    },\n    {\n        \"product_id\": 8,\n        \"product_name\": \"GoPro HERO9 Black - 5K video, waterproof, for action photography\",\n    },\n    {\n        \"product_id\": 9,\n        \"product_name\": \"Nespresso Vertuo Next Coffee Machine - Quality coffee, easy to use, compact design\",\n    },\n    {\n        \"product_id\": 10,\n        \"product_name\": \"Project Hail Mary by Andy Weir - Latest sci-fi book from a renowned author\",\n    },\n]\n</code></pre> <p>Let's now define our models for structured extraction. Note: instructor will conveniently let us use <code>Iterable</code> to model an iterable of our class. In this case, once we define our product recommendation model, we can slap on <code>Iterable</code> to define what we ultimately want - a (ranked) list of product recommendations.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.function_calls.Mode.JSON)\n\n\nclass ProductRecommendation(BaseModel):\n    product_id: str\n    product_name: str\n\n\nRecommendations = Iterable[ProductRecommendation]\n</code></pre> <p>Now let's use our instructor patch. Since we don't want to wait for all the tokens to finish, will set stream to <code>True</code> and process each product recommendation as it comes in:</p> <pre><code>prompt = (\n    f\"Based on the following user profile:\\n{profile_data}\\nRank the following products from most relevant to least relevant:\\n\"\n    + '\\n'.join(\n        f\"{product['product_id']} {product['product_name']}\" for product in products\n    )\n)\n\nstart_perf = time.perf_counter()\nrecommendations_stream = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",\n    temperature=0.1,\n    response_model=Iterable[ProductRecommendation],\n    stream=True,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"Generate product recommendations based on the customer profile. Return in order of highest recommended first.\",\n        },\n        {\"role\": \"user\", \"content\": prompt},\n    ],\n)\nfor product in recommendations_stream:\n    print(product)\n    end_perf = time.perf_counter()\n    print(f\"Time for first result (generator): {end_perf - start_perf:.2f} seconds\")\n    break\n</code></pre> <pre><code>product_id='1' product_name='Apple MacBook Air (2023)'\nTime for first result (generator): 4.33 seconds\n</code></pre> <p><code>recommendations_stream</code> is a generator! It yields the extracted products as it's processing the stream in real-time. Now let's get the same response without streaming and see how they compare.</p> <pre><code>start_perf = time.perf_counter()\nrecommendations_list = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",\n    temperature=0.1,\n    response_model=Iterable[ProductRecommendation],\n    stream=False,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"Generate product recommendations based on the customer profile. Return in order of highest recommended first.\",\n        },\n        {\"role\": \"user\", \"content\": prompt},\n    ],\n)\nprint(recommendations_list[0])\nend_perf = time.perf_counter()\nprint(f\"Time for first result (list): {end_perf - start_perf:.2f} seconds\")\n</code></pre> <pre><code>product_id='1' product_name='Apple MacBook Air (2023)'\nTime for first result (list): 8.63 seconds\n</code></pre> <p>Our web application now displays results faster. Even a 100ms improvement can lead to a 1% increase in revenue.</p>"},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#fastapi","title":"FastAPI","text":"<p>We can also take this and set up a streaming LLM API endpoint using FastAPI. Check out our docs on using FastAPI here!</p>"},{"location":"blog/2023/11/26/python-generators-and-llm-streaming/#key-takeaways","title":"Key Takeaways","text":"<p>To summarize, we looked at:</p> <p>\u2022 Generators in Python: A powerful feature that allows for efficient data handling with reduced latency</p> <p>\u2022 LLM Streaming: LLMs provide us generators to stream tokens and Instructor can let us validate and extract data from this stream. Real-time data validation ftw!</p> <p>Don't forget to check our GitHub for more resources and give us a star if you find the library helpful!</p> <p>If you have any questions or need further clarifications, feel free to reach out or dive into the Instructor library's documentation for more detailed information. Happy coding!</p>"},{"location":"blog/2024/08/20/should-i-be-using-structured-outputs/","title":"Should I Be Using Structured Outputs?","text":"<p>OpenAI recently announced Structured Outputs which ensures that generated responses match any arbitrary provided JSON Schema. In their announcement article, they acknowledged that it had been inspired by libraries such as <code>instructor</code>.</p>","tags":["OpenAI"]},{"location":"blog/2024/08/20/should-i-be-using-structured-outputs/#main-challenges","title":"Main Challenges","text":"<p>If you're building complex LLM workflows, you've likely considered OpenAI's Structured Outputs as a potential replacement for <code>instructor</code>.</p> <p>But before you do so, three key challenges remain:</p> <ol> <li>Limited Validation And Retry Logic: Structured Outputs ensure adherence to the schema but not useful content. You might get perfectly formatted yet unhelpful responses</li> <li>Streaming Challenges: Parsing raw JSON objects from streamed responses with the sdk is error-prone and inefficient</li> <li>Unpredictable Latency Issues : Structured Outputs suffers from random latency spikes that might result in an almost 20x increase in response time</li> </ol> <p>Additionally, adopting Structured Outputs locks you into OpenAI's ecosystem, limiting your ability to experiment with diverse models or providers that might better suit specific use-cases.</p> <p>This vendor lock-in increases vulnerability to provider outages, potentially causing application downtime and SLA violations, which can damage user trust and impact your business reputation.</p> <p>In this article, we'll show how <code>instructor</code> addresses many of these challenges with features such as automatic reasking when validation fails, automatic support for validated streaming data and more.</p>","tags":["OpenAI"]},{"location":"blog/2024/08/20/should-i-be-using-structured-outputs/#limited-validation-and-retry-logic","title":"Limited Validation and Retry Logic","text":"<p>Validation is crucial for building reliable and effective applications. We want to catch errors in real time using <code>Pydantic</code> validators in order to allow our LLM to correct its responses on the fly.</p> <p>Let's see an example of a simple validator below which ensures user names are always in uppercase.</p> <pre><code>import openai\nfrom pydantic import BaseModel, field_validator\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"name\")\n    def ensure_uppercase(cls, v: str) -&gt; str:\n        if not v.isupper():\n            raise ValueError(\"All letters must be uppercase. Got: \" + v)\n        return v\n\n\nclient = openai.OpenAI()\ntry:\n    resp = client.beta.chat.completions.parse(\n        response_format=User,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Extract the following user: Jason is 25 years old.\",\n            },\n        ],\n        model=\"gpt-4o-mini\",\n    )\nexcept Exception as e:\n    print(e)\n    \"\"\"\n    1 validation error for User\n    name\n      Value error, All letters must be uppercase. Got: Jason [type=value_error, input_value='Jason', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.8/v/value_error\n    \"\"\"\n</code></pre> <p>We can see that we lose the original completion when validation fails. This leaves developers without the means to implement retry logic so that the LLM can provide a targetted correction and regenerate its response.</p> <p>Without robust validation, applications risk producing inconsistent outputs and losing valuable context for error correction. This leads to degraded user experience and missed opportunities for targeted improvements in LLM responses.</p>","tags":["OpenAI"]},{"location":"blog/2024/08/20/should-i-be-using-structured-outputs/#streaming-challenges","title":"Streaming Challenges","text":"<p>Streaming with Structured Outputs is complex. It requires manual parsing, lacks partial validation, and needs a context manager to be used with. Effective implementation with the <code>beta.chat.completions.stream</code> method demands significant effort.</p> <p>Let's see an example below.</p> <pre><code>import openai\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclient = openai.OpenAI()\nwith client.beta.chat.completions.stream(\n    response_format=User,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract the following user: Jason is 25 years old.\",\n        },\n    ],\n    model=\"gpt-4o-mini\",\n) as stream:\n    for event in stream:\n        if event.type == \"content.delta\":\n            print(event.snapshot, flush=True, end=\"\\n\")\n            #&gt;\n            #&gt; {\"\n            #&gt; {\"name\n            #&gt; {\"name\":\"\n            #&gt; {\"name\":\"Jason\n            #&gt; {\"name\":\"Jason\",\"\n            #&gt; {\"name\":\"Jason\",\"age\n            #&gt; {\"name\":\"Jason\",\"age\":\n            #&gt; {\"name\":\"Jason\",\"age\":25\n            #&gt; {\"name\":\"Jason\",\"age\":25}\n</code></pre>","tags":["OpenAI"]},{"location":"blog/2024/08/20/should-i-be-using-structured-outputs/#unpredictable-latency-spikes","title":"Unpredictable Latency Spikes","text":"<p>In order to benchmark the two modes, we made 200 identical requests to OpenAI and noted the time taken for each request to complete. The results are summarized in the following table:</p> mode mean min max std_dev variance Tool Calling 6.84 6.21 12.84 0.69 0.47 Structured Outputs 28.20 14.91 136.90 9.27 86.01 <p>Structured Outputs suffers from unpredictable latency spikes while Tool Calling maintains consistent performance. This could cause users to occasionally experience significant delays in response times, potentially impacting the overall user satisfication and retention rates.</p>","tags":["OpenAI"]},{"location":"blog/2024/08/20/should-i-be-using-structured-outputs/#why-use-instructor","title":"Why use <code>instructor</code>","text":"<p><code>instructor</code> is fully compatible with Structured Outputs and provides three main benefits to developers.</p> <ol> <li>Automatic Validation and Retries: Regenerates LLM responses on Pydantic validation failures, ensuring data integrity.</li> <li>Real-time Streaming Validation: Incrementally validates partial JSON against Pydantic models, enabling immediate use of validated properties.</li> <li>Provider-Agnostic API: Switch between LLM providers and models with a single line of code.</li> </ol> <p>Let's see this in action below</p>","tags":["OpenAI"]},{"location":"blog/2024/08/20/should-i-be-using-structured-outputs/#automatic-validation-and-retries","title":"Automatic Validation and Retries","text":"<p>With <code>instructor</code>, all it takes is a simple Pydantic Schema and a validator for you to get the extracted names as an upper case value.</p> <pre><code>import instructor\nimport openai\nfrom pydantic import BaseModel, field_validator\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"name\")\n    def ensure_uppercase(cls, v: str) -&gt; str:\n        if not v.isupper():\n            raise ValueError(\"All letters must be uppercase. Got: \" + v)\n        return v\n\n\nclient = instructor.from_openai(openai.OpenAI(), mode=instructor.Mode.TOOLS_STRICT)\n\nresp = client.chat.completions.create(\n    response_model=User,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract the following user: Jason is 25 years old.\",\n        }\n    ],\n    model=\"gpt-4o-mini\",\n)\n\nprint(resp)\n#&gt; name='JASON' age=25\n</code></pre> <p>This built-in retry logic allows for targetted correction to the generated response, ensuring that outputs are not only consistent with your schema but also correct for your use-case. This is invaluable in building reliable LLM systems.</p>","tags":["OpenAI"]},{"location":"blog/2024/08/20/should-i-be-using-structured-outputs/#real-time-streaming-validation","title":"Real-time Streaming Validation","text":"<p>A common use-case is to define a single schema and extract multiple instances of it. With <code>instructor</code>, doing this is relatively straightforward by using our <code>create_iterable</code> method.</p> <pre><code>import instructor\nimport openai\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI(), mode=instructor.Mode.TOOLS_STRICT)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nusers = client.chat.completions.create_iterable(\n    model=\"gpt-4o-mini\",\n    response_model=User,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a perfect entity extraction system\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": (f\"Extract `Jason is 10 and John is 10`\"),\n        },\n    ],\n)\n\nfor user in users:\n    print(user)\n    #&gt; name='Jason' age=10\n    #&gt; name='John' age=10\n</code></pre> <p>Other times, we might also want to stream out information as it's dynamically generated into some sort of frontend component With <code>instructor</code>, you'll be able to do just that using the <code>create_partial</code> method.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nfrom rich.console import Console\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.Mode.TOOLS_STRICT)\n\ntext_block = \"\"\"\nIn our recent online meeting, participants from various backgrounds joined to discuss the upcoming tech conference. The names and contact details of the participants were as follows:\n\n- Name: John Doe, Email: johndoe@email.com, Twitter: @TechGuru44\n- Name: Jane Smith, Email: janesmith@email.com, Twitter: @DigitalDiva88\n- Name: Alex Johnson, Email: alexj@email.com, Twitter: @CodeMaster2023\n\nDuring the meeting, we agreed on several key points. The conference will be held on March 15th, 2024, at the Grand Tech Arena located at 4521 Innovation Drive. Dr. Emily Johnson, a renowned AI researcher, will be our keynote speaker.\n\nThe budget for the event is set at $50,000, covering venue costs, speaker fees, and promotional activities. Each participant is expected to contribute an article to the conference blog by February 20th.\n\nA follow-up meeting is scheduled for January 25th at 3 PM GMT to finalize the agenda and confirm the list of speakers.\n\"\"\"\n\n\nclass User(BaseModel):\n    name: str\n    email: str\n    twitter: str\n\n\nclass MeetingInfo(BaseModel):\n    users: list[User]\n    date: str\n    location: str\n    budget: int\n    deadline: str\n\n\nextraction_stream = client.chat.completions.create_partial(\n    model=\"gpt-4o-mini\",\n    response_model=MeetingInfo,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"Get the information about the meeting and the users {text_block}\",\n        },\n    ],\n    stream=True,\n)\n\n\nconsole = Console()\n\nfor extraction in extraction_stream:\n    obj = extraction.model_dump()\n    console.clear()\n    console.print(obj)\n</code></pre> <p>This will output the following</p> <p></p>","tags":["OpenAI"]},{"location":"blog/2024/08/20/should-i-be-using-structured-outputs/#provider-agnostic-api","title":"Provider-Agnostic API","text":"<p>With <code>instructor</code>, switching between different providers is easy due to our unified API.</p> <p>For example, the swtich from OpenAI to Anthropic requires only three adjustments</p> <ol> <li>Import the Anthropic client</li> <li>Use <code>from_anthropic</code> instead of <code>from_openai</code></li> <li>Update the model name (e.g., from gpt-4o-mini to claude-3-5-sonnet)</li> </ol> <p>This makes it incredibly flexible for users looking to migrate and test different providers for their use cases. Let's see this in action with an example below.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    response_model=User,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract the user from the string belo - Chris is a 27 year old engineer in San Francisco\",\n        }\n    ],\n    max_tokens=100,\n)\n\nprint(resp)\n#&gt; name='Chris' age=27\n</code></pre> <p>Now let's see how we can achieve the same with Anthropic.</p> <pre><code>import instructor\nfrom anthropic import Anthropic  # (1)!\nfrom pydantic import BaseModel\n\nclient = instructor.from_anthropic(Anthropic())  # (2)!\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nresp = client.chat.completions.create(\n    model=\"claude-3-5-sonnet-20240620\",  # (3)!\n    response_model=User,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract the user from the string belo - Chris is a 27 year old engineer in San Francisco\",\n        }\n    ],\n    max_tokens=100,\n)\n\nprint(resp)\n#&gt; name='Chris' age=27\n</code></pre> <ol> <li>Import the Anthropic client</li> <li>Use <code>from_anthropic</code> instead of <code>from_openai</code></li> <li>Update the model name to <code>claude-3-5-sonnet-20240620</code></li> </ol>","tags":["OpenAI"]},{"location":"blog/2024/08/20/should-i-be-using-structured-outputs/#conclusion","title":"Conclusion","text":"<p>While OpenAI's Structured Outputs shows promise, it has key limitations. The system lacks support for extra JSON fields to provide output examples, default value factories, and pattern matching in defined schemas. These constraints limit developers' ability to express complex return types, potentially impacting application performance and flexibility.</p> <p>If you're interested in Structured Outputs, <code>instructor</code> addresses these critical issues. It provides automatic retries, real-time input validation, and multi-provider integration, allowing developers to more effectively implement Structured Outputs in their AI projects.</p> <p>if you haven't given <code>instructor</code> a shot, try it today!</p>","tags":["OpenAI"]},{"location":"blog/2023/09/11/generating-structured-output--json-from-llms/","title":"Generating Structured Output / JSON from LLMs","text":"<p>Language models have seen significant growth. Using them effectively often requires complex frameworks. This post discusses how Instructor simplifies this process using Pydantic.</p>"},{"location":"blog/2023/09/11/generating-structured-output--json-from-llms/#the-problem-with-existing-llm-frameworks","title":"The Problem with Existing LLM Frameworks","text":"<p>Current frameworks for Language Learning Models (LLMs) have complex setups. Developers find it hard to control interactions with language models. Some frameworks require complex JSON Schema setups.</p>"},{"location":"blog/2023/09/11/generating-structured-output--json-from-llms/#the-openai-function-calling-game-changer","title":"The OpenAI Function Calling Game-Changer","text":"<p>OpenAI's Function Calling feature provides a constrained interaction model. However, it has its own complexities, mostly around JSON Schema.</p>"},{"location":"blog/2023/09/11/generating-structured-output--json-from-llms/#why-pydantic","title":"Why Pydantic?","text":"<p>Instructor uses Pydantic to simplify the interaction between the programmer and the language model.</p> <ul> <li>Widespread Adoption: Pydantic is a popular tool among Python developers.</li> <li>Simplicity: Pydantic allows model definition in Python.</li> <li>Framework Compatibility: Many Python frameworks already use Pydantic.</li> </ul> <pre><code>import pydantic\nimport instructor\nfrom openai import OpenAI\n\n# Enables the response_model\nclient = instructor.from_openai(OpenAI())\n\n\nclass UserDetail(pydantic.BaseModel):\n    name: str\n    age: int\n\n    def introduce(self):\n        return f\"Hello I'm {self.name} and I'm {self.age} years old\"\n\n\nuser: UserDetail = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n    ],\n)\n</code></pre>"},{"location":"blog/2023/09/11/generating-structured-output--json-from-llms/#simplifying-validation-flow-with-pydantic","title":"Simplifying Validation Flow with Pydantic","text":"<p>Pydantic validators simplify features like re-asking or self-critique. This makes these tasks less complex compared to other frameworks.</p> <pre><code>from typing_extensions import Annotated\nfrom pydantic import BaseModel, BeforeValidator\nfrom instructor import llm_validator\n\n\nclass QuestionAnswerNoEvil(BaseModel):\n    question: str\n    answer: Annotated[\n        str,\n        BeforeValidator(llm_validator(\"don't say objectionable things\")),\n    ]\n</code></pre>"},{"location":"blog/2023/09/11/generating-structured-output--json-from-llms/#the-modular-approach","title":"The Modular Approach","text":"<p>Pydantic allows for modular output schemas. This leads to more organized code.</p>"},{"location":"blog/2023/09/11/generating-structured-output--json-from-llms/#composition-of-schemas","title":"Composition of Schemas","text":"<pre><code>class UserDetails(BaseModel):\n    name: str\n    age: int\n\n\nclass UserWithAddress(UserDetails):\n    address: str\n</code></pre>"},{"location":"blog/2023/09/11/generating-structured-output--json-from-llms/#defining-relationships","title":"Defining Relationships","text":"<pre><code>class UserDetail(BaseModel):\n    id: int\n    age: int\n    name: str\n    friends: List[int]\n\n\nclass UserRelationships(BaseModel):\n    users: List[UserDetail]\n</code></pre>"},{"location":"blog/2023/09/11/generating-structured-output--json-from-llms/#using-enums","title":"Using Enums","text":"<pre><code>from enum import Enum, auto\n\n\nclass Role(Enum):\n    PRINCIPAL = auto()\n    TEACHER = auto()\n    STUDENT = auto()\n    OTHER = auto()\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role\n</code></pre>"},{"location":"blog/2023/09/11/generating-structured-output--json-from-llms/#flexible-schemas","title":"Flexible Schemas","text":"<pre><code>from typing import List\n\n\nclass Property(BaseModel):\n    key: str\n    value: str\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Property]\n</code></pre>"},{"location":"blog/2023/09/11/generating-structured-output--json-from-llms/#chain-of-thought","title":"Chain of Thought","text":"<pre><code>class TimeRange(BaseModel):\n    chain_of_thought: str\n    start_time: int\n    end_time: int\n\n\nclass UserDetail(BaseModel):\n    id: int\n    age: int\n    name: str\n    work_time: TimeRange\n    leisure_time: TimeRange\n</code></pre>"},{"location":"blog/2023/09/11/generating-structured-output--json-from-llms/#language-models-as-microservices","title":"Language Models as Microservices","text":"<p>The architecture resembles FastAPI. Most code can be written as Python functions that use Pydantic objects. This eliminates the need for prompt chains.</p>"},{"location":"blog/2023/09/11/generating-structured-output--json-from-llms/#fastapi-stub","title":"FastAPI Stub","text":"<pre><code>import fastapi\nfrom pydantic import BaseModel\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\napp = fastapi.FastAPI()\n\n@app.get(\"/user/{user_id}\", response_model=UserDetails)\nasync def get_user(user_id: int) -&gt; UserDetails:\n    return ...\n</code></pre>"},{"location":"blog/2023/09/11/generating-structured-output--json-from-llms/#using-instructor-as-a-function","title":"Using Instructor as a Function","text":"<pre><code>def extract_user(str) -&gt; UserDetails:\n    return client.chat.completions(\n           response_model=UserDetails,\n           messages=[]\n    )\n</code></pre>"},{"location":"blog/2023/09/11/generating-structured-output--json-from-llms/#response-modeling","title":"Response Modeling","text":"<pre><code>class MaybeUser(BaseModel):\n    result: Optional[UserDetail]\n    error: bool\n    message: Optional[str]\n</code></pre>"},{"location":"blog/2023/09/11/generating-structured-output--json-from-llms/#conclusion","title":"Conclusion","text":"<p>Instructor, with Pydantic, simplifies interaction with language models. It is usable for both experienced and new developers.</p> <p>If you enjoy the content or want to try out <code>instructor</code> please check out the github and give us a star!</p>"},{"location":"blog/2024/02/18/seamless-support-with-langsmith/","title":"Seamless Support with Langsmith","text":"<p>Its a common misconception that LangChain's LangSmith is only compatible with LangChain's models. In reality, LangSmith is a unified DevOps platform for developing, collaborating, testing, deploying, and monitoring LLM applications. In this blog we will explore how LangSmith can be used to enhance the OpenAI client alongside <code>instructor</code>.</p>"},{"location":"blog/2024/02/18/seamless-support-with-langsmith/#langsmith","title":"LangSmith","text":"<p>In order to use langsmith, you first need to set your LangSmith API key.</p> <pre><code>export LANGCHAIN_API_KEY=&lt;your-api-key&gt;\n</code></pre> <p>Next, you will need to install the LangSmith SDK:</p> <pre><code>pip install -U langsmith\npip install -U instructor\n</code></pre> <p>If you want to pull this example down from instructor-hub you can use the following command:</p> <pre><code>instructor hub pull --slug batch_classification_langsmith --py &gt; batch_classification_langsmith.py\n</code></pre> <p>In this example we'll use the <code>wrap_openai</code> function to wrap the OpenAI client with LangSmith. This will allow us to use LangSmith's observability and monitoring features with the OpenAI client. Then we'll use <code>instructor</code> to patch the client with the <code>TOOLS</code> mode. This will allow us to use <code>instructor</code> to add additional functionality to the client. We'll use asyncio to classify a list of questions.</p> <pre><code>import instructor\nimport asyncio\n\nfrom langsmith import traceable\nfrom langsmith.wrappers import wrap_openai\n\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel, Field, field_validator\nfrom typing import List\nfrom enum import Enum\n\n# Wrap the OpenAI client with LangSmith\nclient = wrap_openai(AsyncOpenAI())\n\n# Patch the client with instructor\nclient = instructor.from_openai(client, mode=instructor.Mode.TOOLS)\n\n# Rate limit the number of requests\nsem = asyncio.Semaphore(5)\n\n# Use an Enum to define the types of questions\nclass QuestionType(Enum):\n    CONTACT = \"CONTACT\"\n    TIMELINE_QUERY = \"TIMELINE_QUERY\"\n    DOCUMENT_SEARCH = \"DOCUMENT_SEARCH\"\n    COMPARE_CONTRAST = \"COMPARE_CONTRAST\"\n    EMAIL = \"EMAIL\"\n    PHOTOS = \"PHOTOS\"\n    SUMMARY = \"SUMMARY\"\n\n\n# You can add more instructions and examples in the description\n# or you can put it in the prompt in `messages=[...]`\nclass QuestionClassification(BaseModel):\n    \"\"\"\n    Predict the type of question that is being asked.\n    Here are some tips on how to predict the question type:\n    CONTACT: Searches for some contact information.\n    TIMELINE_QUERY: \"When did something happen?\n    DOCUMENT_SEARCH: \"Find me a document\"\n    COMPARE_CONTRAST: \"Compare and contrast two things\"\n    EMAIL: \"Find me an email, search for an email\"\n    PHOTOS: \"Find me a photo, search for a photo\"\n    SUMMARY: \"Summarize a large amount of data\"\n    \"\"\"\n\n    # If you want only one classification, just change it to\n    #   `classification: QuestionType` rather than `classifications: List[QuestionType]``\n    chain_of_thought: str = Field(\n        ..., description=\"The chain of thought that led to the classification\"\n    )\n    classification: List[QuestionType] = Field(\n        description=f\"An accuracy and correct prediction predicted class of question. Only allowed types: {[t.value for t in QuestionType]}, should be used\",\n    )\n\n    @field_validator(\"classification\", mode=\"before\")\n    def validate_classification(cls, v):\n        # sometimes the API returns a single value, just make sure it's a list\n        if not isinstance(v, list):\n            v = [v]\n        return v\n\n\n@traceable(name=\"classify-question\")\nasync def classify(data: str) -&gt; QuestionClassification:\n    \"\"\"\n    Perform multi-label classification on the input text.\n    Change the prompt to fit your use case.\n\n    Args:\n        data (str): The input text to classify.\n    \"\"\"\n    async with sem:  # some simple rate limiting\n        return data, await client.chat.completions.create(\n            model=\"gpt-4-turbo-preview\",\n            response_model=QuestionClassification,\n            max_retries=2,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Classify the following question: {data}\",\n                },\n            ],\n        )\n\n\nasync def main(questions: List[str]):\n    tasks = [classify(question) for question in questions]\n\n    for task in asyncio.as_completed(tasks):\n        question, label = await task\n        resp = {\n            \"question\": question,\n            \"classification\": [c.value for c in label.classification],\n            \"chain_of_thought\": label.chain_of_thought,\n        }\n        resps.append(resp)\n    return resps\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    questions = [\n        \"What was that ai app that i saw on the news the other day?\",\n        \"Can you find the trainline booking email?\",\n        \"what did I do on Monday?\",\n        \"Tell me about todays meeting and how it relates to the email on Monday\",\n    ]\n\n    resp = asyncio.run(main(questions))\n\n    for r in resp:\n        print(\"q:\", r[\"question\"])\n        #&gt; q: what did I do on Monday?\n        print(\"c:\", r[\"classification\"])\n        #&gt; c: ['SUMMARY']\n</code></pre> <p>If you follow what we've done is wrapped the client and proceeded to quickly use asyncio to classify a list of questions. This is a simple example of how you can use LangSmith to enhance the OpenAI client. You can use LangSmith to monitor and observe the client, and use <code>instructor</code> to add additional functionality to the client.</p> <p>To take a look at trace of this run check out this shareable link.</p> <p></p>"},{"location":"blog/2023/11/13/learn-async/","title":"Async Processing OpenAI using <code>asyncio</code> and <code>Instructor</code> with Python","text":"<p>Today, I will introduce you to various approaches for using asyncio in Python. We will apply this to batch process data using <code>instructor</code> and learn how to use <code>asyncio.gather</code> and <code>asyncio.as_completed</code> for concurrent data processing. Additionally, we will explore how to limit the number of concurrent requests to a server using <code>asyncio.Semaphore</code>.</p> <p>Github Example</p> <p>If you want to run the code examples in this article, you can find them on jxnl/instructor</p> <p>We will start by defining an <code>async</code> function that calls <code>openai</code> to extract data, and then examine four different ways to execute it. We will discuss the pros and cons of each approach and analyze the results of running them on a small batch.</p>"},{"location":"blog/2023/11/13/learn-async/#understanding-asyncio","title":"Understanding <code>asyncio</code>","text":"<p><code>asyncio</code> is a Python library that enables writing concurrent code using the async/await syntax. It is particularly useful for IO-bound and structured network code. If you are familiar with OpenAI's SDK, you might have encountered two classes: <code>OpenAI()</code> and <code>AsyncOpenAI()</code>. Today, we will be using the <code>AsyncOpenAI()</code> class, which processes data asynchronously.</p> <p>By utilizing these tools in web applications or bulk processing, we can significantly improve performance by handling multiple requests concurrently instead of sequentially.</p>"},{"location":"blog/2023/11/13/learn-async/#understanding-async-and-await","title":"Understanding <code>async</code> and <code>await</code>","text":"<p>We will be using the <code>async</code> and <code>await</code> keywords to define asynchronous functions. The <code>async</code> keyword is used to define a function that returns a coroutine object. The <code>await</code> keyword is used to wait for the result of a coroutine object.</p> <p>If you want to understand the deeper details of <code>asyncio</code>, I recommend reading this article by Real Python.</p>"},{"location":"blog/2023/11/13/learn-async/#understanding-gather-vs-as_completed","title":"Understanding <code>gather</code> vs <code>as_completed</code>","text":"<p>In this post we'll show two ways to run tasks concurrently: <code>asyncio.gather</code> and <code>asyncio.as_completed</code>. The <code>gather</code> method is used to run multiple tasks concurrently and return the results as a <code>list</code>. The <code>as_completed</code> returns a <code>iterable</code> is used to run multiple tasks concurrently and return the results as they complete. Another great resource on the differences between the two can be found here.</p>"},{"location":"blog/2023/11/13/learn-async/#example-batch-processing","title":"Example: Batch Processing","text":"<p>In this example, we will demonstrate how to use <code>asyncio</code> for async processing tasks, specifically for extracting and processing data concurrently. The script will extract data from a list of texts and process it concurrently using <code>asyncio</code>.</p> <pre><code>import instructor\nfrom pydantic import BaseModel\nfrom openai import AsyncOpenAI\n\n# Enables `response_model` in `create` method\nclient = instructor.apatch(AsyncOpenAI())  # (1)!\n\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n\nasync def extract_person(text: str) -&gt; Person:\n    return await client.chat.completions.create(  # (2)!\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"user\", \"content\": text},\n        ],\n        response_model=Person,\n    )\n</code></pre> <ol> <li>We use <code>instructor.apatch</code> to patch the <code>create</code> method of <code>AsyncOpenAI</code> to accept a <code>response_model</code> argument. This is because the <code>create</code> method of <code>AsyncOpenAI</code> does not accept a <code>response_model</code> argument without this patch.</li> <li>We use <code>await</code> here to wait for the response from the server before we return the result. This is because <code>create</code> returns a coroutine object, not the result of the coroutine.</li> </ol> <p>Notice that now there are <code>async</code> and <code>await</code> keywords in the function definition. This is because we're using the <code>asyncio</code> library to run the function concurrently. Now let's define a batch of texts to process.</p> <pre><code>dataset = [\n    \"My name is John and I am 20 years old\",\n    \"My name is Mary and I am 21 years old\",\n    \"My name is Bob and I am 22 years old\",\n    \"My name is Alice and I am 23 years old\",\n    \"My name is Jane and I am 24 years old\",\n    \"My name is Joe and I am 25 years old\",\n    \"My name is Jill and I am 26 years old\",\n]\n</code></pre>"},{"location":"blog/2023/11/13/learn-async/#for-loop-running-tasks-sequentially","title":"<code>for loop</code>: Running tasks sequentially.","text":"<pre><code>persons = []\nfor text in dataset:\n    person = await extract_person(text)\n    persons.append(person)\n</code></pre> <p>Even though there is an <code>await</code> keyword, we still have to wait for each task to finish before starting the next one. This is because we're using a <code>for</code> loop to iterate over the dataset. This method, which uses a <code>for</code> loop, will be the slowest among the four methods discussed today.</p>"},{"location":"blog/2023/11/13/learn-async/#asynciogather-running-tasks-concurrently","title":"<code>asyncio.gather</code>: Running tasks concurrently.","text":"<pre><code>async def gather():\n    tasks_get_persons = [extract_person(text) for text in dataset]\n    all_persons = await asyncio.gather(*tasks_get_persons)  # (1)!\n</code></pre> <ol> <li>We use <code>await</code> here to wait for all the tasks to finish before assigning the result to <code>all_persons</code>. This is because <code>asyncio.gather</code> returns a coroutine object, not the result of the coroutine. Alternatively, we can use <code>asyncio.as_completed</code> to achieve the same result.</li> </ol> <p>Using <code>asyncio.gather</code> allows us to return all the results at once. It is an effective way to speed up our code, but it's not the only way. Particularly, if we have a large dataset, we might not want to wait for everything to finish before starting to process the results. This is where <code>asyncio.as_completed</code> comes into play.</p>"},{"location":"blog/2023/11/13/learn-async/#asyncioas_completed-handling-tasks-as-they-complete","title":"<code>asyncio.as_completed</code>: Handling tasks as they complete.","text":"<pre><code>async def as_completed():\n    all_persons = []\n    tasks_get_persons = [extract_person(text) for text in dataset]\n    for person in asyncio.as_completed(tasks_get_persons):\n        all_persons.append(await person)  # (1)!\n</code></pre> <ol> <li>We use <code>await</code> here to wait for each task to complete before appending it to the list. This is because <code>as_completed</code> returns a coroutine object, not the result of the coroutine. Alternatively, we can use <code>asyncio.gather</code> to achieve the same result.</li> </ol> <p>This method is a great way to handle large datasets. We can start processing the results as they come in, especially if we are streaming data back to a client.</p> <p>However, these methods aim to complete as many tasks as possible as quickly as possible. This can be problematic if we want to be considerate to the server we're making requests to. This is where rate limiting comes into play. While there are libraries available to assist with rate limiting, for our initial defense, we will use a semaphore to limit the number of concurrent requests we make.</p> <p>Ordering of results</p> <p>It is important to note that the order of the results will not be the same as the order of the dataset. This is because the tasks are completed in the order they finish, not the order they were started. If you need to preserve the order of the results, you can use <code>asyncio.gather</code> instead.</p>"},{"location":"blog/2023/11/13/learn-async/#rate-limited-gather-using-semaphores-to-limit-concurrency","title":"Rate-Limited Gather: Using semaphores to limit concurrency.","text":"<pre><code>sem = asyncio.Semaphore(2)\n\n\nasync def rate_limited_extract_person(text: str, sem: Semaphore) -&gt; Person:\n    async with sem:  # (1)!\n        return await extract_person(text)\n\n\nasync def rate_limited_gather(sem: Semaphore):\n    tasks_get_persons = [rate_limited_extract_person(text, sem) for text in dataset]\n    resp = await asyncio.gather(*tasks_get_persons)\n</code></pre> <ol> <li>We use a semaphore to limit the number of concurrent requests to 2. This approach strikes a balance between speed and being considerate to the server we're making requests to.</li> </ol>"},{"location":"blog/2023/11/13/learn-async/#rate-limited-as-completed-using-semaphores-to-limit-concurrency","title":"Rate-Limited As Completed: Using semaphores to limit concurrency.","text":"<pre><code>sem = asyncio.Semaphore(2)\n\n\nasync def rate_limited_extract_person(text: str, sem: Semaphore) -&gt; Person:\n    async with sem:  # (1)!\n        return await extract_person(text)\n\n\nasync def rate_limited_as_completed(sem: Semaphore):\n    all_persons = []\n    tasks_get_persons = [rate_limited_extract_person(text, sem) for text in dataset]\n    for person in asyncio.as_completed(tasks_get_persons):\n        all_persons.append(await person)  # (2)!\n</code></pre> <ol> <li> <p>We use a semaphore to limit the number of concurrent requests to 2. This approach strikes a balance between speed and being considerate to the server we're making requests to.</p> </li> <li> <p>We use <code>await</code> here to wait for each task to complete before appending it to the list. This is because <code>as_completed</code> returns a coroutine object, not the result of the coroutine. Alternatively, we can use <code>asyncio.gather</code> to achieve the same result.</p> </li> </ol> <p>Now that we have seen the code, let's examine the results of processing 7 texts. As the prompts become longer or if we use GPT-4, the differences between these methods will become more pronounced.</p> <p>Other Options</p> <p>It is important to also note that here we are using a <code>semaphore</code> to limit the number of concurrent requests. However, there are other ways to limit concurrency especially since we have rate limit information from the <code>openai</code> request. You can imagine using a library like <code>ratelimit</code> to limit the number of requests per second. OR catching rate limit exceptions and using <code>tenacity</code> to retry the request after a certain amount of time.</p> <ul> <li>tenacity</li> <li>aiolimiter</li> </ul>"},{"location":"blog/2023/11/13/learn-async/#results","title":"Results","text":"<p>As you can see, the <code>for</code> loop is the slowest, while <code>asyncio.as_completed</code> and <code>asyncio.gather</code> are the fastest without any rate limiting.</p> Method Execution Time Rate Limited (Semaphore) For Loop 6.17 seconds Asyncio.gather 0.85 seconds Asyncio.as_completed 0.95 seconds Asyncio.gather 3.04 seconds 2 Asyncio.as_completed 3.26 seconds 2"},{"location":"blog/2023/11/13/learn-async/#practical-implications-of-async-processing","title":"Practical implications of async processing","text":"<p>The choice of approach depends on the task's nature and the desired balance between speed and resource utilization.</p> <p>Here are some guidelines to consider:</p> <ul> <li>Use <code>asyncio.gather</code> for handling multiple independent tasks quickly.</li> <li>Apply <code>asyncio.as_completed</code> for large datasets to process tasks as they complete.</li> <li>Implement rate-limiting to avoid overwhelming servers or API endpoints.</li> </ul> <p>If you find the content helpful or want to try out <code>Instructor</code>, please visit our GitHub page and give us a star!</p>"},{"location":"blog/2024/05/01/instructor-logfire/","title":"Logfire","text":""},{"location":"blog/2024/05/01/instructor-logfire/#introduction","title":"Introduction","text":"<p>Logfire is a new observability platform coming from the creators of Pydantic. It integrates almost seamlessly with many of your favourite libraries such as Pydantic, HTTPx and Instructor. In this article, we'll show you how to use Logfire with Instructor to gain visibility into the performance of your entire application.</p> <p>We'll walk through the following examples</p> <ol> <li>Classifying scam emails using Instructor</li> <li>Performing simple validation using the <code>llm_validator</code></li> <li>Extracting data into a markdown table from an infographic with GPT4V</li> </ol> <p>As usual, all of the code that we refer to here is provided in examples/logfire for you to use in your projects.</p> <ul> <li><code>classify.py</code>: Email Classification Example</li> <li><code>image.py</code> : GPT4-V Example</li> <li><code>validate.py</code> : <code>llm_validator</code> example</li> </ul> Configure Logfire <p>Before starting this tutorial, make sure that you've registered for a Logfire account. You'll also need to create a project to track these logs.</p> <p>We'll need to install our dependencies and configure logfire auth before proceeding so simply run the commands below. Logfire will handle the authentication and configuration of your project.</p> <pre><code>pip install logfire openai instructor pydantic pandas tabulate\nlogfire auth\n</code></pre>"},{"location":"blog/2024/05/01/instructor-logfire/#classification","title":"Classification","text":"<p>Now that we've got Logfire setup, let's see how we can get it to help us track a simple classification job.</p> <p>Logfire is dead simple to integrate - all it takes is 2 lines of code and we have it setup.</p> <pre><code>from pydantic import BaseModel\nfrom openai import OpenAI\nimport instructor\nimport logfire\n\n\nopenai_client = OpenAI()\nlogfire.configure(pydantic_plugin=logfire.PydanticPlugin(record=\"all\")) #(1)!\nlogfire.instrument_openai(openai_client) #(2)!\nclient = instructor.from_openai(openai_client)\n</code></pre> <ol> <li>We add Pydantic logging using <code>logfire</code>. Note that depending on your use-case, you can configure what you want to log with Pydantic</li> <li>We use their openai_integration to configure logging for our client before using instructor on it</li> </ol> <p>In this example, we'll be looking at classifying emails as either spam or not spam. To do so, we can define a simple Pydantic model as seen below.</p> <pre><code>import enum\n\nclass Labels(str, enum.Enum):\n    \"\"\"Enumeration for single-label text classification.\"\"\"\n\n    SPAM = \"spam\"\n    NOT_SPAM = \"not_spam\"\n\n\nclass SinglePrediction(BaseModel):\n    \"\"\"\n    Class for a single class label prediction.\n    \"\"\"\n\n    class_label: Labels\n</code></pre> <p>We can then use this in a generic instructor function as seen below that simply asks the model to classify text and return it in the form of a <code>SinglePrediction</code> Pydantic object.</p> <p>Logfire can help us to log this entire function, and what's happening inside it, even down to the model validation level by using their <code>logfire.instrument</code> decorator.</p> <pre><code>@logfire.instrument(\"classification\", extract_args=True) #(1)!\ndef classify(data: str) -&gt; SinglePrediction:\n    \"\"\"Perform single-label classification on the input text.\"\"\"\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo-0613\",\n        response_model=SinglePrediction,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Classify the following text: {data}\",\n            },\n        ],\n    )\n</code></pre> <ol> <li>Logfire allows us to use the <code>logfire.instrument</code> decorator and tag a function to a specific name.</li> </ol> <p>Let's see what happens when we run this against a list of different emails</p> <pre><code>emails = [\n    \"Hello there I'm a Nigerian prince and I want to give you money\",\n    \"Meeting with Thomas has been set at Friday next week\",\n    \"Here are some weekly product updates from our marketing team\",\n]\n\nfor email in emails:\n    classify(email)\n</code></pre> <p>There are a few important things here that the logs immediately give us</p> <ol> <li>The duration that each individual portion of our code took to run</li> <li>The payload that we sent over to OpenAI</li> <li>The exact arguments and results that were passed to each individual portion of our code at each step</li> </ol> <p></p>"},{"location":"blog/2024/05/01/instructor-logfire/#llm-validators","title":"LLM Validators","text":"<p>For our second example, we'll use the inbuilt <code>llm_validator</code> that instructor provides out of the box to validate that our statements don't contain unsafe content that we might not want to serve to users. Let's start by defining a simple Pydantic Model that can do so and configure our logfire integration.</p> <pre><code>from typing import Annotated\nfrom pydantic import BaseModel, ValidationError\nfrom pydantic.functional_validators import AfterValidator\nfrom instructor import llm_validator\nimport logfire\nimport instructor\nfrom openai import OpenAI\n\nopenai_client = OpenAI()\nlogfire.configure(pydantic_plugin=logfire.PydanticPlugin(record=\"all\"))\nlogfire.instrument_openai(openai_client)\nclient = instructor.from_openai(openai_client)\n\n\nclass Statement(BaseModel):\n    message: Annotated[\n        str,\n        AfterValidator(\n            llm_validator(\"Don't allow any objectionable content\", client=client)\n        ),\n    ]\n</code></pre> <p>We can then test out our new validator with a few sample statements to see how our validator is working in practice.</p> <pre><code>messages = [\n    \"I think we should always treat violence as the best solution\",\n    \"There are some great pastries down the road at this bakery I know\",\n]\n\nfor message in messages:\n    try:\n        Statement(message=message)\n    except ValidationError as e:\n        print(e)\n</code></pre> <p>With Logfire, we can capture the entirety of the validation proccess. As seen below, we have access to not only the original input data, but also the schema that was being used, the errors that were thrown and even the exact field that threw the error.</p> <p></p>"},{"location":"blog/2024/05/01/instructor-logfire/#vision-models","title":"Vision Models","text":"<p>For our last example, let's see how we can use Logfire to extract structured data from an image using GPT-4V with OpenAI. We'll be using a simple bar graph here and using <code>GPT4V</code> to extract the data from the image from statista below and convert it into a markdown format.</p> <p></p> <p>What we want is an output of the combined numbers as seen below</p> Country Total Skier Visits (M) United States 55.5 Austria 43.6 France 40.7 Japan 26.6 Italy 22.3 Switzerland 22 Canada 18.5 China 17.9 Sweden 9.2 Germany 7 <p>This is relatively simple with Pydantic. What we need to do is to define a custom type which will handle the conversion process as seen below</p> <pre><code>from pydantic import BaseModel, Field, BeforeValidator, PlainSerializer, InstanceOf, WithJsonSchema\n\ndef md_to_df(data: Any) -&gt; Any:\n    # Convert markdown to DataFrame\n    if isinstance(data, str):\n        return (\n            pd.read_csv(\n                StringIO(data),  # Process data\n                sep=\"|\",\n                index_col=1,\n            )\n            .dropna(axis=1, how=\"all\")\n            .iloc[1:]\n            .applymap(lambda x: x.strip())\n        )\n    return data\n\n\nMarkdownDataFrame = Annotated[\n    InstanceOf[pd.DataFrame], #(1)!\n    BeforeValidator(md_to_df), #(2)!\n    WithJsonSchema( #(3)!\n        {\n            \"type\": \"string\",\n            \"description\": \"The markdown representation of the table, each one should be tidy, do not try to join tables that should be seperate\",\n        }\n    ),\n]\n</code></pre> <ol> <li>We indicate that the type of this type should be a pandas dataframe</li> <li>We run a validation step to ensure that we can convert the input into a valid pandas dataframe and return a new pandas Dataframe for our model to use</li> <li>We then override the type of the schema so that when we pass it to OpenAI, it knows to generate a table in a markdown format.</li> </ol> <p>We can then use this in a normal instructor call</p> <pre><code>import instructor\nimport logfire\n\n\nopenai_client = OpenAI()\nlogfire.configure(pydantic_plugin=logfire.PydanticPlugin(record=\"all\"))\nlogfire.instrument_openai(openai_client)\nclient = instructor.from_openai(\n    openai_client, mode=instructor.Mode.MD_JSON\n)\n\n@logfire.instrument(\"extract-table\", extract_args=True)\ndef extract_table_from_image(url: str) -&gt; Iterable[Table]:\n    return client.chat.completions.create(\n        model=\"gpt-4-vision-preview\",\n        response_model=Iterable[Table],\n        max_tokens=1800,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"Extract out a table from the image. Only extract out the total number of skiiers.\",\n                    },\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": url}},\n                ],\n            }\n        ],\n    )\n</code></pre> <p>We can then call it as seen below</p> <pre><code>url = \"https://cdn.statcdn.com/Infographic/images/normal/16330.jpeg\"\ntables = extract_table_from_image(url)\nfor table in tables:\n    print(table.caption, end=\"\\n\")\n    print(table.dataframe.to_markdown())\n</code></pre> <p>Logfire is able to capture the stack track of the entire call as seen below, profile each part of our application and most importantly capture the raw inputs of the OpenAI call alongside any potential errors.</p> <p></p>"},{"location":"blog/2024/03/28/matching-language-summaries/","title":"Matching Language in Multilingual Summarization Tasks","text":"<p>When asking language models to summarize text, there's a risk that the generated summary ends up in English, even if the source text is in another language. This is likely due to the instructions being provided in English, biasing the model towards English output.</p> <p>In this post, we explore techniques to ensure the language of the generated summary matches the language of the source text. We leverage Pydantic for data validation and the <code>langdetect</code> library for language identification.</p>"},{"location":"blog/2024/03/28/matching-language-summaries/#the-problem","title":"The Problem","text":"<p>Consider the following example where we ask a language model to summarize text in various languages:</p> <pre><code>\u053c\u0565\u0566\u057e\u0561\u056f\u0561\u0576 \u0574\u0578\u0564\u0565\u056c\u0576\u0565\u0580\u0568 \u057e\u0565\u0580\u057b\u056b\u0576 \u057f\u0561\u0580\u056b\u0576\u0565\u0580\u056b\u0576 \u0564\u0561\u0580\u0571\u0565\u056c \u0565\u0576 \u0561\u057e\u0565\u056c\u056b \u0570\u0561\u0580\u0578\u0582\u057d\u057f \u0587 \u056f\u0561\u057f\u0561\u0580\u0575\u0561\u056c, \u0570\u0576\u0561\u0580\u0561\u057e\u0578\u0580\u0578\u0582\u0569\u0575\u0578\u0582\u0576 \u0568\u0576\u0571\u0565\u057c\u0565\u056c\u0578\u057e \u057d\u057f\u0565\u0572\u056e\u0565\u056c \u057d\u0561\u0570\u0578\u0582\u0576 \u0587 \u0562\u0576\u0561\u056f\u0561\u0576 \u057f\u0565\u0584\u057d\u057f\u0565\u0580, \u056b\u0576\u0579\u057a\u0565\u057d \u0576\u0561\u0587 \u0563\u0565\u0580\u0561\u0566\u0561\u0576\u0581 \u0561\u0580\u0564\u0575\u0578\u0582\u0576\u0584\u0576\u0565\u0580 \u0581\u0578\u0582\u0581\u0561\u0562\u0565\u0580\u0565\u056c \u0574\u0565\u0584\u0565\u0576\u0561\u0575\u0561\u056f\u0561\u0576 \u0569\u0561\u0580\u0563\u0574\u0561\u0576\u0578\u0582\u0569\u0575\u0561\u0576, \u0570\u0561\u0580\u0581\u0565\u0580\u056b \u057a\u0561\u057f\u0561\u057d\u056d\u0561\u0576\u0574\u0561\u0576 \u0587 \u057d\u057f\u0565\u0572\u056e\u0561\u0563\u0578\u0580\u056e \u057f\u0565\u0584\u057d\u057f\u0565\u0580\u056b \u057d\u057f\u0565\u0572\u056e\u0574\u0561\u0576 \u0576\u0574\u0561\u0576 \u057f\u0561\u0580\u0562\u0565\u0580 \u0561\u057c\u0561\u057b\u0561\u0564\u0580\u0561\u0576\u0584\u0576\u0565\u0580\u0578\u0582\u0574\u0589 \u0531\u0575\u057d \u0574\u0578\u0564\u0565\u056c\u0576\u0565\u0580\u0568 \u0574\u0577\u0561\u056f\u057e\u0578\u0582\u0574 \u0565\u0576 \u0570\u057d\u056f\u0561\u0575\u0561\u056f\u0561\u0576 \u057f\u0565\u0584\u057d\u057f\u0561\u0575\u056b\u0576 \u057f\u057e\u0575\u0561\u056c\u0576\u0565\u0580\u056b \u0570\u056b\u0574\u0561\u0576 \u057e\u0580\u0561 \u0587 \u056f\u0561\u0580\u0578\u0572 \u0565\u0576 \u0562\u057c\u0576\u0565\u056c \u0562\u0576\u0561\u056f\u0561\u0576 \u056c\u0565\u0566\u057e\u056b \u056f\u0561\u057c\u0578\u0582\u0581\u057e\u0561\u056e\u0584\u0576 \u0578\u0582 \u0576\u0580\u0562\u0578\u0582\u0569\u0575\u0578\u0582\u0576\u0576\u0565\u0580\u0568\u055d \u0570\u0565\u0572\u0561\u0583\u0578\u056d\u0578\u0582\u0569\u0575\u0578\u0582\u0576 \u0561\u057c\u0561\u057b\u0561\u0581\u0576\u0565\u056c\u0578\u057e \u0570\u0561\u0574\u0561\u056f\u0561\u0580\u0563\u056b\u0579\u0576\u0565\u0580\u056b \u0587 \u0574\u0561\u0580\u0564\u056f\u0561\u0576\u0581 \u0574\u056b\u057b\u0587 \u0570\u0561\u0572\u0578\u0580\u0564\u0561\u056f\u0581\u0578\u0582\u0569\u0575\u0561\u0576 \u0578\u056c\u0578\u0580\u057f\u0578\u0582\u0574\u0589\n\n---\n\nMga modelo ng wika ay naging mas sopistikado sa nagdaang mga taon, na nagbibigay-daan sa pagbuo ng mga natural at madaling basahing teksto, at nagpapakita ng mahusay na pagganap sa iba't ibang gawain tulad ng awtomatikong pagsasalin, pagsagot sa mga tanong, at pagbuo ng malikhain na teksto. Ang mga modelo na ito ay sinanay sa napakalaking mga dataset ng teksto at kayang hulihin ang istruktura at mga nuances ng natural na wika. Ang mga pagpapabuti sa mga modelo ng wika ay maaaring magdulot ng rebolusyon sa komunikasyon sa pagitan ng mga computer at tao, at inaasahan ang higit pang pag-unlad sa hinaharap.\n\n---\n\nNgaahi motu\u02bba lea kuo nau hoko \u02bbo faka\u02bbofo\u02bbofa ange \u02bbi he ngaahi ta\u02bbu fakamuimui ni, \u02bbo fakafaingofua\u02bbi e fakatupu \u02bbo e ngaahi konga tohi \u02bboku lelei mo fakanatula pea \u02bboku nau fakahaa\u02bbi \u02bba e ngaahi ola lelei \u02bbi he ngaahi ng\u0101ue kehekehe \u02bbo hang\u0113 ko e liliu faka\u02bb\u0113tita, tali fehu\u02bbi, mo e fakatupu \u02bbo e konga tohi faka\u02bbatamai. Ko e ako \u02bba e ngaahi motu\u02bba ni \u02bbi he ngaahi seti \u02bbo e fakamatala tohi lahi pea \u02bboku nau malava \u02bbo puke \u02bba e fakafuofua mo e ngaahi me\u02bba iiki \u02bbo e lea fakanatula. \u02bbE lava ke fakatupu \u02bbe he ngaahi fakalelei\u02bbi ki he ngaahi motu\u02bba lea ha liliu lahi \u02bbi he fetu'utaki \u02bbi he vaha\u02bba \u02bbo e ngaahi komipiuta mo e kakai, pea \u02bboku \u02bbamanaki \u02bbe toe fakalakalaka ange ia \u02bbi he kaha\u02bbu.\n</code></pre> <p>If we use a simple instructor prompt, even when we ask for the language to be correct, we oftentimes will get English instead. </p> Expand to see documents examples <p>\u053c\u0565\u0566\u057e\u0561\u056f\u0561\u0576 \u0574\u0578\u0564\u0565\u056c\u0576\u0565\u0580\u0568 \u057e\u0565\u0580\u057b\u056b\u0576 \u057f\u0561\u0580\u056b\u0576\u0565\u0580\u056b\u0576 \u0564\u0561\u0580\u0571\u0565\u056c \u0565\u0576 \u0561\u057e\u0565\u056c\u056b \u0570\u0561\u0580\u0578\u0582\u057d\u057f \u0587 \u056f\u0561\u057f\u0561\u0580\u0575\u0561\u056c, \u0570\u0576\u0561\u0580\u0561\u057e\u0578\u0580\u0578\u0582\u0569\u0575\u0578\u0582\u0576 \u0568\u0576\u0571\u0565\u057c\u0565\u056c\u0578\u057e \u057d\u057f\u0565\u0572\u056e\u0565\u056c \u057d\u0561\u0570\u0578\u0582\u0576 \u0587 \u0562\u0576\u0561\u056f\u0561\u0576 \u057f\u0565\u0584\u057d\u057f\u0565\u0580, \u056b\u0576\u0579\u057a\u0565\u057d \u0576\u0561\u0587 \u0563\u0565\u0580\u0561\u0566\u0561\u0576\u0581 \u0561\u0580\u0564\u0575\u0578\u0582\u0576\u0584\u0576\u0565\u0580 \u0581\u0578\u0582\u0581\u0561\u0562\u0565\u0580\u0565\u056c \u0574\u0565\u0584\u0565\u0576\u0561\u0575\u0561\u056f\u0561\u0576 \u0569\u0561\u0580\u0563\u0574\u0561\u0576\u0578\u0582\u0569\u0575\u0561\u0576, \u0570\u0561\u0580\u0581\u0565\u0580\u056b \u057a\u0561\u057f\u0561\u057d\u056d\u0561\u0576\u0574\u0561\u0576 \u0587 \u057d\u057f\u0565\u0572\u056e\u0561\u0563\u0578\u0580\u056e \u057f\u0565\u0584\u057d\u057f\u0565\u0580\u056b \u057d\u057f\u0565\u0572\u056e\u0574\u0561\u0576 \u0576\u0574\u0561\u0576 \u057f\u0561\u0580\u0562\u0565\u0580 \u0561\u057c\u0561\u057b\u0561\u0564\u0580\u0561\u0576\u0584\u0576\u0565\u0580\u0578\u0582\u0574\u0589 \u0531\u0575\u057d \u0574\u0578\u0564\u0565\u056c\u0576\u0565\u0580\u0568 \u0574\u0577\u0561\u056f\u057e\u0578\u0582\u0574 \u0565\u0576 \u0570\u057d\u056f\u0561\u0575\u0561\u056f\u0561\u0576 \u057f\u0565\u0584\u057d\u057f\u0561\u0575\u056b\u0576 \u057f\u057e\u0575\u0561\u056c\u0576\u0565\u0580\u056b \u0570\u056b\u0574\u0561\u0576 \u057e\u0580\u0561 \u0587 \u056f\u0561\u0580\u0578\u0572 \u0565\u0576 \u0562\u057c\u0576\u0565\u056c \u0562\u0576\u0561\u056f\u0561\u0576 \u056c\u0565\u0566\u057e\u056b \u056f\u0561\u057c\u0578\u0582\u0581\u057e\u0561\u056e\u0584\u0576 \u0578\u0582 \u0576\u0580\u0562\u0578\u0582\u0569\u0575\u0578\u0582\u0576\u0576\u0565\u0580\u0568\u055d \u0570\u0565\u0572\u0561\u0583\u0578\u056d\u0578\u0582\u0569\u0575\u0578\u0582\u0576 \u0561\u057c\u0561\u057b\u0561\u0581\u0576\u0565\u056c\u0578\u057e \u0570\u0561\u0574\u0561\u056f\u0561\u0580\u0563\u056b\u0579\u0576\u0565\u0580\u056b \u0587 \u0574\u0561\u0580\u0564\u056f\u0561\u0576\u0581 \u0574\u056b\u057b\u0587 \u0570\u0561\u0572\u0578\u0580\u0564\u0561\u056f\u0581\u0578\u0582\u0569\u0575\u0561\u0576 \u0578\u056c\u0578\u0580\u057f\u0578\u0582\u0574\u0589</p> <p>Mga modelo ng wika ay naging mas sopistikado sa nagdaang mga taon, na nagbibigay-daan sa pagbuo ng mga natural at madaling basahing teksto, at nagpapakita ng mahusay na pagganap sa iba't ibang gawain tulad ng awtomatikong pagsasalin, pagsagot sa mga tanong, at pagbuo ng malikhain na teksto. Ang mga modelo na ito ay sinanay sa napakalaking mga dataset ng teksto at kayang hulihin ang istruktura at mga nuances ng natural na wika. Ang mga pagpapabuti sa mga modelo ng wika ay maaaring magdulot ng rebolusyon sa komunikasyon sa pagitan ng mga computer at tao, at inaasahan ang higit pang pag-unlad sa hinaharap.</p> <p>Ngaahi motu\u02bba lea kuo nau hoko \u02bbo faka\u02bbofo\u02bbofa ange \u02bbi he ngaahi ta\u02bbu fakamuimui ni, \u02bbo fakafaingofua\u02bbi e fakatupu \u02bbo e ngaahi konga tohi \u02bboku lelei mo fakanatula pea \u02bboku nau fakahaa\u02bbi \u02bba e ngaahi ola lelei \u02bbi he ngaahi ng\u0101ue kehekehe \u02bbo hang\u0113 ko e liliu faka\u02bb\u0113tita, tali fehu\u02bbi, mo e fakatupu \u02bbo e konga tohi faka\u02bbatamai. Ko e ako \u02bba e ngaahi motu\u02bba ni \u02bbi he ngaahi seti \u02bbo e fakamatala tohi lahi pea \u02bboku nau malava \u02bbo puke \u02bba e fakafuofua mo e ngaahi me\u02bba iiki \u02bbo e lea fakanatula. \u02bbE lava ke fakatupu \u02bbe he ngaahi fakalelei\u02bbi ki he ngaahi motu\u02bba lea ha liliu lahi \u02bbi he fetu'utaki \u02bbi he vaha\u02bba \u02bbo e ngaahi komipiuta mo e kakai, pea \u02bboku \u02bbamanaki \u02bbe toe fakalakalaka ange ia \u02bbi he kaha\u02bbu.</p> <p>Dil modelleri son y\u0131llarda daha da geli\u015fti, ak\u0131c\u0131 ve do\u011fal metinler \u00fcretmeyi m\u00fcmk\u00fcn k\u0131l\u0131yor ve makine \u00e7evirisi, soru cevaplama ve yarat\u0131c\u0131 metin olu\u015fturma gibi \u00e7e\u015fitli g\u00f6revlerde m\u00fckemmel performans g\u00f6steriyor. Bu modeller, devasa metin veri setlerinde e\u011fitilir ve do\u011fal dilin yap\u0131s\u0131n\u0131 ve n\u00fcanslar\u0131n\u0131 yakalayabilir. Dil modellerindeki iyile\u015ftirmeler, bilgisayarlar ve insanlar aras\u0131ndaki ileti\u015fimde devrim yaratabilir ve gelecekte daha da ilerleme bekleniyor.</p> <p>M\u00f4 h\u00ecnh ng\u00f4n ng\u1eef \u0111\u00e3 tr\u1edf n\u00ean tinh vi h\u01a1n trong nh\u1eefng n\u0103m g\u1ea7n \u0111\u00e2y, cho ph\u00e9p t\u1ea1o ra c\u00e1c v\u0103n b\u1ea3n tr\u00f4i ch\u1ea3y v\u00e0 t\u1ef1 nhi\u00ean, \u0111\u1ed3ng th\u1eddi th\u1ec3 hi\u1ec7n hi\u1ec7u su\u1ea5t xu\u1ea5t s\u1eafc trong c\u00e1c nhi\u1ec7m v\u1ee5 kh\u00e1c nhau nh\u01b0 d\u1ecbch m\u00e1y, tr\u1ea3 l\u1eddi c\u00e2u h\u1ecfi v\u00e0 t\u1ea1o v\u0103n b\u1ea3n s\u00e1ng t\u1ea1o. C\u00e1c m\u00f4 h\u00ecnh n\u00e0y \u0111\u01b0\u1ee3c hu\u1ea5n luy\u1ec7n tr\u00ean c\u00e1c t\u1eadp d\u1eef li\u1ec7u v\u0103n b\u1ea3n kh\u1ed5ng l\u1ed3 v\u00e0 c\u00f3 th\u1ec3 n\u1eafm b\u1eaft c\u1ea5u tr\u00fac v\u00e0 s\u1eafc th\u00e1i c\u1ee7a ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean. Nh\u1eefng c\u1ea3i ti\u1ebfn trong m\u00f4 h\u00ecnh ng\u00f4n ng\u1eef c\u00f3 th\u1ec3 mang l\u1ea1i cu\u1ed9c c\u00e1ch m\u1ea1ng trong giao ti\u1ebfp gi\u1eefa m\u00e1y t\u00ednh v\u00e0 con ng\u01b0\u1eddi, v\u00e0 ng\u01b0\u1eddi ta k\u1ef3 v\u1ecdng s\u1ebd c\u00f3 nh\u1eefng ti\u1ebfn b\u1ed9 h\u01a1n n\u1eefa trong t\u01b0\u01a1ng lai.</p> <p>Les mod\u00e8les de langage sont devenus de plus en plus sophistiqu\u00e9s ces derni\u00e8res ann\u00e9es, permettant de g\u00e9n\u00e9rer des textes fluides et naturels, et de performer dans une vari\u00e9t\u00e9 de t\u00e2ches telles que la traduction automatique, la r\u00e9ponse aux questions et la g\u00e9n\u00e9ration de texte cr\u00e9atif. Entra\u00een\u00e9s sur d'immenses ensembles de donn\u00e9es textuelles, ces mod\u00e8les sont capables de capturer la structure et les nuances du langage naturel, ouvrant la voie \u00e0 une r\u00e9volution dans la communication entre les ordinateurs et les humains.</p> <p>\u8fd1\u5e74\u6765,\u8bed\u8a00\u6a21\u578b\u53d8\u5f97\u8d8a\u6765\u8d8a\u590d\u6742,\u80fd\u591f\u751f\u6210\u6d41\u7545\u81ea\u7136\u7684\u6587\u672c,\u5e76\u5728\u673a\u5668\u7ffb\u8bd1\u3001\u95ee\u7b54\u548c\u521b\u610f\u6587\u672c\u751f\u6210\u7b49\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u8fd9\u4e9b\u6a21\u578b\u5728\u6d77\u91cf\u6587\u672c\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3,\u53ef\u4ee5\u6355\u6349\u81ea\u7136\u8bed\u8a00\u7684\u7ed3\u6784\u548c\u7ec6\u5fae\u5dee\u522b\u3002\u8bed\u8a00\u6a21\u578b\u7684\u6539\u8fdb\u6709\u671b\u5f7b\u5e95\u6539\u53d8\u8ba1\u7b97\u673a\u548c\u4eba\u7c7b\u4e4b\u95f4\u7684\u4ea4\u6d41\u65b9\u5f0f,\u672a\u6765\u6709\u671b\u5b9e\u73b0\u66f4\u5927\u7684\u7a81\u7834\u3002</p> <p>In den letzten Jahren sind Sprachmodelle immer ausgefeilter geworden und k\u00f6nnen fl\u00fcssige, nat\u00fcrlich klingende Texte generieren und in verschiedenen Aufgaben wie maschineller \u00dcbersetzung, Beantwortung von Fragen und Generierung kreativer Texte hervorragende Leistungen erbringen. Diese Modelle werden auf riesigen Textdatens\u00e4tzen trainiert und k\u00f6nnen die Struktur und Nuancen nat\u00fcrlicher Sprache erfassen, was zu einer Revolution in der Kommunikation zwischen Computern und Menschen f\u00fchren k\u00f6nnte.</p> <p>\u092a\u093f\u091b\u0932\u0947 \u0915\u0941\u091b \u0935\u0930\u094d\u0937\u094b\u0902 \u092e\u0947\u0902 \u092d\u093e\u0937\u093e \u092e\u0949\u0921\u0932 \u092c\u0939\u0941\u0924 \u0905\u0927\u093f\u0915 \u092a\u0930\u093f\u0937\u094d\u0915\u0943\u0924 \u0939\u094b \u0917\u090f \u0939\u0948\u0902, \u091c\u094b \u092a\u094d\u0930\u093e\u0915\u0943\u0924\u093f\u0915 \u0914\u0930 \u092a\u094d\u0930\u0935\u093e\u0939\u092e\u092f \u092a\u093e\u0920 \u0909\u0924\u094d\u092a\u0928\u094d\u0928 \u0915\u0930 \u0938\u0915\u0924\u0947 \u0939\u0948\u0902, \u0914\u0930 \u092e\u0936\u0940\u0928 \u0905\u0928\u0941\u0935\u093e\u0926, \u092a\u094d\u0930\u0936\u094d\u0928\u094b\u0924\u094d\u0924\u0930, \u0914\u0930 \u0930\u091a\u0928\u093e\u0924\u094d\u092e\u0915 \u092a\u093e\u0920 \u0909\u0924\u094d\u092a\u093e\u0926\u0928 \u091c\u0948\u0938\u0947 \u0935\u093f\u092d\u093f\u0928\u094d\u0928 \u0915\u093e\u0930\u094d\u092f\u094b\u0902 \u092e\u0947\u0902 \u0909\u0924\u094d\u0915\u0943\u0937\u094d\u091f \u092a\u094d\u0930\u0926\u0930\u094d\u0936\u0928 \u0915\u0930 \u0938\u0915\u0924\u0947 \u0939\u0948\u0902\u0964 \u092f\u0947 \u092e\u0949\u0921\u0932 \u0935\u093f\u0936\u093e\u0932 \u092a\u093e\u0920 \u0921\u0947\u091f\u093e\u0938\u0947\u091f \u092a\u0930 \u092a\u094d\u0930\u0936\u093f\u0915\u094d\u0937\u093f\u0924 \u0939\u094b\u0924\u0947 \u0939\u0948\u0902 \u0914\u0930 \u092a\u094d\u0930\u093e\u0915\u0943\u0924\u093f\u0915 \u092d\u093e\u0937\u093e \u0915\u0940 \u0938\u0902\u0930\u091a\u0928\u093e \u0914\u0930 \u092c\u093e\u0930\u0940\u0915\u093f\u092f\u094b\u0902 \u0915\u094b \u0938\u092e\u091d \u0938\u0915\u0924\u0947 \u0939\u0948\u0902\u0964 \u092d\u093e\u0937\u093e \u092e\u0949\u0921\u0932 \u092e\u0947\u0902 \u0938\u0941\u0927\u093e\u0930 \u0915\u0902\u092a\u094d\u092f\u0942\u091f\u0930 \u0914\u0930 \u092e\u093e\u0928\u0935 \u0915\u0947 \u092c\u0940\u091a \u0938\u0902\u0935\u093e\u0926 \u092e\u0947\u0902 \u0915\u094d\u0930\u093e\u0902\u0924\u093f \u0932\u093e \u0938\u0915\u0924\u093e \u0939\u0948, \u0914\u0930 \u092d\u0935\u093f\u0937\u094d\u092f \u092e\u0947\u0902 \u0914\u0930 \u092a\u094d\u0930\u0917\u0924\u093f \u0915\u0940 \u0909\u092e\u094d\u092e\u0940\u0926 \u0939\u0948\u0964</p> <p>\u8fd1\u5e74\u3001\u8a00\u8a9e\u30e2\u30c7\u30eb\u306f\u975e\u5e38\u306b\u6d17\u7df4\u3055\u308c\u3001\u81ea\u7136\u3067\u6d41\u66a2\u306a\u30c6\u30ad\u30b9\u30c8\u3092\u751f\u6210\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u3001\u6a5f\u68b0\u7ffb\u8a33\u3001\u8cea\u554f\u5fdc\u7b54\u3001\u30af\u30ea\u30a8\u30a4\u30c6\u30a3\u30d6\u306a\u30c6\u30ad\u30b9\u30c8\u751f\u6210\u306a\u3069\u3001\u69d8\u3005\u306a\u30bf\u30b9\u30af\u3067\u512a\u308c\u305f\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u3092\u767a\u63ee\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u30e2\u30c7\u30eb\u306f\u81a8\u5927\u306a\u30c6\u30ad\u30b9\u30c8\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u5b66\u7fd2\u3055\u308c\u3001\u81ea\u7136\u8a00\u8a9e\u306e\u69cb\u9020\u3068\u30cb\u30e5\u30a2\u30f3\u30b9\u3092\u6349\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u6539\u5584\u306b\u3088\u308a\u3001\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30fc\u3068\u4eba\u9593\u306e\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3\u306b\u9769\u547d\u304c\u8d77\u3053\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u3001\u5c06\u6765\u306e\u3055\u3089\u306a\u308b\u9032\u6b69\u304c\u671f\u5f85\u3055\u308c\u3066\u3044\u307e\u3059\u3002</p> <p>In this example, we'll do something very simple, asking for the language to be correct. And generating a base model that only asks for a summary. To test we will use the library <code>langdetect</code> to detect the language of the text. To challenge us even more, we'll limit ourselves using 3.5 rather than 4 in order to use a 'dumber' model.</p> <pre><code>from pydantic import BaseModel, Field\nfrom instructor import patch\nfrom openai import AsyncOpenAI\nfrom langdetect import detect\n\ndocs = # To see the text, expand the notes above. \n\n# Patch the OpenAI client to enable response_model\nclient = patch(AsyncOpenAI())\n\n\nclass GeneratedSummary(BaseModel):\n    summary: str\n\nasync def summarize_text(text: str):\n    response = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=GeneratedSummary,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Generate a concise summary in the language of the article. \",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Summarize the following text in a concise way:\\n{text}\",\n            },\n        ],\n    )  # type: ignore\n    return response.summary, text\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    async def main():\n        results = await asyncio.gather(*[summarize_text(doc) for doc in docs])\n        for summary, doc in results:\n            source_lang = detect(doc)\n            target_lang = detect(summary)\n            print(\n                f\"Source: {source_lang}, Summary: {target_lang}, Match: {source_lang == target_lang}\"\n            )\n\n    asyncio.run(main())\n    \"\"\"\n    Source: et, Summary: en, Match: False\n    Source: tl, Summary: tl, Match: True\n    Source: sw, Summary: en, Match: False\n    Source: tr, Summary: tr, Match: True\n    Source: vi, Summary: en, Match: False\n    Source: fr, Summary: fr, Match: True\n    Source: zh-cn, Summary: en, Match: False\n    Source: de, Summary: de, Match: True\n    Source: hi, Summary: en, Match: False\n    Source: ja, Summary: en, Match: False\n    \"\"\"\n</code></pre> <p>In this example, you'll notice that not all the languages are matching. Many of them respond in English, and so we get pretty terrible results. Only 3 out of 9 passed!</p>"},{"location":"blog/2024/03/28/matching-language-summaries/#reiterating-instructions","title":"Reiterating instructions","text":"<p>A simple trick that I found to work very well is to add a language detection attribute before the summary. </p> <pre><code>class GeneratedSummary(BaseModel):\n    detected_language: str = Field(\n        description=\"The language code of the original article. The summary must be generated in this same language.\",\n    )\n    summary: str\n</code></pre> <p>Just by adding this single attribute, we end up getting 100% correctness on language matches. If you want to see for yourself, checkout the complete script below </p> <pre><code>from pydantic import BaseModel, Field\nfrom instructor import patch\nfrom openai import AsyncOpenAI\nfrom langdetect import detect\n\ndocs = map(\n    lambda x: x.strip(),\n    \"\"\"\n\u053c\u0565\u0566\u057e\u0561\u056f\u0561\u0576 \u0574\u0578\u0564\u0565\u056c\u0576\u0565\u0580\u0568 \u057e\u0565\u0580\u057b\u056b\u0576 \u057f\u0561\u0580\u056b\u0576\u0565\u0580\u056b\u0576 \u0564\u0561\u0580\u0571\u0565\u056c \u0565\u0576 \u0561\u057e\u0565\u056c\u056b \u0570\u0561\u0580\u0578\u0582\u057d\u057f \u0587 \u056f\u0561\u057f\u0561\u0580\u0575\u0561\u056c, \u0570\u0576\u0561\u0580\u0561\u057e\u0578\u0580\u0578\u0582\u0569\u0575\u0578\u0582\u0576 \u0568\u0576\u0571\u0565\u057c\u0565\u056c\u0578\u057e \u057d\u057f\u0565\u0572\u056e\u0565\u056c \u057d\u0561\u0570\u0578\u0582\u0576 \u0587 \u0562\u0576\u0561\u056f\u0561\u0576 \u057f\u0565\u0584\u057d\u057f\u0565\u0580, \u056b\u0576\u0579\u057a\u0565\u057d \u0576\u0561\u0587 \u0563\u0565\u0580\u0561\u0566\u0561\u0576\u0581 \u0561\u0580\u0564\u0575\u0578\u0582\u0576\u0584\u0576\u0565\u0580 \u0581\u0578\u0582\u0581\u0561\u0562\u0565\u0580\u0565\u056c \u0574\u0565\u0584\u0565\u0576\u0561\u0575\u0561\u056f\u0561\u0576 \u0569\u0561\u0580\u0563\u0574\u0561\u0576\u0578\u0582\u0569\u0575\u0561\u0576, \u0570\u0561\u0580\u0581\u0565\u0580\u056b \u057a\u0561\u057f\u0561\u057d\u056d\u0561\u0576\u0574\u0561\u0576 \u0587 \u057d\u057f\u0565\u0572\u056e\u0561\u0563\u0578\u0580\u056e \u057f\u0565\u0584\u057d\u057f\u0565\u0580\u056b \u057d\u057f\u0565\u0572\u056e\u0574\u0561\u0576 \u0576\u0574\u0561\u0576 \u057f\u0561\u0580\u0562\u0565\u0580 \u0561\u057c\u0561\u057b\u0561\u0564\u0580\u0561\u0576\u0584\u0576\u0565\u0580\u0578\u0582\u0574\u0589 \u0531\u0575\u057d \u0574\u0578\u0564\u0565\u056c\u0576\u0565\u0580\u0568 \u0574\u0577\u0561\u056f\u057e\u0578\u0582\u0574 \u0565\u0576 \u0570\u057d\u056f\u0561\u0575\u0561\u056f\u0561\u0576 \u057f\u0565\u0584\u057d\u057f\u0561\u0575\u056b\u0576 \u057f\u057e\u0575\u0561\u056c\u0576\u0565\u0580\u056b \u0570\u056b\u0574\u0561\u0576 \u057e\u0580\u0561 \u0587 \u056f\u0561\u0580\u0578\u0572 \u0565\u0576 \u0562\u057c\u0576\u0565\u056c \u0562\u0576\u0561\u056f\u0561\u0576 \u056c\u0565\u0566\u057e\u056b \u056f\u0561\u057c\u0578\u0582\u0581\u057e\u0561\u056e\u0584\u0576 \u0578\u0582 \u0576\u0580\u0562\u0578\u0582\u0569\u0575\u0578\u0582\u0576\u0576\u0565\u0580\u0568\u055d \u0570\u0565\u0572\u0561\u0583\u0578\u056d\u0578\u0582\u0569\u0575\u0578\u0582\u0576 \u0561\u057c\u0561\u057b\u0561\u0581\u0576\u0565\u056c\u0578\u057e \u0570\u0561\u0574\u0561\u056f\u0561\u0580\u0563\u056b\u0579\u0576\u0565\u0580\u056b \u0587 \u0574\u0561\u0580\u0564\u056f\u0561\u0576\u0581 \u0574\u056b\u057b\u0587 \u0570\u0561\u0572\u0578\u0580\u0564\u0561\u056f\u0581\u0578\u0582\u0569\u0575\u0561\u0576 \u0578\u056c\u0578\u0580\u057f\u0578\u0582\u0574\u0589\n\n---\n\nMga modelo ng wika ay naging mas sopistikado sa nagdaang mga taon, na nagbibigay-daan sa pagbuo ng mga natural at madaling basahing teksto, at nagpapakita ng mahusay na pagganap sa iba't ibang gawain tulad ng awtomatikong pagsasalin, pagsagot sa mga tanong, at pagbuo ng malikhain na teksto. Ang mga modelo na ito ay sinanay sa napakalaking mga dataset ng teksto at kayang hulihin ang istruktura at mga nuances ng natural na wika. Ang mga pagpapabuti sa mga modelo ng wika ay maaaring magdulot ng rebolusyon sa komunikasyon sa pagitan ng mga computer at tao, at inaasahan ang higit pang pag-unlad sa hinaharap.\n\n---\n\nNgaahi motu\u02bba lea kuo nau hoko \u02bbo faka\u02bbofo\u02bbofa ange \u02bbi he ngaahi ta\u02bbu fakamuimui ni, \u02bbo fakafaingofua\u02bbi e fakatupu \u02bbo e ngaahi konga tohi \u02bboku lelei mo fakanatula pea \u02bboku nau fakahaa\u02bbi \u02bba e ngaahi ola lelei \u02bbi he ngaahi ng\u0101ue kehekehe \u02bbo hang\u0113 ko e liliu faka\u02bb\u0113tita, tali fehu\u02bbi, mo e fakatupu \u02bbo e konga tohi faka\u02bbatamai. Ko e ako \u02bba e ngaahi motu\u02bba ni \u02bbi he ngaahi seti \u02bbo e fakamatala tohi lahi pea \u02bboku nau malava \u02bbo puke \u02bba e fakafuofua mo e ngaahi me\u02bba iiki \u02bbo e lea fakanatula. \u02bbE lava ke fakatupu \u02bbe he ngaahi fakalelei\u02bbi ki he ngaahi motu\u02bba lea ha liliu lahi \u02bbi he fetu'utaki \u02bbi he vaha\u02bba \u02bbo e ngaahi komipiuta mo e kakai, pea \u02bboku \u02bbamanaki \u02bbe toe fakalakalaka ange ia \u02bbi he kaha\u02bbu.\n\n---\n\nDil modelleri son y\u0131llarda daha da geli\u015fti, ak\u0131c\u0131 ve do\u011fal metinler \u00fcretmeyi m\u00fcmk\u00fcn k\u0131l\u0131yor ve makine \u00e7evirisi, soru cevaplama ve yarat\u0131c\u0131 metin olu\u015fturma gibi \u00e7e\u015fitli g\u00f6revlerde m\u00fckemmel performans g\u00f6steriyor. Bu modeller, devasa metin veri setlerinde e\u011fitilir ve do\u011fal dilin yap\u0131s\u0131n\u0131 ve n\u00fcanslar\u0131n\u0131 yakalayabilir. Dil modellerindeki iyile\u015ftirmeler, bilgisayarlar ve insanlar aras\u0131ndaki ileti\u015fimde devrim yaratabilir ve gelecekte daha da ilerleme bekleniyor.\n\n---\n\nM\u00f4 h\u00ecnh ng\u00f4n ng\u1eef \u0111\u00e3 tr\u1edf n\u00ean tinh vi h\u01a1n trong nh\u1eefng n\u0103m g\u1ea7n \u0111\u00e2y, cho ph\u00e9p t\u1ea1o ra c\u00e1c v\u0103n b\u1ea3n tr\u00f4i ch\u1ea3y v\u00e0 t\u1ef1 nhi\u00ean, \u0111\u1ed3ng th\u1eddi th\u1ec3 hi\u1ec7n hi\u1ec7u su\u1ea5t xu\u1ea5t s\u1eafc trong c\u00e1c nhi\u1ec7m v\u1ee5 kh\u00e1c nhau nh\u01b0 d\u1ecbch m\u00e1y, tr\u1ea3 l\u1eddi c\u00e2u h\u1ecfi v\u00e0 t\u1ea1o v\u0103n b\u1ea3n s\u00e1ng t\u1ea1o. C\u00e1c m\u00f4 h\u00ecnh n\u00e0y \u0111\u01b0\u1ee3c hu\u1ea5n luy\u1ec7n tr\u00ean c\u00e1c t\u1eadp d\u1eef li\u1ec7u v\u0103n b\u1ea3n kh\u1ed5ng l\u1ed3 v\u00e0 c\u00f3 th\u1ec3 n\u1eafm b\u1eaft c\u1ea5u tr\u00fac v\u00e0 s\u1eafc th\u00e1i c\u1ee7a ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean. Nh\u1eefng c\u1ea3i ti\u1ebfn trong m\u00f4 h\u00ecnh ng\u00f4n ng\u1eef c\u00f3 th\u1ec3 mang l\u1ea1i cu\u1ed9c c\u00e1ch m\u1ea1ng trong giao ti\u1ebfp gi\u1eefa m\u00e1y t\u00ednh v\u00e0 con ng\u01b0\u1eddi, v\u00e0 ng\u01b0\u1eddi ta k\u1ef3 v\u1ecdng s\u1ebd c\u00f3 nh\u1eefng ti\u1ebfn b\u1ed9 h\u01a1n n\u1eefa trong t\u01b0\u01a1ng lai.\n\n---\n\nLes mod\u00e8les de langage sont devenus de plus en plus sophistiqu\u00e9s ces derni\u00e8res ann\u00e9es, permettant de g\u00e9n\u00e9rer des textes fluides et naturels, et de performer dans une vari\u00e9t\u00e9 de t\u00e2ches telles que la traduction automatique, la r\u00e9ponse aux questions et la g\u00e9n\u00e9ration de texte cr\u00e9atif. Entra\u00een\u00e9s sur d'immenses ensembles de donn\u00e9es textuelles, ces mod\u00e8les sont capables de capturer la structure et les nuances du langage naturel, ouvrant la voie \u00e0 une r\u00e9volution dans la communication entre les ordinateurs et les humains.\n\n---\n\n\u8fd1\u5e74\u6765,\u8bed\u8a00\u6a21\u578b\u53d8\u5f97\u8d8a\u6765\u8d8a\u590d\u6742,\u80fd\u591f\u751f\u6210\u6d41\u7545\u81ea\u7136\u7684\u6587\u672c,\u5e76\u5728\u673a\u5668\u7ffb\u8bd1\u3001\u95ee\u7b54\u548c\u521b\u610f\u6587\u672c\u751f\u6210\u7b49\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u8fd9\u4e9b\u6a21\u578b\u5728\u6d77\u91cf\u6587\u672c\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3,\u53ef\u4ee5\u6355\u6349\u81ea\u7136\u8bed\u8a00\u7684\u7ed3\u6784\u548c\u7ec6\u5fae\u5dee\u522b\u3002\u8bed\u8a00\u6a21\u578b\u7684\u6539\u8fdb\u6709\u671b\u5f7b\u5e95\u6539\u53d8\u8ba1\u7b97\u673a\u548c\u4eba\u7c7b\u4e4b\u95f4\u7684\u4ea4\u6d41\u65b9\u5f0f,\u672a\u6765\u6709\u671b\u5b9e\u73b0\u66f4\u5927\u7684\u7a81\u7834\u3002\n\n---\n\nIn den letzten Jahren sind Sprachmodelle immer ausgefeilter geworden und k\u00f6nnen fl\u00fcssige, nat\u00fcrlich klingende Texte generieren und in verschiedenen Aufgaben wie maschineller \u00dcbersetzung, Beantwortung von Fragen und Generierung kreativer Texte hervorragende Leistungen erbringen. Diese Modelle werden auf riesigen Textdatens\u00e4tzen trainiert und k\u00f6nnen die Struktur und Nuancen nat\u00fcrlicher Sprache erfassen, was zu einer Revolution in der Kommunikation zwischen Computern und Menschen f\u00fchren k\u00f6nnte.\n\n---\n\n\u092a\u093f\u091b\u0932\u0947 \u0915\u0941\u091b \u0935\u0930\u094d\u0937\u094b\u0902 \u092e\u0947\u0902 \u092d\u093e\u0937\u093e \u092e\u0949\u0921\u0932 \u092c\u0939\u0941\u0924 \u0905\u0927\u093f\u0915 \u092a\u0930\u093f\u0937\u094d\u0915\u0943\u0924 \u0939\u094b \u0917\u090f \u0939\u0948\u0902, \u091c\u094b \u092a\u094d\u0930\u093e\u0915\u0943\u0924\u093f\u0915 \u0914\u0930 \u092a\u094d\u0930\u0935\u093e\u0939\u092e\u092f \u092a\u093e\u0920 \u0909\u0924\u094d\u092a\u0928\u094d\u0928 \u0915\u0930 \u0938\u0915\u0924\u0947 \u0939\u0948\u0902, \u0914\u0930 \u092e\u0936\u0940\u0928 \u0905\u0928\u0941\u0935\u093e\u0926, \u092a\u094d\u0930\u0936\u094d\u0928\u094b\u0924\u094d\u0924\u0930, \u0914\u0930 \u0930\u091a\u0928\u093e\u0924\u094d\u092e\u0915 \u092a\u093e\u0920 \u0909\u0924\u094d\u092a\u093e\u0926\u0928 \u091c\u0948\u0938\u0947 \u0935\u093f\u092d\u093f\u0928\u094d\u0928 \u0915\u093e\u0930\u094d\u092f\u094b\u0902 \u092e\u0947\u0902 \u0909\u0924\u094d\u0915\u0943\u0937\u094d\u091f \u092a\u094d\u0930\u0926\u0930\u094d\u0936\u0928 \u0915\u0930 \u0938\u0915\u0924\u0947 \u0939\u0948\u0902\u0964 \u092f\u0947 \u092e\u0949\u0921\u0932 \u0935\u093f\u0936\u093e\u0932 \u092a\u093e\u0920 \u0921\u0947\u091f\u093e\u0938\u0947\u091f \u092a\u0930 \u092a\u094d\u0930\u0936\u093f\u0915\u094d\u0937\u093f\u0924 \u0939\u094b\u0924\u0947 \u0939\u0948\u0902 \u0914\u0930 \u092a\u094d\u0930\u093e\u0915\u0943\u0924\u093f\u0915 \u092d\u093e\u0937\u093e \u0915\u0940 \u0938\u0902\u0930\u091a\u0928\u093e \u0914\u0930 \u092c\u093e\u0930\u0940\u0915\u093f\u092f\u094b\u0902 \u0915\u094b \u0938\u092e\u091d \u0938\u0915\u0924\u0947 \u0939\u0948\u0902\u0964 \u092d\u093e\u0937\u093e \u092e\u0949\u0921\u0932 \u092e\u0947\u0902 \u0938\u0941\u0927\u093e\u0930 \u0915\u0902\u092a\u094d\u092f\u0942\u091f\u0930 \u0914\u0930 \u092e\u093e\u0928\u0935 \u0915\u0947 \u092c\u0940\u091a \u0938\u0902\u0935\u093e\u0926 \u092e\u0947\u0902 \u0915\u094d\u0930\u093e\u0902\u0924\u093f \u0932\u093e \u0938\u0915\u0924\u093e \u0939\u0948, \u0914\u0930 \u092d\u0935\u093f\u0937\u094d\u092f \u092e\u0947\u0902 \u0914\u0930 \u092a\u094d\u0930\u0917\u0924\u093f \u0915\u0940 \u0909\u092e\u094d\u092e\u0940\u0926 \u0939\u0948\u0964\n\n---\n\n\u8fd1\u5e74\u3001\u8a00\u8a9e\u30e2\u30c7\u30eb\u306f\u975e\u5e38\u306b\u6d17\u7df4\u3055\u308c\u3001\u81ea\u7136\u3067\u6d41\u66a2\u306a\u30c6\u30ad\u30b9\u30c8\u3092\u751f\u6210\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u3001\u6a5f\u68b0\u7ffb\u8a33\u3001\u8cea\u554f\u5fdc\u7b54\u3001\u30af\u30ea\u30a8\u30a4\u30c6\u30a3\u30d6\u306a\u30c6\u30ad\u30b9\u30c8\u751f\u6210\u306a\u3069\u3001\u69d8\u3005\u306a\u30bf\u30b9\u30af\u3067\u512a\u308c\u305f\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u3092\u767a\u63ee\u3057\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u30e2\u30c7\u30eb\u306f\u81a8\u5927\u306a\u30c6\u30ad\u30b9\u30c8\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u5b66\u7fd2\u3055\u308c\u3001\u81ea\u7136\u8a00\u8a9e\u306e\u69cb\u9020\u3068\u30cb\u30e5\u30a2\u30f3\u30b9\u3092\u6349\u3048\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u6539\u5584\u306b\u3088\u308a\u3001\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u30fc\u3068\u4eba\u9593\u306e\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3\u306b\u9769\u547d\u304c\u8d77\u3053\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u3001\u5c06\u6765\u306e\u3055\u3089\u306a\u308b\u9032\u6b69\u304c\u671f\u5f85\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n\"\"\".split(\"---\"),\n)\n\n# Patch the OpenAI client to enable response_model\nclient = patch(AsyncOpenAI())\n\n\nclass GeneratedSummary(BaseModel):\n    detected_language: str = Field(\n        description=\"The language code of the original article. The summary must be generated in this same language.\",\n    )\n    summary: str\n\nasync def summarize_text(text: str):\n    response = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=GeneratedSummary,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Generate a concise summary in the language of the article. \",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Summarize the following text in a concise way:\\n{text}\",\n            },\n        ],\n    )  # type: ignore\n    return response.summary, text\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    async def main():\n        results = await asyncio.gather(*[summarize_text(doc) for doc in docs])\n        for summary, doc in results:\n            source_lang = detect(doc)\n            target_lang = detect(summary)\n            print(\n                f\"Source: {source_lang}, Summary: {target_lang}, Match: {source_lang == target_lang}\"\n            )\n\n    asyncio.run(main())\n    \"\"\"\n    Source: et, Summary: et, Match: True\n    Source: tl, Summary: tl, Match: True\n    Source: sw, Summary: sw, Match: True\n    Source: tr, Summary: tr, Match: True\n    Source: vi, Summary: vi, Match: True\n    Source: fr, Summary: fr, Match: True\n    Source: zh-cn, Summary: zh-cn, Match: True\n    Source: de, Summary: de, Match: True\n    Source: hi, Summary: hi, Match: True\n    Source: ja, Summary: ja, Match: True\n    \"\"\"\n</code></pre>"},{"location":"blog/2024/03/07/open-source-local-structured-output-pydantic-json-openai/","title":"Structured Output for Open Source and Local LLMS","text":"<p>Originally, Instructor facilitated API interactions solely via the OpenAI SDK, with an emphasis on function call by incorporating Pydantic for structured data validation and serialization. </p> <p>As the year progressed, we expanded our toolkit by integrating JSON mode, thus enhancing our adaptability to vision models and open source models. This advancement now enables us to support an extensive range of models, from GPT and Mistral to virtually any model accessible through Ollama and Hugging Face, facilitated by llama-cpp-python. For more insights into leveraging JSON mode with various models, refer back to our detailed guide on Patching.</p> <p>If you want to check out a course on how to use Instructor with Pydantic, check out our course on Steering language models towards structured outputs..</p>"},{"location":"blog/2024/03/07/open-source-local-structured-output-pydantic-json-openai/#exploring-different-openai-clients-with-instructor","title":"Exploring Different OpenAI Clients with Instructor","text":"<p>The landscape of OpenAI clients is diverse, each offering unique functionalities tailored to different needs. Below, we explore some of the notable clients integrated with Instructor, providing structured outputs and enhanced capabilities, complete with examples of how to initialize and patch each client.</p>"},{"location":"blog/2024/03/07/open-source-local-structured-output-pydantic-json-openai/#local-models","title":"Local Models","text":""},{"location":"blog/2024/03/07/open-source-local-structured-output-pydantic-json-openai/#ollama-a-new-frontier-for-local-models","title":"Ollama: A New Frontier for Local Models","text":"<p>Ollama's introduction significantly impacts the open-source community, offering a way to merge structured outputs with local models via JSON schema, as detailed in our Ollama documentation.</p> <p>For an in-depth exploration of Ollama, including setup and advanced features, refer to the documentation. The Ollama official website also provides essential resources, model downloads, and community support for newcomers.</p> <pre><code>ollama run llama2\n</code></pre> <pre><code>from openai import OpenAI\nfrom pydantic import BaseModel\nimport instructor\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n# enables `response_model` in create call\nclient = instructor.from_openai(\n    OpenAI(\n        base_url=\"http://localhost:11434/v1\",\n        api_key=\"ollama\",  # required, but unused\n    ),\n    mode=instructor.Mode.JSON,\n)\n\nuser = client.chat.completions.create(\n    model=\"llama2\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Jason is 30 years old\",\n        }\n    ],\n    response_model=UserDetail,\n)\n\nprint(user)\n#&gt; name='Jason' age=30\n</code></pre>"},{"location":"blog/2024/03/07/open-source-local-structured-output-pydantic-json-openai/#llama-cpp-python","title":"llama-cpp-python","text":"<p>Open-source LLMS are gaining popularity, and llama-cpp-python has made the <code>llama-cpp</code> model available to obtain structured outputs using JSON schema via a mixture of constrained sampling and speculative decoding. They also support a OpenAI compatible client, which can be used to obtain structured output as an in-process mechanism to avoid any network dependency.</p> <p>For those interested in leveraging the power of llama-cpp-python for structured outputs, here's a quick example:</p> <pre><code>import llama_cpp\nimport instructor\n\nfrom llama_cpp.llama_speculative import LlamaPromptLookupDecoding\nfrom pydantic import BaseModel\n\n\nllama = llama_cpp.Llama(\n    model_path=\"../../models/OpenHermes-2.5-Mistral-7B-GGUF/openhermes-2.5-mistral-7b.Q4_K_M.gguf\",\n    n_gpu_layers=-1,\n    chat_format=\"chatml\",\n    n_ctx=2048,\n    draft_model=LlamaPromptLookupDecoding(num_pred_tokens=2),\n    logits_all=True,\n    verbose=False,\n)\n\n\ncreate = instructor.patch(\n    create=llama.create_chat_completion_openai_v1,\n    mode=instructor.Mode.JSON_SCHEMA, \n)\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\nuser = create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract `Jason is 30 years old`\",\n        }\n    ],\n    response_model=UserDetail,\n)\n\nprint(user)\n#&gt; name='Jason' age=30\n</code></pre>"},{"location":"blog/2024/03/07/open-source-local-structured-output-pydantic-json-openai/#alternative-providers","title":"Alternative Providers","text":""},{"location":"blog/2024/03/07/open-source-local-structured-output-pydantic-json-openai/#anyscale","title":"Anyscale","text":"<p>Anyscale's Mistral model, as detailed in our Anyscale documentation and on Anyscale's official documentation, introduces the ability to obtain structured outputs using JSON schema.</p> <pre><code>export ANYSCALE_API_KEY=\"your-api-key\"\n</code></pre> <pre><code>import os\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nimport instructor\n\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\n\n# enables `response_model` in create call\nclient = instructor.from_openai(\n    OpenAI(\n        base_url=\"https://api.endpoints.anyscale.com/v1\",\n        api_key=os.environ[\"ANYSCALE_API_KEY\"],\n    ),\n    # This uses Anyscale's json schema output mode\n    mode=instructor.Mode.JSON_SCHEMA,\n)\n\nresp = client.chat.completions.create(\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a world class extractor\"},\n        {\"role\": \"user\", \"content\": 'Extract the following entities: \"Jason is 20\"'},\n    ],\n    response_model=UserDetails,\n)\nprint(resp)\n#&gt; name='Jason' age=20\n</code></pre>"},{"location":"blog/2024/03/07/open-source-local-structured-output-pydantic-json-openai/#groq","title":"Groq","text":"<p>Groq's platform, detailed further in our Groq documentation and on Groq's official documentation, offers a unique approach to processing with its tensor architecture. This innovation significantly enhances the performance of structured output processing.</p> <pre><code>export GROQ_API_KEY=\"your-api-key\"\n</code></pre> <pre><code>import os\nimport instructor\nimport groq\nfrom pydantic import BaseModel\n\nclient = qrog.Groq(\n    api_key=os.environ.get(\"GROQ_API_KEY\"),\n)\n\n# By default, the patch function will patch the ChatCompletion.create and ChatCompletion.create methods to support the response_model parameter\nclient = instructor.from_openai(client, mode=instructor.Mode.MD_JSON)\n\n\n# Now, we can use the response_model parameter using only a base model\n# rather than having to use the OpenAISchema class\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n\nuser: UserExtract = client.chat.completions.create(\n    model=\"mixtral-8x7b-32768\",\n    response_model=UserExtract,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nassert isinstance(user, UserExtract), \"Should be instance of UserExtract\"\nprint(user)\n#&gt; name='jason' age=25\n\"\"\"\n</code></pre>"},{"location":"blog/2024/03/07/open-source-local-structured-output-pydantic-json-openai/#together-ai","title":"Together AI","text":"<p>Together AI, when combined with Instructor, offers a seamless experience for developers looking to leverage structured outputs in their applications. For more details, refer to our Together AI documentation and explore the patching guide to enhance your applications.</p> <pre><code>export TOGETHER_API_KEY=\"your-api-key\"\n</code></pre> <pre><code>import os\nimport openai\nfrom pydantic import BaseModel\nimport instructor\n\nclient = openai.OpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n)\n\nclient = instructor.from_openai(client, mode=instructor.Mode.TOOLS)\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n\nuser: UserExtract = client.chat.completions.create(\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n    response_model=UserExtract,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nassert isinstance(user, UserExtract), \"Should be instance of UserExtract\"\nprint(user)\n\n#&gt; name='jason' age=25\n</code></pre>"},{"location":"blog/2024/03/07/open-source-local-structured-output-pydantic-json-openai/#mistral","title":"Mistral","text":"<p>For those interested in exploring the capabilities of Mistral Large with Instructor, we highly recommend checking out our comprehensive guide on Mistral Large.</p> <pre><code>import instructor\n\nfrom pydantic import BaseModel\nfrom mistralai.client import MistralClient\n\nclient = MistralClient()\n\npatched_chat = instructor.from_openai(create=client.chat, mode=instructor.Mode.MISTRAL_TOOLS)\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\nresp = patched_chat(\n    model=\"mistral-large-latest\",\n    response_model=UserDetails,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f'Extract the following entities: \"Jason is 20\"',\n        },\n    ],\n)\nprint(resp)\n#&gt; name='Jason' age=20\n</code></pre>"},{"location":"blog/2024/07/17/parea-for-observing-testing--fine-tuning-of-instructor/","title":"Parea for Observing, Testing &amp; Fine-tuning of Instructor","text":"<p>Parea is a platform that enables teams to monitor, collaborate, test &amp; label for LLM applications. In this blog we will explore how Parea can be used to enhance the OpenAI client alongside <code>instructor</code> and debug + improve <code>instructor</code> calls. Parea has some features which makes it particularly useful for <code>instructor</code>:</p> <ul> <li>it automatically groups any LLM calls due to reties under a single trace</li> <li>it automatically tracks any validation error counts &amp; fields that occur when using <code>instructor</code></li> <li>it provides a UI to label JSON responses by filling out a form instead of editing JSON objects</li> </ul> Configure Parea <p>Before starting this tutorial, make sure that you've registered for a Parea account. You'll also need to create an API key.</p>"},{"location":"blog/2024/07/17/parea-for-observing-testing--fine-tuning-of-instructor/#example-writing-emails-with-urls-from-instructor-docs","title":"Example: Writing Emails with URLs from Instructor Docs","text":"<p>We will demonstrate Parea by using <code>instructor</code> to write emails which only contain URLs from the <code>instructor</code> docs. We'll need to install our dependencies before proceeding so simply run the command below. </p> <pre><code>pip install -U parea-ai instructor\n</code></pre> <p>Parea is dead simple to integrate - all it takes is 2 lines of code, and we have it setup.</p> <pre><code>import os\n\nimport instructor\nimport requests\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\nfrom pydantic import BaseModel, field_validator, Field\nimport re\nfrom parea import Parea #(1)!\n\nload_dotenv()\n\nclient = OpenAI()\n\np = Parea(api_key=os.getenv(\"PAREA_API_KEY\")) #(2)!\np.wrap_openai_client(client, \"instructor\")\n\nclient = instructor.from_openai(client)\n</code></pre> <ol> <li>Import <code>Parea</code> from the <code>parea</code> module</li> <li>Setup tracing using their native integration with <code>instructor</code></li> </ol> <p>In this example, we'll be looking at writing emails which only contain links to the instructor docs. To do so, we can define a simple Pydantic model as seen below.</p> <pre><code>class Email(BaseModel):\n    subject: str\n    body: str = Field(\n        ...,\n        description=\"Email body, Should contain links to instructor documentation. \",\n    )\n\n    @field_validator(\"body\")\n    def check_urls(cls, v):\n        urls = re.findall(r\"https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\", v)\n        errors = []\n        for url in urls:\n            if not url.startswith(\"https://python.useinstructor.com\"):\n                errors.append(\n                    f\"URL {url} is not from useinstructor.com, Only include URLs that include use instructor.com. \"\n                )\n            response = requests.get(url)\n            if response.status_code != 200:\n                errors.append(\n                    f\"URL {url} returned status code {response.status_code}. Only include valid URLs that exist.\"\n                )\n            elif \"404\" in response.text:\n                errors.append(\n                    f\"URL {url} contained '404' in the body. Only include valid URLs that exist.\"\n                )\n        if errors:\n            raise ValueError(\"\\n\".join(errors))\n        return\n</code></pre> <p>Now we can proceed to create an email using above Pydantic model.</p> <pre><code>email = client.messages.create(\n    model=\"gpt-3.5-turbo\",\n    max_tokens=1024,\n    max_retries=3,\n    messages=[ #(1)!\n        {\n            \"role\": \"user\",\n            \"content\": \"I'm responding to a student's question. Here is the link to the documentation: {{doc_link1}} and {{doc_link2}}\",\n        }\n    ],\n    template_inputs={\n        \"doc_link1\": \"https://python.useinstructor.com/docs/tutorial/tutorial-1\",\n        \"doc_link2\": \"https://jxnl.github.io/docs/tutorial/tutorial-2\",\n    },\n    response_model=Email,\n)\nprint(email)\n</code></pre> <ol> <li>Parea supports templated prompts via <code>{{...}}</code> syntax in the <code>messages</code> parameter. We can pass the template inputs as a dictionary to the <code>template_inputs</code> parameter.</li> </ol> <p>If you follow what we've done, Parea has wrapped the client, and we wrote an email with links from the instructor docs.</p>"},{"location":"blog/2024/07/17/parea-for-observing-testing--fine-tuning-of-instructor/#validation-error-tracking","title":"Validation Error Tracking","text":"<p>To take a look at trace of this execution checkout the screenshot below. Noticeable:</p> <ul> <li>left sidebar: all related LLM calls are grouped under a trace called <code>instructor</code></li> <li>middle section: the root trace visualizes the <code>templated_inputs</code> as inputs and the created <code>Email</code> object as output</li> <li>bottom of right sidebar: any validation errors are captured and tracked as score for the trace which enables visualizing them in dashboards and filtering by them on tables</li> </ul> <p></p> <p>Above we can see that while the email was successfully created, there was a validation error which meant that additional cost &amp; latency were introduced because of the initially failed validation. Below we can see a visualization of the average validation error count for our instructor usage over time.</p> <p></p>"},{"location":"blog/2024/07/17/parea-for-observing-testing--fine-tuning-of-instructor/#label-responses-for-fine-tuning","title":"Label Responses for Fine-Tuning","text":"<p>Sometimes you may want to let subject-matter experts (SMEs) label responses to use them for fine-tuning. Parea provides a way to do this via an annotation queue. Editing raw JSON objects to correct tool use &amp; function calling responses can be error-prone, esp. for non-devs. For that purpose, Parea has a so-called Form Mode which allows the user to safely fill-out a form instead of editing the JSON object. The labeled data can then be exported and used for fine-tuning.</p> <p></p> Export Labeled Data &amp; Fine-Tune <p>After labeling the data, you can export them as JSONL file:</p> <pre><code>from parea import Parea\n\np = Parea(api_key=os.getenv(\"PAREA_API_KEY\"))\n\ndataset = p.get_collection(DATASET_ID)  #(1)!\ndataset.write_to_finetune_jsonl(\"finetune.jsonl\")  #(2)!\n</code></pre> <ol> <li>Replace <code>DATASET_ID</code> with the actual dataset ID</li> <li>Writes the dataset to a JSONL file</li> </ol> <p>Now we can use <code>instructor</code> to fine-tune the model:</p> <pre><code>instructor jobs create-from-file finetune.jsonl\n</code></pre>"},{"location":"blog/2024/09/07/pydantic-is-still-all-you-need/","title":"Pydantic is Still All You Need: Reflections on a Year of Structured Outputs","text":"<p>A year ago, I gave a talk titled \"Pydantic: All You Need\" that kickstarted my Twitter career. Today, I'm back to reaffirm that message and share what I've learned in the past year about using structured outputs with language models.</p> <p>Watch the youtube video</p>"},{"location":"blog/2024/09/07/pydantic-is-still-all-you-need/#the-problem-with-unstructured-outputs","title":"The Problem with Unstructured Outputs","text":"<p>Imagine hiring an intern to write an API that returns a string you have to JSON load into a dictionary and pray the data is still there. You'd probably fire them and replace them with GPT. Yet, many of us are content using LLMs in the same haphazard way.</p> <p>By not using schemas and structured responses, we lose compatibility, composability, and reliability when building tools that interact with external systems. But there's a better way.</p>"},{"location":"blog/2024/09/07/pydantic-is-still-all-you-need/#the-power-of-pydantic","title":"The Power of Pydantic","text":"<p>Pydantic, combined with function calling, offers a superior alternative for structured outputs. It allows for:</p> <ul> <li>Nested objects and models for modular structures</li> <li>Validators to improve system reliability</li> <li>Cleaner, more maintainable code</li> </ul> <p>For more details on how Pydantic enhances data validation, check out our Data Validation with Pydantic guide.</p> <p>And here's the kicker: nothing's really changed in the past year. The core API is still just:</p> <pre><code>from instructor import from_openai\nclient = from_openai(OpenAI())\n\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=User,\n    messages=[...]\n)\n</code></pre>"},{"location":"blog/2024/09/07/pydantic-is-still-all-you-need/#whats-new-in-pydantic","title":"What's New in Pydantic?","text":"<p>Since last year:</p> <ul> <li>We've released version 1.0</li> <li>Launched in 5 languages (Python, TypeScript, Ruby, Go, Elixir)</li> <li>Built a version in Rust</li> <li>Seen 40% month-over-month growth in the Python library</li> </ul> <p>We now support Ollama, llama-cpp-python, Anthropic, Cohere, Google, Vertex AI, and more. As long as language models support function calling capabilities, this API will remain standard.</p>"},{"location":"blog/2024/09/07/pydantic-is-still-all-you-need/#key-features","title":"Key Features","text":"<ol> <li> <p>Streaming with Structure: Get objects as they return, improving latency while maintaining structured output. Learn more about this in our Streaming Support guide.</p> </li> <li> <p>Partials: Validate entire objects, enabling real-time rendering for generative UI without complex JSON parsing. See our Partial documentation for implementation details.</p> </li> <li> <p>Validators: Add custom logic to ensure correct outputs, with the ability to retry on errors. Dive deeper into this topic in our Reasking and Validation guide.</p> </li> </ol>"},{"location":"blog/2024/09/07/pydantic-is-still-all-you-need/#real-world-applications","title":"Real-World Applications","text":""},{"location":"blog/2024/09/07/pydantic-is-still-all-you-need/#generation-and-extraction","title":"Generation and Extraction","text":"<p>Structured outputs shine in tasks like:</p> <ul> <li>Generating follow-up questions in RAG applications</li> <li>Validating URLs in generated content</li> <li>Extracting structured data from transcripts or images</li> </ul> <p>For a practical example, see our Structured Data Extraction from Images case study.</p>"},{"location":"blog/2024/09/07/pydantic-is-still-all-you-need/#search-queries","title":"Search Queries","text":"<p>For complex search scenarios:</p> <pre><code>class Search(BaseModel):\n    query: str\n    start_date: Optional[datetime]\n    end_date: Optional[datetime]\n    limit: Optional[int]\n    source: Literal[\"news\", \"social\", \"blog\"]\n</code></pre> <p>This structure allows for more sophisticated search capabilities, handling queries like \"What is the latest news from X?\" that embeddings alone can't handle.</p>"},{"location":"blog/2024/09/07/pydantic-is-still-all-you-need/#lessons-learned","title":"Lessons Learned","text":"<ol> <li>Validation errors are crucial for improving system performance.</li> <li>Not all language models support retry logic effectively yet.</li> <li>Structured outputs benefit vision, text, RAG, and agent applications alike.</li> </ol>"},{"location":"blog/2024/09/07/pydantic-is-still-all-you-need/#the-future-of-programming-with-llms","title":"The Future of Programming with LLMs","text":"<p>We're not changing the language of programming; we're relearning how to program with data structures. Structured outputs allow us to:</p> <ul> <li>Own the objects we define</li> <li>Control the functions we implement</li> <li>Manage the control flow</li> <li>Own the prompts</li> </ul> <p>This approach makes Software 3.0 backwards compatible with existing software, demystifying language models and returning us to a more classical programming structure.</p>"},{"location":"blog/2024/09/07/pydantic-is-still-all-you-need/#wrapping-up","title":"Wrapping Up","text":"<p>Pydantic is still all you need for effective structured outputs with LLMs. It's not just about generating accurate responses; it's about doing so in a way that's compatible with our existing programming paradigms and tools.</p> <p>As we continue to refine AI language models, keeping these principles in mind will lead to more robust, maintainable, and powerful applications. The future of AI isn't just about what the models can do, but how seamlessly we can integrate them into our existing software ecosystems.</p> <p>For more advanced use cases and integrations, check out our examples section, which covers various LLM providers and specialized implementations.</p>"},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/","title":"RAG is more than just embedding search","text":"<p>With the advent of large language models (LLM), retrieval augmented generation (RAG) has become a hot topic. However throughout the past year of helping startups integrate LLMs into their stack I've noticed that the pattern of taking user queries, embedding them, and directly searching a vector store is effectively demoware.</p> <p>What is RAG?</p> <p>Retrieval augmented generation (RAG) is a technique that uses an LLM to generate responses, but uses a search backend to augment the generation. In the past year using text embeddings with a vector databases has been the most popular approach I've seen being socialized.</p> <p> </p> Simple RAG that embedded the user query and makes a search. <p>So let's kick things off by examining what I like to call the 'Dumb' RAG Model\u2014a basic setup that's more common than you'd think.</p>"},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#the-dumb-rag-model","title":"The 'Dumb' RAG Model","text":"<p>When you ask a question like, \"what is the capital of France?\" The RAG 'dumb' model embeds the query and searches in some unopinionated search endpoint. Limited to a single method API like <code>search(query: str) -&gt; List[str]</code>. This is fine for simple queries, since you'd expect words like 'paris is the capital of france' to be in the top results of say, your wikipedia embeddings.</p>"},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#why-is-this-a-problem","title":"Why is this a problem?","text":"<ul> <li> <p>Query-Document Mismatch: This model assumes that query embedding and the content embedding are similar in the embedding space, which is not always true based on the text you're trying to search over. Only using queries that are semantically similar to the content is a huge limitation!</p> </li> <li> <p>Monolithic Search Backend: Assumes a single search backend, which is not always the case. You may have multiple search backends, each with their own API, and you want to route the query to vector stores, search clients, sql databases, and more.</p> </li> <li> <p>Limitation of text search: Restricts complex queries to a single string (<code>{query: str}</code>), sacrificing expressiveness, in using keywords, filters, and other advanced features. For example, asking <code>what problems did we fix last week</code> cannot be answered by a simple text search since documents that contain <code>problem, last week</code> are going to be present at every week.</p> </li> <li> <p>Limited ability to plan: Assumes that the query is the only input to the search backend, but you may want to use other information to improve the search, like the user's location, or the time of day using the context to rewrite the query. For example, if you present the language model of more context it is able to plan a suite of queries to execute to return the best results.</p> </li> </ul> <p>Now let's dive into how we can make it smarter with query understanding. This is where things get interesting.</p>"},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#improving-the-rag-model-with-query-understanding","title":"Improving the RAG Model with Query Understanding","text":"<p>Shoutouts</p> <p>Much of this work has been inspired by / done in collab with a few of my clients at new.computer, Metaphor Systems, and Naro, go check them out!</p> <p>Ultimately what you want to deploy is a system that understands how to take the query and rewrite it to improve precision and recall.</p> <p> </p> Query Understanding system routes to multiple search backends. <p>Not convinced? Let's move from theory to practice with a real-world example. First up, Metaphor Systems.</p>"},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#whats-instructor","title":"Whats instructor?","text":"<p>Instructor uses Pydantic to simplify the interaction between the programmer and language models via the function calling API.</p> <ul> <li>Widespread Adoption: Pydantic is a popular tool among Python developers.</li> <li>Simplicity: Pydantic allows model definition in Python.</li> <li>Framework Compatibility: Many Python frameworks already use Pydantic.</li> </ul>"},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#case-study-1-metaphor-systems","title":"Case Study 1: Metaphor Systems","text":"<p>Take Metaphor Systems, which turns natural language queries into their custom search-optimized query. If you take a look web UI you'll notice that they have an auto-prompt option, which uses function calls to furthur optimize your query using a language model, and turns it into a fully specified metaphor systems query.</p> <p></p> Metaphor Systems UI <p>If we peek under the hood, we can see that the query is actually a complex object, with a date range, and a list of domains to search in. It's actually more complex than this but this is a good start. We can model this structured output in Pydantic using the instructor library</p> <pre><code>class DateRange(BaseModel):\n    start: datetime.date\n    end: datetime.date\n\n\nclass MetaphorQuery(BaseModel):\n    rewritten_query: str\n    published_daterange: DateRange\n    domains_allow_list: List[str]\n\n    async def execute():\n        return await metaphor.search(...)\n</code></pre> <p>Note how we model a rewritten query, range of published dates, and a list of domains to search in. This powerful pattern allows the user query to be restructured for better performance without the user having to know the details of how the search backend works.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Enables response_model in the openai client\nclient = instructor.from_openai(OpenAI())\n\nquery = client.chat.completions.create(\n    model=\"gpt-4\",\n    response_model=MetaphorQuery,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You're a query understanding system for the Metafor Systems search engine. Here are some tips: ...\",\n        },\n        {\"role\": \"user\", \"content\": \"What are some recent developments in AI?\"},\n    ],\n)\n</code></pre> <p>Example Output</p> <pre><code>{\n  \"rewritten_query\": \"novel developments advancements ai artificial intelligence machine learning\",\n  \"published_daterange\": {\n    \"start\": \"2023-09-17\",\n    \"end\": \"2021-06-17\"\n  },\n  \"domains_allow_list\": [\"arxiv.org\"]\n}\n</code></pre> <p>This isn't just about adding some date ranges. It's about nuanced, tailored searches, that are deeply integrated with the backend. Metaphor Systems has a whole suite of other filters and options that you can use to build a powerful search query. They can even use some chain of thought prompting to improve how they use some of these advanced features.</p> <pre><code>class DateRange(BaseModel):\n    start: datetime.date\n    end: datetime.date\n    chain_of_thought: str = Field(\n        None,\n        description=\"Think step by step to plan what is the best time range to search in\",\n    )\n</code></pre> <p>Now, let's see how this approach can help model an agent like personal assistant.</p>"},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#case-study-2-personal-assistant","title":"Case Study 2: Personal Assistant","text":"<p>Another great example of this multiple dispatch pattern is a personal assistant. You might ask, \"What do I have today?\", from a vague query you might want events, emails, reminders etc. That data will likely exist in multiple backends, but what you want is one unified summary of results. Here you can't assume that text of those documents are all embedded in a search backend. There might be a calendar client, email client, across personal and profession accounts.</p> <pre><code>class ClientSource(enum.Enum):\n    GMAIL = \"gmail\"\n    CALENDAR = \"calendar\"\n\n\nclass SearchClient(BaseModel):\n    query: str\n    keywords: List[str]\n    email: str\n    source: ClientSource\n    start_date: datetime.date\n    end_date: datetime.date\n\n    async def execute(self) -&gt; str:\n        if self.source == ClientSource.GMAIL:\n            ...\n        elif self.source == ClientSource.CALENDAR:\n            ...\n\n\nclass Retrieval(BaseModel):\n    queries: List[SearchClient]\n\n    async def execute(self) -&gt; str:\n        return await asyncio.gather(*[query.execute() for query in self.queries])\n</code></pre> <p>Now we can call this with a simple query like \"What do I have today?\" and it will try to async dispatch to the correct backend. It's still important to prompt the language model well, but we'll leave that for another day.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Enables response_model in the openai client\nclient = instructor.from_openai(OpenAI())\n\nretrieval = client.chat.completions.create(\n    model=\"gpt-4\",\n    response_model=Retrieval,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are Jason's personal assistant.\"},\n        {\"role\": \"user\", \"content\": \"What do I have today?\"},\n    ],\n)\n</code></pre> <p>Example Output</p> <pre><code>{\n    \"queries\": [\n        {\n            \"query\": None,\n            \"keywords\": None,\n            \"email\": \"jason@example.com\",\n            \"source\": \"gmail\",\n            \"start_date\": \"2023-09-17\",\n            \"end_date\": None\n        },\n        {\n            \"query\": None,\n            \"keywords\": [\"meeting\", \"call\", \"zoom\"]]],\n            \"email\": \"jason@example.com\",\n            \"source\": \"calendar\",\n            \"start_date\": \"2023-09-17\",\n            \"end_date\": None\n\n        }\n    ]\n}\n</code></pre> <p>Notice that we have a list of queries that route to different search backends (email and calendar). We can even dispatch them async to be as performance as possible. Not only do we dispatch to different backends (that we have no control over), but you are likely going to render them to the user differently as well. Perhaps you want to summarize the emails in text, but you want to render the calendar events as a list that they can scroll across on a mobile app.</p> <p>Can I used framework X?</p> <p>I get this question a lot, but it's just code. Within these dispatches you can do whatever you want. You can use <code>input()</code> to ask the user for more information, make a post request, call a Langchain agent or LLamaindex query engine to get more information. The sky is the limit.</p> <p>Both of these examples showcase how both search providers and consumers can use <code>instructor</code> to model their systems. This is a powerful pattern that allows you to build a system that can be used by anyone, and can be used to build an LLM layer, from scratch, in front of any arbitrary backend.</p>"},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#conclusion","title":"Conclusion","text":"<p>This is not about fancy embedding tricks, it's just plain old information retrieval and query understanding. The beauty of instructor is that it simplifies modeling the complex and lets you define the output of the language model, the prompts, and the payload we send to the backend in a single place.</p>"},{"location":"blog/2023/09/17/rag-is-more-than-just-embedding-search/#whats-next","title":"What's Next?","text":"<p>Here I want to show that `instructor`` isn\u2019t just about data extraction. It\u2019s a powerful framework for building a data model and integrating it with your LLM. Structured output is just the beginning \u2014 the untapped goldmine is skilled use of tools and APIs.</p> <p>If you enjoy the content or want to try out <code>instructor</code> please check out the github and give us a star!</p>"},{"location":"blog/2024/06/06/enhancing-rag-with-time-filters-using-instructor/","title":"Enhancing RAG with Time Filters Using Instructor","text":"<p>Retrieval-augmented generation (RAG) systems often need to handle queries with time-based constraints, like \"What new features were released last quarter?\" or \"Show me support tickets from the past week.\" Effective time filtering is crucial for providing accurate, relevant responses.</p> <p>Instructor is a Python library that simplifies integrating large language models (LLMs) with data sources and APIs. It allows defining structured output models using Pydantic, which can be used as prompts or to parse LLM outputs.</p>"},{"location":"blog/2024/06/06/enhancing-rag-with-time-filters-using-instructor/#modeling-time-filters","title":"Modeling Time Filters","text":"<p>To handle time filters, we can define a Pydantic model representing a time range:</p> <pre><code>from datetime import datetime\nfrom typing import Optional\nfrom pydantic import BaseModel\n\nclass TimeFilter(BaseModel):\n    start_date: Optional[datetime] = None\n    end_date: Optional[datetime] = None\n</code></pre> <p>The <code>TimeFilter</code> model can represent an absolute date range or a relative time range like \"last week\" or \"previous month.\"</p> <p>We can then combine this with a search query string:</p> <pre><code>class SearchQuery(BaseModel):\n    query: str\n    time_filter: TimeFilter\n</code></pre>"},{"location":"blog/2024/06/06/enhancing-rag-with-time-filters-using-instructor/#prompting-the-llm","title":"Prompting the LLM","text":"<p>Using Instructor, we can prompt the LLM to generate a <code>SearchQuery</code> object based on the user's query:</p> <pre><code>import instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    response_model=SearchQuery,\n    messages=[\n        {\n            \"role\": \"system\", \n            \"content\": \"You are a query generator for customer support tickets. The current date is 2024-02-17\"},\n        {\n            \"role\": \"user\", \n            \"content\": \"Show me customer support tickets opened in the past week.\"\n        },\n    ],\n)\n\n{\n    \"query\": \"Show me customer support tickets opened in the past week.\",\n    \"time_filter\": {\n        \"start_date\": \"2024-02-10T00:00:00\",\n        \"end_date\": \"2024-02-17T00:00:00\"\n    }\n}\n</code></pre>"},{"location":"blog/2024/06/06/enhancing-rag-with-time-filters-using-instructor/#nuances-in-dates-and-timezones","title":"Nuances in dates and timezones","text":"<p>When working with time-based queries, it's important to consider the nuances of dates, timezones, and publication times. Depending on the data source, the user's location, and when the content was originally published, the definition of \"past week\" or \"last month\" may vary.</p> <p>To handle this, you'll want to design your <code>TimeFilter</code> model to intelligently reason about these relative time periods. This could involve:</p> <ul> <li>Defaulting to the user's local timezone if available, or using a consistent default like UTC  </li> <li>Defining clear rules for how to calculate the start and end of relative periods like \"week\" or \"month\"</li> <li>e.g. does \"past week\" mean the last 7 days or the previous Sunday-Saturday range?</li> <li>Allowing for flexibility in how users specify dates (exact datetimes, just dates, natural language phrases)</li> <li>Validating and normalizing user input to fit the expected <code>TimeFilter</code> format</li> <li>Considering the original publication timestamp of the content, not just the current date</li> <li>e.g. \"articles published in the last month\" should look at the publish date, not the query date</li> </ul> <p>By building this logic into the <code>TimeFilter</code> model, you can abstract away the complexity and provide a consistent interface for the rest of your RAG system to work with standardized absolute datetime ranges</p> <p>Of course, there may be edge cases or ambiguities that are hard to resolve programmatically. In these situations, you may need to prompt the user for clarification or make a best guess based on the available information. The key is to strive for a balance of flexibility and consistency in how you handle time-based queries, factoring in publication dates when relevant.</p> <p>By modeling time filters with Pydantic and leveraging Instructor, RAG systems can effectively handle time-based queries. Clear prompts, careful model design, and appropriate parsing strategies enable accurate retrieval of information within specific time frames, enhancing the system's overall relevance and accuracy.</p>"},{"location":"blog/2024/06/15/zero-cost-abstractions/","title":"Why Instructor is the best way to get JSON from LLMs","text":"<p>Large Language Models (LLMs) like GPT are incredibly powerful, but getting them to return well-formatted JSON can be challenging. This is where the Instructor library shines. Instructor allows you to easily map LLM outputs to JSON data using Python type annotations and Pydantic models.</p> <p>Instructor makes it easy to get structured data like JSON from LLMs like GPT-3.5, GPT-4, GPT-4-Vision, and open-source models including Mistral/Mixtral, Anyscale, Ollama, and llama-cpp-python.</p> <p>It stands out for its simplicity, transparency, and user-centric design, built on top of Pydantic. Instructor helps you manage validation context, retries with Tenacity, and streaming Lists and Partial responses.</p> <ul> <li>Instructor provides support for a wide range of programming languages, including:</li> <li>Python</li> <li>TypeScript</li> <li>Ruby</li> <li>Go</li> <li>Elixir</li> </ul>","tags":["python","llms"]},{"location":"blog/2024/06/15/zero-cost-abstractions/#the-simple-patch-for-json-llm-outputs","title":"The Simple Patch for JSON LLM Outputs","text":"<p>Instructor works as a lightweight patch over the OpenAI Python SDK. To use it, you simply apply the patch to your OpenAI client:</p> <pre><code>import instructor\nimport openai\n\nclient = instructor.from_openai(openai.OpenAI())\n</code></pre> <p>Then, you can pass a <code>response_model</code> parameter to the <code>completions.create</code> or <code>chat.completions.create</code> methods. This parameter takes in a Pydantic model class that defines the JSON structure you want the LLM output mapped to. Just like <code>response_model</code> when using FastAPI.</p> <p>Here's an example of a <code>response_model</code> for a simple user profile:</p> <pre><code>from pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n    email: str\n\nclient = instructor.from_openai(openai.OpenAI())\n\nuser = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=User,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract the user's name, age, and email from this: John Doe is 25 years old. His email is john@example.com\"\n        }\n    ]\n)\n\nprint(user.model_dump())\n# &gt; { \n#     \"name\": \"John Doe\",\n#     \"age\": 25,\n#     \"email\": \"john@example.com\"\n#   }\n</code></pre> <p>Instructor extracts the JSON data from the LLM output and returns an instance of your specified Pydantic model. You can then use the <code>model_dump()</code> method to serialize the model instance to a JSON string.</p> <p>Some key benefits of Instructor:</p> <ul> <li>Zero new syntax to learn - it builds on standard Python type hints</li> <li>Seamless integration with existing OpenAI SDK code</li> <li>Incremental, zero-overhead adoption path</li> <li>Direct access to the <code>messages</code> parameter for flexible prompt engineering</li> <li>Broad compatibility with any OpenAI SDK-compatible platform or provider</li> </ul>","tags":["python","llms"]},{"location":"blog/2024/06/15/zero-cost-abstractions/#pydantic-more-powerful-than-plain-dictionaries","title":"Pydantic: More Powerful than Plain Dictionaries","text":"<p>You might be wondering, why use Pydantic models instead of just returning a dictionary of key-value pairs? While a dictionary could hold JSON data, Pydantic models provide several powerful advantages:</p> <ol> <li> <p>Type validation: Pydantic models enforce the types of the fields. If the LLM returns an incorrect type (e.g. a string for an int field), it will raise a validation error.</p> </li> <li> <p>Field requirements: You can mark fields as required or optional. Pydantic will raise an error if a required field is missing.</p> </li> <li> <p>Default values: You can specify default values for fields that aren't always present.</p> </li> <li> <p>Advanced types: Pydantic supports more advanced field types like dates, UUIDs, URLs, lists, nested models, and more.</p> </li> <li> <p>Serialization: Pydantic models can be easily serialized to JSON, which is helpful for saving results or passing them to other systems.</p> </li> <li> <p>IDE support: Because Pydantic models are defined as classes, IDEs can provide autocompletion, type checking, and other helpful features when working with the JSON data.</p> </li> </ol> <p>So while dictionaries can work for very simple JSON structures, Pydantic models are far more powerful for working with complex, validated JSON in a maintainable way.</p>","tags":["python","llms"]},{"location":"blog/2024/06/15/zero-cost-abstractions/#json-from-llms-made-easy","title":"JSON from LLMs Made Easy","text":"<p>Instructor and Pydantic together provide a fantastic way to extract and work with JSON data from LLMs. The lightweight patching of Instructor combined with the powerful validation and typing of Pydantic models makes it easy to integrate JSON outputs into your LLM-powered applications. Give Instructor a try and see how much easier it makes getting JSON from LLMs!</p>","tags":["python","llms"]},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/","title":"Good LLM Validation is Just Good Validation","text":"<p>What if your validation logic could learn and adapt like a human, but operate at the speed of software? This is the future of validation and it's already here.</p> <p>Validation is the backbone of reliable software. But traditional methods are static, rule-based, and can't adapt to new challenges. This post looks at how to bring dynamic, machine learning-driven validation into your software stack using Python libraries like <code>Pydantic</code> and <code>Instructor</code>. We validate these outputs using a validation function which conforms to the structure seen below.</p> <pre><code>def validation_function(value):\n    if condition(value):\n        raise ValueError(\"Value is not valid\")\n    return mutation(value)\n</code></pre>"},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#what-is-instructor","title":"What is Instructor?","text":"<p><code>Instructor</code> helps to ensure you get the exact response type you're looking for when using openai's function call api. Once you've defined the <code>Pydantic</code> model for your desired response, <code>Instructor</code> handles all the complicated logic in-between - from the parsing/validation of the response to the automatic retries for invalid responses. This means that we can build in validators 'for free' and have a clear separation of concerns between the prompt and the code that calls openai.</p> <pre><code>from openai import OpenAI\nimport instructor  # pip install instructor\nfrom pydantic import BaseModel\n\n# This enables response_model keyword\n# from client.chat.completions.create\nclient = instructor.from_openai(OpenAI())  # (1)!\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\nuser: UserDetail = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n    ],\n    max_retries=3,  # (2)!\n)\n\nassert user.name == \"Jason\"  # (3)!\nassert user.age == 25\n</code></pre> <ol> <li> <p>To simplify your work with OpenAI models and streamline the extraction of Pydantic objects from prompts, we     offer a patching mechanism for the <code>ChatCompletion</code> class.</p> </li> <li> <p>Invalid responses that fail to be validated successfully will trigger up to as many reattempts as you define.</p> </li> <li> <p>As long as you pass in a <code>response_model</code> parameter to the <code>ChatCompletion</code> api call, the returned object will always     be a validated <code>Pydantic</code> object.</p> </li> </ol> <p>In this post, we'll explore how to evolve from static, rule-based validation methods to dynamic, machine learning-driven ones. You'll learn to use <code>Pydantic</code> and <code>Instructor</code> to leverage language models and dive into advanced topics like content moderation, validating chain of thought reasoning, and contextual validation.</p> <p>Let's examine how these approaches with an example. Imagine that you run a software company that wants to ensure you never serve hateful and racist content. This isn't an easy job since the language around these topics change very quickly and frequently.</p>"},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#software-10-introduction-to-validations-in-pydantic","title":"Software 1.0: Introduction to Validations in Pydantic","text":"<p>A simple method could be to compile a list of different words that are often associated with hate speech. For simplicity, let's assume that we've found that the words <code>Steal</code> and <code>Rob</code> are good predictors of hateful speech from our database. We can modify our validation structure above to accomodate this.</p> <p>This will throw an error if we pass in a string like <code>Let's rob the bank!</code> or <code>We should steal from the supermarkets</code>.</p> <p>Pydantic offers two approaches for this validation: using the <code>field_validator</code> decorator or the <code>Annotated</code> hints.</p>"},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#using-field_validator-decorator","title":"Using <code>field_validator</code> decorator","text":"<p>We can use the <code>field_validator</code> decorator to define a validator for a field in Pydantic. Here's a quick example of how we might be able to do so.</p> <pre><code>from pydantic import BaseModel, ValidationError, field_validator\n\n\nclass UserMessage(BaseModel):\n    message: str\n\n    @field_validator('message')\n    def message_cannot_have_blacklisted_words(cls, v: str) -&gt; str:\n        for word in v.split():  # (1)!\n            if word.lower() in {'rob', 'steal'}:\n                raise ValueError(f\"`{word}` was found in the message `{v}`\")\n        return v\n\n\ntry:\n    UserMessage(message=\"This is a lovely day\")\n    UserMessage(message=\"We should go and rob a bank\")\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserMessage\n    message\n      Value error, `rob` was found in the message `We should go and rob a bank` [type=value_error, input_value='We should go and rob a bank', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.6/v/value_error\n    \"\"\"\n</code></pre> <ol> <li>We split the sentence into its individual words and iterate through each of the words. We then try to see if any of these     words are in our blacklist which in this case is just <code>rob</code> and <code>steal</code></li> </ol> <p>Since the message <code>This is a lovely day</code> does not have any blacklisted words, no errors are thrown. However, in the given example above, the validation fails for the message <code>We should go and rob a bank</code> due to the presence of the word <code>rob</code> and the corresponding error message is displayed.</p> <pre><code>1 validation error for UserMessage\nmessage\n  Value error, `rob` was found in the message `We should go and rob a bank` [type=value_error, input_value='We should go and rob a bank', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error\n</code></pre>"},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#using-annotated","title":"Using <code>Annotated</code>","text":"<p>Alternatively, you can use the <code>Annotated</code> function to perform the same validation. Here's an example where we utilise the same function we started with.</p> <pre><code>from pydantic import BaseModel, ValidationError\nfrom typing import Annotated\nfrom pydantic.functional_validators import AfterValidator\n\n\ndef message_cannot_have_blacklisted_words(value: str):\n    for word in value.split():\n        if word.lower() in {'rob', 'steal'}:\n            raise ValueError(f\"`{word}` was found in the message `{value}`\")\n    return value\n\n\nclass UserMessage(BaseModel):\n    message: Annotated[str, AfterValidator(message_cannot_have_blacklisted_words)]\n\n\ntry:\n    UserMessage(message=\"This is a lovely day\")\n    UserMessage(message=\"We should go and rob a bank\")\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserMessage\n    message\n      Value error, `rob` was found in the message `We should go and rob a bank` [type=value_error, input_value='We should go and rob a bank', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.6/v/value_error\n    \"\"\"\n</code></pre> <p>This code snippet achieves the same validation result. If the user message contains any of the words in the blacklist, a <code>ValueError</code> is raised and the corresponding error message is displayed.</p> <pre><code>1 validation error for UserMessage\nmessage\n  Value error, `rob` was found in the message `We should go and rob a bank` [type=value_error, input_value='We should go and rob a bank', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error\n</code></pre> <p>Validation is a fundamental concept in software development and remains the same when applied to AI systems. Existing programming concepts should be leveraged when possible instead of introducing new terms and standards. The underlying principles of validation remain unchanged.</p> <p>Suppose now that we've gotten a new message - <code>Violence is always acceptable, as long as we silence the witness</code>. Our original validator wouldn't throw any errors when passed this new message since it uses neither the words <code>rob</code> or <code>steal</code>. However, it's clear that it is not a message which should be published. How can we ensure that our validation logic can adapt to new challenges?</p>"},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#software-30-validation-for-llms-or-powered-by-llms","title":"Software 3.0: Validation for LLMs or powered by LLMs","text":"<p>Building upon the understanding of simple field validators, let's delve into probabilistic validation in software 3.0, (prompt engineering). We'll introduce an LLM-powered validator called <code>llm_validator</code> that uses a statement to verify the value.</p> <p>We can get around this by using the inbuilt <code>llm_validator</code> class from <code>Instructor</code>.</p> <pre><code>from instructor import llm_validator\nfrom pydantic import BaseModel, ValidationError\nfrom typing import Annotated\nfrom pydantic.functional_validators import AfterValidator\n\n\nclass UserMessage(BaseModel):\n    message: Annotated[\n        str, AfterValidator(llm_validator(\"don't say objectionable things\"))\n    ]\n\n\ntry:\n    UserMessage(\n        message=\"Violence is always acceptable, as long as we silence the witness\"\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserMessage\n    message\n      Assertion failed, The statement promotes violence, which is objectionable. [type=assertion_error, input_value='Violence is always accep... we silence the witness', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.6/v/assertion_error\n    \"\"\"\n</code></pre> <p>This produces the following error message as seen below</p> <pre><code>1 validation error for UserMessage\nmessage\n  Assertion failed, The statement promotes violence, which is objectionable. [type=assertion_error, input_value='Violence is always accep... we silence the witness', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/assertion_error\n</code></pre> <p>The error message is generated by the language model (LLM) rather than the code itself, making it helpful for re-asking the model in a later section. To better understand this approach, let's see how to build an <code>llm_validator</code> from scratch.</p>"},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#creating-your-own-field-level-llm_validator","title":"Creating Your Own Field Level <code>llm_validator</code>","text":"<p>Building your own <code>llm_validator</code> can be a valuable exercise to get started with <code>Instructor</code> and create custom validators.</p> <p>Before we continue, let's review the anatomy of a validator:</p> <pre><code>def validation_function(value):\n    if condition(value):\n        raise ValueError(\"Value is not valid\")\n    return value\n</code></pre> <p>As we can see, a validator is simply a function that takes in a value and returns a value. If the value is not valid, it raises a <code>ValueError</code>. We can represent this using the following structure:</p> <pre><code>class Validation(BaseModel):\n    is_valid: bool = Field(\n        ..., description=\"Whether the value is valid based on the rules\"\n    )\n    error_message: Optional[str] = Field(\n        ...,\n        description=\"The error message if the value is not valid, to be used for re-asking the model\",\n    )\n</code></pre> <p>Using this structure, we can implement the same logic as before and utilize <code>Instructor</code> to generate the validation.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Enables `response_model` and `max_retries` parameters\nclient = instructor.from_openai(OpenAI())\n\n\ndef validator(v):\n    statement = \"don't say objectionable things\"\n    resp = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a validator. Determine if the value is valid for the statement. If it is not, explain why.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Does `{v}` follow the rules: {statement}\",\n            },\n        ],\n        # this comes from client = instructor.from_openai(OpenAI())\n        response_model=Validation,  # (1)!\n    )\n    if not resp.is_valid:\n        raise ValueError(resp.error_message)\n    return v\n</code></pre> <ol> <li>The new parameter of <code>response_model</code> comes from <code>client = instructor.from_openai(OpenAI())</code> and does not exist in the original OpenAI SDK. This    allows us to pass in the <code>Pydantic</code> model that we want as a response.</li> </ol> <p>Now we can use this validator in the same way we used the <code>llm_validator</code> from <code>Instructor</code>.</p> <pre><code>class UserMessage(BaseModel):\n    message: Annotated[str, AfterValidator(validator)]\n</code></pre>"},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#writing-more-complex-validations","title":"Writing more complex validations","text":""},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#validating-chain-of-thought","title":"Validating Chain of Thought","text":"<p>A popular way of prompting large language models nowadays is known as chain of thought. This involves getting a model to generate reasons and explanations for an answer to a prompt.</p> <p>We can utilise <code>Pydantic</code> and <code>Instructor</code> to perform a validation to check if the reasoning is reasonable, given both the answer and the chain of thought. To do this we can't build a field validator since we need to access multiple fields in the model. Instead we can use a model validator.</p> <pre><code>def validate_chain_of_thought(values):\n    chain_of_thought = values[\"chain_of_thought\"]\n    answer = values[\"answer\"]\n    resp = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a validator. Determine if the value is valid for the statement. If it is not, explain why.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Verify that `{answer}` follows the chain of thought: {chain_of_thought}\",\n            },\n        ],\n        # this comes from client = instructor.from_openai(OpenAI())\n        response_model=Validation,\n    )\n    if not resp.is_valid:\n        raise ValueError(resp.error_message)\n    return values\n</code></pre> <p>We can then take advantage of the <code>model_validator</code> decorator to perform a validation on a subset of the model's data.</p> <p>We're defining a model validator here which runs before <code>Pydantic</code> parses the input into its respective fields. That's why we have a before keyword used in the <code>model_validator</code> class.</p> <pre><code>from pydantic import BaseModel, model_validator\n\n\nclass AIResponse(BaseModel):\n    chain_of_thought: str\n    answer: str\n\n    @model_validator(mode='before')\n    @classmethod\n    def chain_of_thought_makes_sense(cls, data: Any) -&gt; Any:\n        # here we assume data is the dict representation of the model\n        # since we use 'before' mode.\n        return validate_chain_of_thought(data)\n</code></pre> <p>Now, when you create a <code>AIResponse</code> instance, the <code>chain_of_thought_makes_sense</code> validator will be invoked. Here's an example:</p> <pre><code>try:\n    resp = AIResponse(chain_of_thought=\"1 + 1 = 2\", answer=\"The meaning of life is 42\")\nexcept ValidationError as e:\n    print(e)\n</code></pre> <p>If we create a <code>AIResponse</code> instance with an answer that does not follow the chain of thought, we will get an error.</p> <pre><code>1 validation error for AIResponse\n    Value error, The statement 'The meaning of life is 42' does not follow the chain of thought: 1 + 1 = 2.\n    [type=value_error, input_value={'chain_of_thought': '1 +... meaning of life is 42'}, input_type=dict]\n</code></pre>"},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#validating-citations-from-original-text","title":"Validating Citations From Original Text","text":"<p>Let's see a more concrete example. Let's say that we've asked our model a question about some text source and we want to validate that the generated answer is supported by the source. This would allow us to minimize hallucinations and prevent statements that are not backed by the original text. While we could verify this by looking up the original source manually, a more scalable approach is to use a validator to do this automatically.</p> <p>We can pass in additional context to our validation functions using the <code>model_validate</code> function in <code>Pydantic</code> so that our models have more information to work with when performing validation. This context is a normal python dictionary and can be accessed inside the <code>info</code> argument in our validator functions.</p> <pre><code>from pydantic import ValidationInfo, BaseModel, field_validator\n\n\nclass AnswerWithCitation(BaseModel):\n    answer: str\n    citation: str\n\n    @field_validator('citation')\n    @classmethod\n    def citation_exists(cls, v: str, info: ValidationInfo):  # (1)!\n        context = info.context\n        if context:\n            context = context.get('text_chunk')\n            if v not in context:\n                raise ValueError(f\"Citation `{v}` not found in text chunks\")\n        return v\n</code></pre> <ol> <li>This <code>info</code> object corresponds to the value of <code>context</code> that we pass into the <code>model_validate</code> function as seen below.</li> </ol> <p>We can then take our original example and test it against our new model</p> <pre><code>try:\n    AnswerWithCitation.model_validate(\n        {\"answer\": \"Jason is a cool guy\", \"citation\": \"Jason is cool\"},\n        context={\"text_chunk\": \"Jason is just a guy\"},  # (1)!\n    )\nexcept ValidationError as e:\n    print(e)\n</code></pre> <ol> <li>This <code>context</code> object is just a normal python dictionary and can take in and store any arbitrary values</li> </ol> <p>This in turn generates the following error since <code>Jason is cool</code> does not exist in the text <code>Jason is just a guy</code>.</p> <pre><code>1 validation error for AnswerWithCitation\ncitation\nValue error, Citation `Jason is cool` not found in text chunks [type=value_error, input_value='Jason is cool', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error\n</code></pre>"},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#putting-it-all-together-with-client-instructorfrom_openaiopenai","title":"Putting it all together with <code>client = instructor.from_openai(OpenAI())</code>","text":"<p>To pass this context from the <code>client.chat.completions.create</code> call, <code>client = instructor.from_openai(OpenAI())</code> also passes the <code>validation_context</code>, which will be accessible from the <code>info</code> argument in the decorated validator functions.</p> <pre><code>from openai import OpenAI\nimport instructor\n\n# Enables `response_model` and `max_retries` parameters\nclient = instructor.from_openai(OpenAI())\n\n\ndef answer_question(question: str, text_chunk: str) -&gt; AnswerWithCitation:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Answer the question: {question} with the text chunk: {text_chunk}\",\n            },\n        ],\n        response_model=AnswerWithCitation,\n        validation_context={\"text_chunk\": text_chunk},\n    )\n</code></pre>"},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#error-handling-and-re-asking","title":"Error Handling and Re-Asking","text":"<p>Validators can ensure certain properties of the outputs by throwing errors, in an AI system we can use the errors and allow language model to self correct. Then by running <code>client = instructor.from_openai(OpenAI())</code> not only do we add <code>response_model</code> and <code>validation_context</code> it also allows you to use the <code>max_retries</code> parameter to specify the number of times to try and self correct.</p> <p>This approach provides a layer of defense against two types of bad outputs:</p> <ol> <li>Pydantic Validation Errors (code or LLM-based)</li> <li>JSON Decoding Errors (when the model returns an incorrect response)</li> </ol>"},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#define-the-response-model-with-validators","title":"Define the Response Model with Validators","text":"<p>To keep things simple let's assume we have a model that returns a <code>UserModel</code> object. We can define the response model using Pydantic and add a field validator to ensure that the name is in uppercase.</p> <pre><code>from pydantic import BaseModel, field_validator\n\n\nclass UserModel(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"name\")\n    @classmethod\n    def validate_name(cls, v):\n        if v.upper() != v:\n            raise ValueError(\"Name must be in uppercase.\")\n        return v\n</code></pre> <p>This is where the <code>max_retries</code> parameter comes in. It allows the model to self correct and retry the prompt using the error message rather than the prompt.</p> <pre><code>model = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n    # Powered by client = instructor.from_openai(OpenAI())\n    response_model=UserModel,\n    max_retries=2,\n)\n\nassert model.name == \"JASON\"\n</code></pre> <p>In this example, even though there is no code explicitly transforming the name to uppercase, the model is able to correct the output.</p>"},{"location":"blog/2023/10/23/good-llm-validation-is-just-good-validation/#conclusion","title":"Conclusion","text":"<p>From the simplicity of Pydantic and Instructor to the dynamic validation capabilities of LLMs, the landscape of validation is changing but without needing to introduce new concepts. It's clear that the future of validation is not just about preventing bad data but about allowing llms to understand the data and correcting it.</p> <p>If you enjoy the content or want to try out <code>Instructor</code> please check out the github and give us a star!</p>"},{"location":"blog/2024/04/01/announce-instructor-v1/","title":"Announcing instructor=1.0.0","text":"<p>Over the past 10 months, we've build up instructor with the principle of 'easy to try, and easy to delete'. We accomplished this by patching the openai client with the <code>instructor</code> package and adding new arguments like <code>response_model</code>, <code>max_retries</code>, and <code>validation_context</code>. As a result I truely believe isntructor is the best way to get structured data out of llm apis.</p> <p>But as a result, we've been a bit stuck on getting typing to work well while giving you more control at development time. I'm excited to launch version 1.0.0 which cleans up the api w.r.t. typing without compromising the ease of use.</p>","tags":["instructor"]},{"location":"blog/2024/04/01/announce-instructor-v1/#growth","title":"Growth","text":"<p>Over the past 10 months, we've enjoyed healthy growth with over 4000+ github stars and 100+ contributors, and more importantly, 120k monthly downloads, and 20k unique monthly visitors with 500k requests per month to our docs</p> <p></p>","tags":["instructor"]},{"location":"blog/2024/04/01/announce-instructor-v1/#whats-new","title":"Whats new?","text":"<p>Honestly, nothing much, the simplest change you'll need to make is to replace <code>instructor.patch</code> with <code>instructor.from_openai</code>.</p> <pre><code>import openai\nimport instructor\n\nclient = instructor.from_openai(openai.OpenAI())\n</code></pre> <p>Except now, any default arguments you want to place into the <code>create</code> call will be passed to the client. via kwargs. </p> <p>IF you know you want to pass in tempurature, seed, or model, you can do so.</p> <pre><code>import openai\nimport instructor\n\nclient = instructor.from_openai(\n    openai.OpenAI(), \n    model=\"gpt-4-turbo-preview\", \n    temperature=0.2\n)\n</code></pre> <p>Now, whenever you call <code>client.chat.completions.create</code> the <code>model</code> and <code>temperature</code> will be passed to the openai client! </p>","tags":["instructor"]},{"location":"blog/2024/04/01/announce-instructor-v1/#no-new-standards","title":"No new Standards","text":"<p>When I first started working on this project, my goal was to ensure that we weren't introducing any new standards. Instead, our focus was on maintaining compatibility with existing ones. By creating our own client, we can seamlessly proxy OpenAI's <code>chat.completions.create</code> and Anthropic's <code>messages.create</code> methods. This approach allows us to provide a smooth upgrade path for your client, enabling support for all the latest models and features as they become available. Additionally, this strategy safeguards us against potential downstream changes.</p> <pre><code>import openai\nimport anthropic\nimport litellm\nimport instructor\n\n# These are all ways to create a client\nclient = instructor.from_openai(openai.OpenAI())\nclient = instructor.from_anthropic(anthropic.Anthropic())\nclient = instructor.from_litellm(litellm.completion)\n\n# all of these will route to the same underlying create function\n# allow you to add instructor to try it out, while easily removing it \nclient.create(..., response_model=Type[T]) -&gt; T\nclient.chat.completions.create(..., response_model=Type[T]) -&gt; T\nclient.messages.create(..., response_model=Type[T]) -&gt; T\n</code></pre>","tags":["instructor"]},{"location":"blog/2024/04/01/announce-instructor-v1/#type-are-infered-correctly","title":"Type are infered correctly","text":"<p>This was the dream of instructor but due to the patching of openai, it wasnt possible for me to get typing to work well. Now, with the new client, we can get typing to work well! We've also added a few <code>create_*</code> methods to make it easier to create iterables and partials, and to access the original completion.</p>","tags":["instructor"]},{"location":"blog/2024/04/01/announce-instructor-v1/#calling-create","title":"Calling <code>create</code>","text":"<pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nclient = instructor.from_openai(openai.OpenAI())\n\nuser = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Create a user\"},\n    ],\n    response_model=User,\n)\n</code></pre> <p>Now if you use a ID, you can see the type is correctly infered.</p> <p></p>","tags":["instructor"]},{"location":"blog/2024/04/01/announce-instructor-v1/#handling-async-await-create","title":"Handling async: <code>await create</code>","text":"<p>This will also work correctly with asynchronous clients. </p> <pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclient = instructor.from_openai(openai.AsyncOpenAI())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nasync def extract():\n    return await client.chat.completions.create(\n        model=\"gpt-4-turbo-preview\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"Create a user\"},\n        ],\n        response_model=User,\n    )\n</code></pre> <p>Notice that simply because we return the <code>create</code> method, the <code>extract()</code> function will return the correct user type.</p> <p></p>","tags":["instructor"]},{"location":"blog/2024/04/01/announce-instructor-v1/#returning-the-original-completion-create_with_completion","title":"Returning the original completion: <code>create_with_completion</code>","text":"<p>You can also return the original completion object</p> <pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nuser, completion = client.chat.completions.create_with_completion(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Create a user\"},\n    ],\n    response_model=User,\n)\n</code></pre> <p></p>","tags":["instructor"]},{"location":"blog/2024/04/01/announce-instructor-v1/#streaming-partial-objects-create_partial","title":"Streaming Partial Objects: <code>create_partial</code>","text":"<p>In order to handle streams, we still support <code>Iterable[T]</code> and <code>Partial[T]</code> but to simply the type inference, we've added <code>create_iterable</code> and <code>create_partial</code> methods as well!</p> <pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nuser_stream = client.chat.completions.create_partial(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Create a user\"},\n    ],\n    response_model=User,\n)\n\nfor user in user_stream:\n    print(user)\n    # name=None age=None\n    # name='' age=None\n    # name='John' age=None\n    # name='John Doe' age=None\n    # name='John Doe' age=30\n</code></pre> <p>Notice now that the type infered is <code>Generator[User, None]</code></p> <p></p>","tags":["instructor"]},{"location":"blog/2024/04/01/announce-instructor-v1/#streaming-iterables-create_iterable","title":"Streaming Iterables: <code>create_iterable</code>","text":"<p>We get an iterable of objects when we want to extract multiple objects.</p> <pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nusers = client.chat.completions.create_iterable(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Create 2 users\"},\n    ],\n    response_model=User,\n)\n\nfor user in users:\n    print(user)\n    # User(name='John Doe', age=30)\n    # User(name='Jane Smith', age=25)\n</code></pre> <p></p>","tags":["instructor"]},{"location":"blog/2024/04/01/announce-instructor-v1/#validation-and-error-handling","title":"Validation and Error Handling","text":"<p>Instructor has always supported validation and error handling. But now, we've added a new <code>validation_context</code> argument to the <code>create</code> call. This allows you to pass in a <code>ValidationContext</code> object which will be passed to the <code>response_model</code>. This allows you to add custom validation logic to the <code>response_model</code>.</p> <p>If you want to learn more check out the docs on retrying and reasking</p>","tags":["instructor"]},{"location":"blog/2024/04/01/announce-instructor-v1/#support-in-multiple-languages","title":"Support in multiple languages","text":"<p>While each flavor is different the core philosophy is the same. Keeping it as close as possible to the common api allows us to support all the same features in all the same languages by hooking into each libraries's popular validation libraries.</p> <p>Check out:</p> <ul> <li>JavaScript</li> <li>Elixir</li> <li>PHP</li> </ul> <p>If you're interested in contributing, check out the contributing guide, and you want to create instructor in your language, let me know and I can help with promotion and connecting all the docs!</p>","tags":["instructor"]},{"location":"blog/2024/07/11/youtube-transcripts/","title":"Analyzing Youtube Transcripts with Instructor","text":""},{"location":"blog/2024/07/11/youtube-transcripts/#extracting-chapter-information","title":"Extracting Chapter Information","text":"<p>Code Snippets</p> <p>As always, the code is readily available in our <code>examples/youtube</code> folder in our repo for your reference in the <code>run.py</code> file.</p> <p>In this post, we'll show you how to summarise Youtube video transcripts into distinct chapters using <code>instructor</code> before exploring some ways you can adapt the code to different applications.</p> <p>By the end of this article, you'll be able to build an application as per the video below.</p> <p></p> <p>Let's first install the required packages.</p> <pre><code>pip install openai instructor pydantic youtube_transcript_api\n</code></pre> <p>Quick Note</p> <p>The video that we'll be using in this tutorial is A Hacker's Guide To Language Models by Jeremy Howard. It has the video id of <code>jkrNMKz9pWU</code>.</p> <p>Next, let's start by defining a Pydantic Model for the structured chapter information that we want.</p> <pre><code>from pydantic import BaseModel, Field\n\n\nclass Chapter(BaseModel):\n    start_ts: float = Field(\n        ...,\n        description=\"Starting timestamp for a chapter.\",\n    )\n    end_ts: float = Field(\n        ...,\n        description=\"Ending timestamp for a chapter\",\n    )\n    title: str = Field(\n        ..., description=\"A concise and descriptive title for the chapter.\"\n    )\n    summary: str = Field(\n        ...,\n        description=\"A brief summary of the chapter's content, don't use words like 'the speaker'\",\n    )\n</code></pre> <p>We can take advantage of <code>youtube-transcript-api</code> to extract out the transcript of a video using the following function</p> <pre><code>from youtube_transcript_api import YouTubeTranscriptApi\n\n\ndef get_youtube_transcript(video_id: str) -&gt; str:\n    try:\n        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n        return \" \".join(\n            [f\"ts={entry['start']} - {entry['text']}\" for entry in transcript]\n        )\n    except Exception as e:\n        print(f\"Error fetching transcript: {e}\")\n        return \"\"\n</code></pre> <p>Once we've done so, we can then put it all together into the following functions.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom youtube_transcript_api import YouTubeTranscriptApi\n\n# Set up OpenAI client\nclient = instructor.from_openai(OpenAI())\n\n\nclass Chapter(BaseModel):\n    start_ts: float = Field(\n        ...,\n        description=\"The start timestamp indicating when the chapter starts in the video.\",\n    )\n    end_ts: float = Field(\n        ...,\n        description=\"The end timestamp indicating when the chapter ends in the video.\",\n    )\n    title: str = Field(\n        ..., description=\"A concise and descriptive title for the chapter.\"\n    )\n    summary: str = Field(\n        ...,\n        description=\"A brief summary of the chapter's content, don't use words like 'the speaker'\",\n    )\n\n\ndef get_youtube_transcript(video_id: str) -&gt; str:\n    try:\n        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n        return [f\"ts={entry['start']} - {entry['text']}\" for entry in transcript]\n    except Exception as e:\n        print(f\"Error fetching transcript: {e}\")\n        return \"\"\n\n\ndef extract_chapters(transcript: str):\n    return client.chat.completions.create_iterable(\n        model=\"gpt-4o\",  # You can experiment with different models\n        response_model=Chapter,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Analyze the given YouTube transcript and extract chapters. For each chapter, provide a start timestamp, end timestamp, title, and summary.\",\n            },\n            {\"role\": \"user\", \"content\": transcript},\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    transcripts = get_youtube_transcript(\"jkrNMKz9pWU\")\n\n    for transcript in transcripts[:2]:\n        print(transcript)\n        #&gt; ts=0.539 - hi I am Jeremy Howard from fast.ai and\n        #&gt; ts=4.62 - this is a hacker's guide to language\n\n    formatted_transcripts = ''.join(transcripts)\n    chapters = extract_chapters(formatted_transcripts)\n\n    for chapter in chapters:\n        print(chapter.model_dump_json(indent=2))\n        \"\"\"\n        {\n          \"start_ts\": 0.539,\n          \"end_ts\": 9.72,\n          \"title\": \"Introduction\",\n          \"summary\": \"Jeremy Howard from fast.ai introduces the video, mentioning it as a hacker's guide to language models, focusing on a code-first approach.\"\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"start_ts\": 9.72,\n          \"end_ts\": 65.6,\n          \"title\": \"Understanding Language Models\",\n          \"summary\": \"Explains the code-first approach to using language models, suggesting prerequisites such as prior deep learning knowledge and recommends the course.fast.ai for in-depth learning.\"\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"start_ts\": 65.6,\n          \"end_ts\": 250.68,\n          \"title\": \"Basics of Language Models\",\n          \"summary\": \"Covers the concept of language models, demonstrating how they predict the next word in a sentence, and showcases OpenAI's text DaVinci for creative brainstorming with examples.\"\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"start_ts\": 250.68,\n          \"end_ts\": 459.199,\n          \"title\": \"How Language Models Work\",\n          \"summary\": \"Dives deeper into how language models like ULMfit and others were developed, their training on datasets like Wikipedia, and the importance of learning various aspects of the world to predict the next word effectively.\"\n        }\n        \"\"\"\n        # ... other chapters\n</code></pre>"},{"location":"blog/2024/07/11/youtube-transcripts/#alternative-ideas","title":"Alternative Ideas","text":"<p>Now that we've seen a complete example of chapter extraction, let's explore some alternative ideas using different Pydantic models. These models can be used to adapt our YouTube transcript analysis for various applications.</p>"},{"location":"blog/2024/07/11/youtube-transcripts/#1-study-notes-generator","title":"1. Study Notes Generator","text":"<pre><code>from pydantic import BaseModel, Field\nfrom typing import List\n\n\nclass Concept(BaseModel):\n    term: str = Field(..., description=\"A key term or concept mentioned in the video\")\n    definition: str = Field(\n        ..., description=\"A brief definition or explanation of the term\"\n    )\n\n\nclass StudyNote(BaseModel):\n    timestamp: float = Field(\n        ..., description=\"The timestamp where this note starts in the video\"\n    )\n    topic: str = Field(..., description=\"The main topic being discussed at this point\")\n    key_points: List[str] = Field(..., description=\"A list of key points discussed\")\n    concepts: List[Concept] = Field(\n        ..., description=\"Important concepts mentioned in this section\"\n    )\n</code></pre> <p>This model structures the video content into clear topics, key points, and important concepts, making it ideal for revision and study purposes.</p>"},{"location":"blog/2024/07/11/youtube-transcripts/#2-content-summarization","title":"2. Content Summarization","text":"<pre><code>from pydantic import BaseModel, Field\nfrom typing import List\n\n\nclass ContentSummary(BaseModel):\n    title: str = Field(..., description=\"The title of the video\")\n    duration: float = Field(\n        ..., description=\"The total duration of the video in seconds\"\n    )\n    main_topics: List[str] = Field(\n        ..., description=\"A list of main topics covered in the video\"\n    )\n    key_takeaways: List[str] = Field(\n        ..., description=\"The most important points from the entire video\"\n    )\n    target_audience: str = Field(\n        ..., description=\"The intended audience for this content\"\n    )\n</code></pre> <p>This model provides a high-level overview of the entire video, perfect for quick content analysis or deciding whether a video is worth watching in full.</p>"},{"location":"blog/2024/07/11/youtube-transcripts/#3-quiz-generator","title":"3. Quiz Generator","text":"<pre><code>from pydantic import BaseModel, Field\nfrom typing import List\n\n\nclass QuizQuestion(BaseModel):\n    question: str = Field(..., description=\"The quiz question\")\n    options: List[str] = Field(\n        ..., min_items=2, max_items=4, description=\"Possible answers to the question\"\n    )\n    correct_answer: int = Field(\n        ...,\n        ge=0,\n        lt=4,\n        description=\"The index of the correct answer in the options list\",\n    )\n    explanation: str = Field(\n        ..., description=\"An explanation of why the correct answer is correct\"\n    )\n\n\nclass VideoQuiz(BaseModel):\n    title: str = Field(\n        ..., description=\"The title of the quiz, based on the video content\"\n    )\n    questions: List[QuizQuestion] = Field(\n        ...,\n        min_items=5,\n        max_items=20,\n        description=\"A list of quiz questions based on the video content\",\n    )\n</code></pre> <p>This model transforms video content into an interactive quiz, perfect for testing comprehension or creating engaging content for social media.</p> <p>To use these alternative models, you would replace the <code>Chapter</code> model in our original code with one of these alternatives and adjust the system prompt in the <code>extract_chapters</code> function accordingly.</p>"},{"location":"blog/2024/07/11/youtube-transcripts/#conclusion","title":"Conclusion","text":"<p>The power of this approach lies in its flexibility. By defining the result of our function calls as Pydantic Models, we're able to quickly adapt code for a wide variety of applications whether it be generating quizzes, creating study materials or just optimizing for simple SEO.</p>"},{"location":"cli/","title":"Instructor CLI","text":"<p>Welcome to the Instructor Command-Line Interface (CLI), a tool designed to ease your experience with the OpenAI API. Whether it's tracking your API usage or fine-tuning your models, Instructor CLI is your go-to utility.</p>"},{"location":"cli/#quick-start","title":"Quick Start","text":"<p>First things first: make sure your OpenAI API key is set as an environment variable. The CLI will use this for authenticating your requests to OpenAI's services.</p> <p>You can set the API key in your terminal as follows:</p> <pre><code>export OPENAI_API_KEY=\"your-api-key-here\"\n</code></pre>"},{"location":"cli/#installation-setup","title":"Installation &amp; Setup","text":"<pre><code>pip install instructor\n</code></pre>"},{"location":"cli/#features","title":"Features","text":"<ul> <li>API Usage Monitoring: Keep tabs on your API usage right from the terminal. Track token counts, total requests, and even calculate the costs. To learn more, consult the Usage Guide.</li> <li>Model Fine-Tuning: Optimize your models to meet your specific requirements using our fine-tuning app. For more details, check out the Fine-Tuning Guide.</li> </ul>"},{"location":"cli/#support-contribution","title":"Support &amp; Contribution","text":"<p>Need help or want to contribute? Visit our GitHub Repository</p>"},{"location":"cli/batch/","title":"Using the Command Line Interface","text":"<p>The instructor CLI provides functionalities for managing batch jobs on OpenAI</p> <pre><code>$ instructor batch --help\n\n Usage: instructor batch [OPTIONS] COMMAND [ARGS]...\n\n Manage OpenAI Batch jobs\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help          Show this message and exit.                                         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 cancel             Cancel a batch job                                               \u2502\n\u2502 create-from-file   Create a batch job from a file                                   \u2502\n\u2502 list               See all existing batch jobs                                      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"cli/batch/#creating-a-batch-job","title":"Creating a Batch Job","text":""},{"location":"cli/batch/#view-jobs","title":"View Jobs","text":"<pre><code>$ instructor batch list --help\n\n Usage: instructor batch list [OPTIONS]\n\n See all existing batch jobs\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --limit                    INTEGER  Total number of batch jobs to show              \u2502\n\u2502                                     [default: 10]                                   \u2502\n\u2502 --poll                     INTEGER  Time in seconds to wait for the batch job to    \u2502\n\u2502                                     complete                                        \u2502\n\u2502                                     [default: 10]                                   \u2502\n\u2502 --screen    --no-screen             Enable or disable screen output                 \u2502\n\u2502                                     [default: no-screen]                            \u2502\n\u2502 --help                              Show this message and exit.                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>This returns a list of jobs as seen below</p> <pre><code>$ instructor batch list --limit 9\n\n                                   OpenAI Batch Jobs\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Batch ID             \u2503 Created At          \u2503 Status    \u2503 Failed \u2503 Completed \u2503 Total \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 batch_BSMSiMMy8on2D\u2026 \u2502 2024-06-19 15:10:21 \u2502 cancelled \u2502 0      \u2502 298       \u2502 300   \u2502\n\u2502 batch_pD5dqHmqjWYF5\u2026 \u2502 2024-06-19 15:09:38 \u2502 completed \u2502 0      \u2502 15        \u2502 15    \u2502\n\u2502 batch_zsTSsWVLgpEan\u2026 \u2502 2024-06-19 15:06:05 \u2502 completed \u2502 0      \u2502 15        \u2502 15    \u2502\n\u2502 batch_igaa2j9VBVw2Z\u2026 \u2502 2024-06-19 15:01:59 \u2502 completed \u2502 0      \u2502 300       \u2502 300   \u2502\n\u2502 batch_HcjI2wG46Y1LY\u2026 \u2502 2024-06-12 15:45:37 \u2502 completed \u2502 0      \u2502 3         \u2502 3     \u2502\n\u2502 batch_YiRKLAmKBhwxM\u2026 \u2502 2024-06-12 15:09:44 \u2502 completed \u2502 0      \u2502 3         \u2502 3     \u2502\n\u2502 batch_hS0XGlXzTVS7S\u2026 \u2502 2024-06-12 15:05:59 \u2502 completed \u2502 0      \u2502 3         \u2502 3     \u2502\n\u2502 batch_6s4FmcaV7woam\u2026 \u2502 2024-06-12 14:26:34 \u2502 completed \u2502 0      \u2502 3         \u2502 3     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cli/batch/#create-from-file","title":"Create From File","text":"<p>You'll need to supply a valid .jsonl file in order to be able to create a Batch job.</p> Don't have a <code>.jsonl</code> file on hand? <p>You can use Instructor to create the <code>.jsonl</code> with nothing more than simple pydantic and our <code>BatchJob</code> object as seen below.</p> <pre><code>from instructor.batch import BatchJob\nfrom pydantic import BaseModel, Field\nfrom typing import Literal\n\n\nclass Classification(BaseModel):\n    label: Literal[\"SPAM\", \"NOT_SPAM\"] = Field(\n        ..., description=\"Whether the email is spam or not\"\n    )\n\n\nemails = [\n    \"Hello there I'm a Nigerian prince and I want to give you money\",\n    \"Meeting with Thomas has been set at Friday next week\",\n    \"Here are some weekly product updates from our marketing team\",\n]\n\nmessages = [\n    [\n        {\n            \"role\": \"system\",\n            \"content\": f\"Classify the following email {email}\",\n        }\n    ]\n    for email in emails\n]\n\nimport json\n\nwith open(\"output.jsonl\", \"w\") as f:\n    for line in BatchJob.create_from_messages(\n        messages,\n        model=\"gpt-3.5-turbo\",\n        response_model=Classification,\n        max_tokens=100,\n    ):\n        f.write(json.dumps(line) + \"\\n\")\n</code></pre> <p>You can then import in the .jsonl file using the <code>instructor batch create-from-file</code> command</p> <pre><code>$ instructor batch create-from-file --help\n\nUsage: instructor batch create-from-file [OPTIONS]\n\n Create a batch job from a file\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --file-path        TEXT  File containing the batch job requests [default: None]  \u2502\n\u2502                             [required]                                              \u2502\n\u2502    --help                   Show this message and exit.                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"cli/batch/#cancelling-a-batch-job","title":"Cancelling a Batch Job","text":"<p>You can also cancel an outstanding batch job by using the <code>cancel</code> command.</p> <pre><code>$ instructor batch cancel --help\n\n Usage: instructor batch cancel [OPTIONS]\n\n Cancel a batch job\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --batch-id        TEXT  Batch job ID to cancel [default: None] [required]        \u2502\n\u2502    --help                  Show this message and exit.                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"cli/finetune/","title":"Using the Command Line Interface","text":"<p>The instructor CLI provides functionalities for managing fine-tuning jobs on OpenAI.</p> <p>Incomplete API</p> <p>The CLI is still under development and does not yet support all features of the API. If you would like to use a feature that is not yet supported, please consider using the contributing to our library jxnl/instructor instead.</p> <pre><code>!!! note \"Low hanging fruit\"\n\n    If you want to contribute we're looking for a few things:\n\n    1. Adding filenames on upload\n</code></pre>"},{"location":"cli/finetune/#creating-a-fine-tuning-job","title":"Creating a Fine-Tuning Job","text":""},{"location":"cli/finetune/#view-jobs-options","title":"View Jobs Options","text":"<pre><code>$ instructor jobs --help\n\n Usage: instructor jobs [OPTIONS] COMMAND [ARGS]...\n\n Monitor and create fine tuning jobs\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help                            Display the help message.                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 cancel                    Cancel a fine-tuning job.                                                         \u2502\n\u2502 create-from-file          Create a fine-tuning job from a file.                                             \u2502\n\u2502 create-from-id            Create a fine-tuning job from an existing ID.                                     \u2502\n\u2502 list                      Monitor the status of the most recent fine-tuning jobs.                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"cli/finetune/#create-from-file","title":"Create from File","text":"<p>The create-from-file command uploads and trains a model in a single step.</p> <pre><code>\u276f instructor jobs create-from-file --help\n\nUsage: instructor jobs create-from-file [OPTIONS] FILE\n\n Create a fine-tuning job from a file.\n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    file      TEXT  Path to the file for fine-tuning [default: None] [required]                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --model                           TEXT     Model to use for fine-tuning [default: gpt-3.5-turbo]  \u2502\n\u2502 --poll                            INTEGER  Polling interval in seconds [default: 2]               \u2502\n\u2502 --n-epochs                        INTEGER  Number of epochs for fine-tuning                       \u2502\n\u2502 --batch-size                      TEXT     Batch size for fine-tuning                             \u2502\n\u2502 --learning-rate-multiplier        TEXT     Learning rate multiplier for fine-tuning               \u2502\n\u2502 --validation-file                 TEXT     Path to the validation file [default: None]            \u2502\n\u2502 --model-suffix                    TEXT     Suffix to identify the model [default: None]           \u2502\n\u2502 --help                                     Show this message and exit.                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n</code></pre>"},{"location":"cli/finetune/#usage","title":"Usage","text":"<pre><code>$ instructor jobs create-from-file transformed_data.jsonl --validation_file validation_data.jsonl --n_epochs 3 --batch_size 16 --learning_rate_multiplier 0.5\n</code></pre>"},{"location":"cli/finetune/#create-from-id","title":"Create from ID","text":"<p>The create-from-id command uses an uploaded file and trains a model</p> <pre><code>\u276f instructor jobs create-from-id --help\n\n Usage: instructor jobs create-from-id [OPTIONS] ID\n\n Create a fine-tuning job from an existing ID.\n\n\u256d\u2500 Arguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *    id      TEXT  ID of the existing fine-tuning job [default: None] [required]      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --model                           TEXT     Model to use for fine-tuning               \u2502\n\u2502                                            [default: gpt-3.5-turbo]                   \u2502\n\u2502 --n-epochs                        INTEGER  Number of epochs for fine-tuning           \u2502\n\u2502 --batch-size                      TEXT     Batch size for fine-tuning                 \u2502\n\u2502 --learning-rate-multiplier        TEXT     Learning rate multiplier for fine-tuning   \u2502\n\u2502 --validation-file-id              TEXT     ID of the uploaded validation file         \u2502\n\u2502                                            [default: None]                            \u2502\n\u2502 --help                                     Show this message and exit.                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"cli/finetune/#usage_1","title":"Usage","text":"<pre><code>$ instructor files upload transformed_data.jsonl\n$ instructor files upload validation_data.jsonl\n$ instructor files list\n...\n$ instructor jobs create_from_id &lt;file_id&gt; --validation_file &lt;validation_file_id&gt; --n_epochs 3 --batch_size 16 --learning_rate_multiplier 0.5\n</code></pre>"},{"location":"cli/finetune/#viewing-files-and-jobs","title":"Viewing Files and Jobs","text":""},{"location":"cli/finetune/#viewing-jobs","title":"Viewing Jobs","text":"<pre><code>$ instructor jobs list\n\nOpenAI Fine Tuning Job Monitoring\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503                \u2503              \u2503                \u2503     Completion \u2503                 \u2503                \u2503        \u2503                 \u2503\n\u2503 Job ID         \u2503 Status       \u2503  Creation Time \u2503           Time \u2503 Model Name      \u2503 File ID        \u2503 Epochs \u2503 Base Model      \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 ftjob-PWo6uwk\u2026 \u2502 \ud83d\udeab cancelled \u2502     2023-08-23 \u2502            N/A \u2502                 \u2502 file-F7lJg6Z4\u2026 \u2502 3      \u2502 gpt-3.5-turbo-\u2026 \u2502\n\u2502                \u2502              \u2502       23:10:54 \u2502                \u2502                 \u2502                \u2502        \u2502                 \u2502\n\u2502 ftjob-1whjva8\u2026 \u2502 \ud83d\udeab cancelled \u2502     2023-08-23 \u2502            N/A \u2502                 \u2502 file-F7lJg6Z4\u2026 \u2502 3      \u2502 gpt-3.5-turbo-\u2026 \u2502\n\u2502                \u2502              \u2502       22:47:05 \u2502                \u2502                 \u2502                \u2502        \u2502                 \u2502\n\u2502 ftjob-wGoBDld\u2026 \u2502 \ud83d\udeab cancelled \u2502     2023-08-23 \u2502            N/A \u2502                 \u2502 file-F7lJg6Z4\u2026 \u2502 3      \u2502 gpt-3.5-turbo-\u2026 \u2502\n\u2502                \u2502              \u2502       22:44:12 \u2502                \u2502                 \u2502                \u2502        \u2502                 \u2502\n\u2502 ftjob-yd5aRTc\u2026 \u2502 \u2705 succeeded \u2502     2023-08-23 \u2502     2023-08-23 \u2502 ft:gpt-3.5-tur\u2026 \u2502 file-IQxAUDqX\u2026 \u2502 3      \u2502 gpt-3.5-turbo-\u2026 \u2502\n\u2502                \u2502              \u2502       14:26:03 \u2502       15:02:29 \u2502                 \u2502                \u2502        \u2502                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                    Automatically refreshes every 5 seconds, press Ctrl+C to exit\n</code></pre>"},{"location":"cli/finetune/#viewing-files","title":"Viewing Files","text":"<pre><code>$ instructor files list\n\nOpenAI Files\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 File ID                       \u2503 Size (bytes) \u2503 Creation Time       \u2503 Filename \u2503 Purpose   \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 file-0lw2BSNRUlXZXRRu2beCCWjl \u2502       369523 \u2502 2023-08-23 23:31:57 \u2502 file     \u2502 fine-tune \u2502\n\u2502 file-IHaUXcMEykmFUp1kt2puCDEq \u2502       369523 \u2502 2023-08-23 23:09:35 \u2502 file     \u2502 fine-tune \u2502\n\u2502 file-ja9vRBf0FydEOTolaa3BMqES \u2502       369523 \u2502 2023-08-23 22:42:29 \u2502 file     \u2502 fine-tune \u2502\n\u2502 file-F7lJg6Z47CREvmx4kyvyZ6Sn \u2502       369523 \u2502 2023-08-23 22:42:03 \u2502 file     \u2502 fine-tune \u2502\n\u2502 file-YUxqZPyJRl5GJCUTw3cNmA46 \u2502       369523 \u2502 2023-08-23 22:29:10 \u2502 file     \u2502 fine-tune \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cli/finetune/#contributions","title":"Contributions","text":"<p>We aim to provide a light wrapper around the API rather than offering a complete CLI. Contributions are welcome! Please feel free to make an issue at jxnl/instructor/issues or submit a pull request.</p>"},{"location":"cli/usage/","title":"Using the OpenAI API Usage CLI","text":"<p>The OpenAI API Usage CLI tool provides functionalities for monitoring your OpenAI API usage, breaking it down by model, date, and cost.</p>"},{"location":"cli/usage/#monitoring-api-usage","title":"Monitoring API Usage","text":""},{"location":"cli/usage/#view-usage-options","title":"View Usage Options","text":"<pre><code>$ instructor usage --help\n\n Usage: instructor usage [OPTIONS] COMMAND [ARGS]...\n\n Check OpenAI API usage data\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help          Show this message and exit.                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 list       Displays OpenAI API usage data for the past N days.  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"cli/usage/#list-usage-for-specific-number-of-days","title":"List Usage for Specific Number of Days","text":"<p>To display API usage for the past 3 days, use the following command:</p> <pre><code>$ instructor usage list --n 3\n</code></pre> <p>This will output a table similar to:</p> <pre><code>                 Usage Summary by Date, Snapshot, and Cost\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Date       \u2503 Snapshot ID               \u2503 Total Requests \u2503 Total Cost ($) \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 2023-09-04 \u2502 gpt-4-0613                \u2502             44 \u2502           0.68 \u2502\n\u2502 2023-09-04 \u2502 gpt-3.5-turbo-16k-0613    \u2502            195 \u2502           0.84 \u2502\n\u2502 2023-09-04 \u2502 text-embedding-ada-002-v2 \u2502            276 \u2502           0.00 \u2502\n\u2502 2023-09-04 \u2502 gpt-4-32k-0613            \u2502            328 \u2502          49.45 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cli/usage/#list-usage-for-today","title":"List Usage for Today","text":"<p>To display the API usage for today, simply run:</p> <pre><code>$ instructor usage list\n</code></pre>"},{"location":"cli/usage/#contributions","title":"Contributions","text":"<p>We aim to provide a light wrapper around the API rather than offering a complete CLI. Contributions are welcome! Please feel free to make an issue at jxnl/instructor/issues or submit a pull request.</p>"},{"location":"concepts/alias/","title":"Alias","text":"<p>This page is a work in progress</p> <p>This page is a work in progress. Check out Pydantic's documentation</p>"},{"location":"concepts/caching/","title":"Caching","text":"<p>If you want to learn more about concepts in caching and how to use them in your own projects, check out our blog on the topic.</p>"},{"location":"concepts/caching/#1-functoolscache-for-simple-in-memory-caching","title":"1. <code>functools.cache</code> for Simple In-Memory Caching","text":"<p>When to Use: Ideal for functions with immutable arguments, called repeatedly with the same parameters in small to medium-sized applications. This makes sense when we might be reusing the same data within a single session. or in an application where we don't need to persist the cache between sessions.</p> <pre><code>import time\nimport functools\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n@functools.cache\ndef extract(data) -&gt; UserDetail:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ],\n    )\n\n\nstart = time.perf_counter()  # (1)\nmodel = extract(\"Extract jason is 25 years old\")\nprint(f\"Time taken: {time.perf_counter() - start}\")\n#&gt; Time taken: 0.41948329200000023\n\nstart = time.perf_counter()\nmodel = extract(\"Extract jason is 25 years old\")  # (2)\nprint(f\"Time taken: {time.perf_counter() - start}\")\n#&gt; Time taken: 1.4579999998431958e-06\n</code></pre> <ol> <li>Using <code>time.perf_counter()</code> to measure the time taken to run the function is better than using <code>time.time()</code> because it's more accurate and less susceptible to system clock changes.</li> <li>The second time we call <code>extract</code>, the result is returned from the cache, and the function is not called.</li> </ol> <p>Changing the Model does not Invalidate the Cache</p> <p>Note that changing the model does not invalidate the cache. This is because the cache key is based on the function's name and arguments, not the model. This means that if we change the model, the cache will still return the old result.</p> <p>Now we can call <code>extract</code> multiple times with the same argument, and the result will be cached in memory for faster access.</p> <p>Benefits: Easy to implement, provides fast access due to in-memory storage, and requires no additional libraries.</p> What is a decorator? <p>A decorator is a function that takes another function and extends the behavior of the latter function without explicitly modifying it. In Python, decorators are functions that take a function as an argument and return a closure.</p> <pre><code>def decorator(func):\n    def wrapper(*args, **kwargs):\n        print(\"Do something before\")  # (1)\n        #&gt; Do something before\n        result = func(*args, **kwargs)\n        print(\"Do something after\")  # (2)\n        #&gt; Do something after\n        return result\n\n    return wrapper\n\n\n@decorator\ndef say_hello():\n    #&gt; Hello!\n    print(\"Hello!\")\n    #&gt; Hello!\n\n\nsay_hello()\n#&gt; \"Do something before\"\n#&gt; \"Hello!\"\n#&gt; \"Do something after\"\n</code></pre> <ol> <li>The code is executed before the function is called</li> <li>The code is executed after the function is called</li> </ol>"},{"location":"concepts/caching/#2-diskcache-for-persistent-large-data-caching","title":"2. <code>diskcache</code> for Persistent, Large Data Caching","text":"Copy Caching Code <p>We'll be using the same <code>instructor_cache</code> decorator for both <code>diskcache</code> and <code>redis</code> caching. You can copy the code below and use it for both examples.</p> <pre><code>import functools\nimport inspect\nimport diskcache\n\ncache = diskcache.Cache('./my_cache_directory')  # (1)\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):  # (2)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n</code></pre> <ol> <li>We create a new <code>diskcache.Cache</code> instance to store the cached data. This will create a new directory called <code>my_cache_directory</code> in the current working directory.</li> <li>We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic in this example code</li> </ol> <p>Remember that you can change this code to support non-Pydantic models, or to use a different caching backend. More over, don't forget that this cache does not invalidate when the model changes, so you might want to encode the <code>Model.model_json_schema()</code> as part of the key.</p> <p>When to Use: Suitable for applications needing cache persistence between sessions or dealing with large datasets. This is useful when we want to reuse the same data across multiple sessions, or when we need to store large amounts of data!</p> <pre><code>import functools\nimport inspect\nimport instructor\nimport diskcache\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI())\ncache = diskcache.Cache('./my_cache_directory')\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation  # (4)\n    if not issubclass(return_type, BaseModel):  # (1)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = (\n            f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"  #  (2)\n        )\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type (3)\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n@instructor_cache\ndef extract(data) -&gt; UserDetail:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ],\n    )\n</code></pre> <ol> <li>We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic</li> <li>We use functool's <code>_make_key</code> to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately.</li> <li>We use Pydantic's <code>model_validate_json</code> to deserialize the cached result into a Pydantic model.</li> <li>We use <code>inspect.signature</code> to get the function's return type annotation, which we use to validate the cached result.</li> </ol> <p>Benefits: Reduces computation time for heavy data processing, provides disk-based caching for persistence.</p>"},{"location":"concepts/caching/#2-redis-caching-decorator-for-distributed-systems","title":"2. Redis Caching Decorator for Distributed Systems","text":"Copy Caching Code <p>We'll be using the same <code>instructor_cache</code> decorator for both <code>diskcache</code> and <code>redis</code> caching. You can copy the code below and use it for both examples.</p> <pre><code>import functools\nimport inspect\nimport redis\n\ncache = redis.Redis(\"localhost\")\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n</code></pre> <p>Remember that you can change this code to support non-Pydantic models, or to use a different caching backend. More over, don't forget that this cache does not invalidate when the model changes, so you might want to encode the <code>Model.model_json_schema()</code> as part of the key.</p> <p>When to Use: Recommended for distributed systems where multiple processes need to access the cached data, or for applications requiring fast read/write access and handling complex data structures.</p> <pre><code>import redis\nimport functools\nimport inspect\nimport instructor\n\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\ncache = redis.Redis(\"localhost\")\n\n\ndef instructor_cache(func):\n    \"\"\"Cache a function that returns a Pydantic model\"\"\"\n    return_type = inspect.signature(func).return_annotation\n    if not issubclass(return_type, BaseModel):  # (1)\n        raise ValueError(\"The return type must be a Pydantic model\")\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = f\"{func.__name__}-{functools._make_key(args, kwargs, typed=False)}\"  # (2)\n        # Check if the result is already cached\n        if (cached := cache.get(key)) is not None:\n            # Deserialize from JSON based on the return type\n            return return_type.model_validate_json(cached)\n\n        # Call the function and cache its result\n        result = func(*args, **kwargs)\n        serialized_result = result.model_dump_json()\n        cache.set(key, serialized_result)\n\n        return result\n\n    return wrapper\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n@instructor_cache\ndef extract(data) -&gt; UserDetail:\n    # Assuming client.chat.completions.create returns a UserDetail instance\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": data},\n        ],\n    )\n</code></pre> <ol> <li>We only want to cache functions that return a Pydantic model to simplify serialization and deserialization logic</li> <li>We use functool's <code>_make_key</code> to generate a unique key based on the function's name and arguments. This is important because we want to cache the result of each function call separately.</li> </ol> <p>Benefits: Scalable for large-scale systems, supports fast in-memory data storage and retrieval, and is versatile for various data types.</p> <p>Looking carefully</p> <p>If you look carefully at the code above you'll notice that we're using the same <code>instructor_cache</code> decorator as before. The implementation is the same, but we're using a different caching backend!</p>"},{"location":"concepts/distillation/","title":"Distilling python functions into LLM","text":"<p><code>Instructions</code> from the <code>Instructor</code> library offers a seamless way to make language models backward compatible with existing Python functions. By employing Pydantic type hints, it not only ensures compatibility but also facilitates fine-tuning <code>gpt-3.5-turbo</code> to emulate these functions end-to-end.</p> <p>If you want to see the full example checkout examples/distillation</p>"},{"location":"concepts/distillation/#the-challenges-in-function-level-fine-tuning","title":"The Challenges in Function-Level Fine-Tuning","text":"<p>Replicating the behavior of a Python function in a language model involves intricate data preparation. For instance, teaching a model to execute three-digit multiplication is not as trivial as implementing <code>def f(a, b): return a * b</code>. OpenAI's fine-tuning script coupled with their function calling utility provides a structured output, thereby simplifying the data collection process. Additionally, this eliminates the need for passing the schema to the model, thus conserving tokens.</p>"},{"location":"concepts/distillation/#the-role-of-instructions-in-simplifying-the-fine-tuning-process","title":"The Role of <code>Instructions</code> in Simplifying the Fine-Tuning Process","text":"<p>By using <code>Instructions</code>, you can annotate a Python function that returns a Pydantic object, thereby automating the dataset creation for fine-tuning. A handler for logging is all that's needed to build this dataset.</p>"},{"location":"concepts/distillation/#how-to-implement-instructions-in-your-code","title":"How to Implement <code>Instructions</code> in Your Code","text":""},{"location":"concepts/distillation/#quick-start-how-to-use-instructors-distillation-feature","title":"Quick Start: How to Use Instructor's Distillation Feature","text":"<p>Before we dig into the nitty-gritty, let's look at how easy it is to use Instructor's distillation feature to use function calling finetuning to export the data to a JSONL file.</p> <pre><code>import logging\nimport random\nfrom pydantic import BaseModel\nfrom instructor import Instructions  # pip install instructor\n\n# Logging setup\nlogging.basicConfig(level=logging.INFO)\n\ninstructions = Instructions(\n    name=\"three_digit_multiply\",\n    finetune_format=\"messages\",\n    # log handler is used to save the data to a file\n    # you can imagine saving it to a database or other storage\n    # based on your needs!\n    log_handlers=[logging.FileHandler(\"math_finetunes.jsonl\")],\n)\n\n\nclass Multiply(BaseModel):\n    a: int\n    b: int\n    result: int\n\n\n# Define a function with distillation\n# The decorator will automatically generate a dataset for fine-tuning\n# They must return a pydantic model to leverage function calling\n@instructions.distil\ndef fn(a: int, b: int) -&gt; Multiply:\n    resp = a * b\n    return Multiply(a=a, b=b, result=resp)\n\n\n# Generate some data\nfor _ in range(10):\n    random.seed(42)\n    a = random.randint(100, 999)\n    b = random.randint(100, 999)\n    print(fn(a, b))\n    #&gt; a=754 b=214 result=161356\n    #&gt; a=754 b=214 result=161356\n    #&gt; a=754 b=214 result=161356\n    #&gt; a=754 b=214 result=161356\n    #&gt; a=754 b=214 result=161356\n    #&gt; a=754 b=214 result=161356\n    #&gt; a=754 b=214 result=161356\n    #&gt; a=754 b=214 result=161356\n    #&gt; a=754 b=214 result=161356\n    #&gt; a=754 b=214 result=161356\n</code></pre>"},{"location":"concepts/distillation/#the-intricacies-of-fine-tuning-language-models","title":"The Intricacies of Fine-tuning Language Models","text":"<p>Fine-tuning isn't just about writing a function like <code>def f(a, b): return a * b</code>. It requires detailed data preparation and logging. However, Instructor provides a built-in logging feature and structured outputs to simplify this.</p>"},{"location":"concepts/distillation/#why-instructor-and-distillation-are-game-changers","title":"Why Instructor and Distillation are Game Changers","text":"<p>The library offers two main benefits:</p> <ol> <li>Efficiency: Streamlines functions, distilling requirements into model weights and a few lines of code.</li> <li>Integration: Eases combining classical machine learning and language models by providing a simple interface that wraps existing functions.</li> </ol>"},{"location":"concepts/distillation/#role-of-instructor-in-simplifying-fine-tuning","title":"Role of Instructor in Simplifying Fine-Tuning","text":"<p>The <code>from instructor import Instructions</code> feature is a time saver. It auto-generates a fine-tuning dataset, making it a breeze to imitate a function's behavior.</p>"},{"location":"concepts/distillation/#logging-output-and-running-a-finetune","title":"Logging Output and Running a Finetune","text":"<p>Here's how the logging output would look:</p> <pre><code>{\n    \"messages\": [\n        {\"role\": \"system\", \"content\": 'Predict the results of this function: ...'},\n        {\"role\": \"user\", \"content\": 'Return fn(133, b=539)'},\n        {\n            \"role\": \"assistant\",\n            \"function_call\": {\n                \"name\": \"Multiply\",\n                \"arguments\": '{\"a\":133,\"b\":539,\"result\":89509}',\n            },\n        },\n    ],\n    \"functions\": [\n        {\"name\": \"Multiply\", \"description\": \"Correctly extracted `Multiply`...\"}\n    ],\n}\n</code></pre> <p>Run a finetune like this:</p> <pre><code>instructor jobs create-from-file math_finetunes.jsonl\n</code></pre> <p>Once a model is trained you can simply change <code>mode</code> to <code>dispatch</code> and it will use the model to run the function!</p> <pre><code>from instructor import Instructions\nfrom pydantic import BaseModel\n\n\nclass Multiply(BaseModel):\n    a: int\n    b: int\n    result: int\n\n\ninstructions = Instructions(\n    name=\"three_digit_multiply\",\n)\n\n\n@instructions.distil(model='gpt-3.5-turbo:finetuned-123', mode=\"dispatch\")\ndef fn(a: int, b: int) -&gt; Multiply:\n    # now this code will be short circuited and the model will be used instead.\n    resp = a + b\n    return Multiply(a=a, b=b, result=resp)\n</code></pre> <p>With this, you can swap the function implementation, making it backward compatible. You can even imagine using the different models for different tasks or validating and runnign evals by using the original function and comparing it to the distillation.</p>"},{"location":"concepts/enums/","title":"Enums","text":"<p>To prevent data misalignment, we can use Enums for standardized fields. Always include an \"Other\" option as a fallback so the model can signal uncertainty.</p> <pre><code>from pydantic import BaseModel, Field\nfrom enum import Enum\n\n\nclass Role(Enum):\n    PRINCIPAL = \"PRINCIPAL\"\n    TEACHER = \"TEACHER\"\n    STUDENT = \"STUDENT\"\n    OTHER = \"OTHER\"\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role = Field(\n        description=\"Correctly assign one of the predefined roles to the user.\"\n    )\n</code></pre> <p>If you're having a hard time with <code>Enum</code> an alternative is to use <code>Literal</code> instead.</p> <pre><code>from typing import Literal\nfrom pydantic import BaseModel\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Literal[\"PRINCIPAL\", \"TEACHER\", \"STUDENT\", \"OTHER\"]\n</code></pre>"},{"location":"concepts/fastapi/","title":"Integrating Pydantic Models with FastAPI","text":"<p>FastAPI is an enjoyable tool for building web applications in Python. It is well known for its integration with <code>Pydantic</code> models, which makes defining and validating data structures straightforward and efficient. In this guide, we explore how simple functions that return <code>Pydantic</code> models can seamlessly integrate with <code>FastAPI</code>.</p>"},{"location":"concepts/fastapi/#why-choose-fastapi-and-pydantic","title":"Why Choose FastAPI and Pydantic?","text":"<ul> <li>FastAPI is a modern, high-performance web framework for building APIs with Python.</li> <li>Supports OpenAPI and JSON Schema for automatic documentation and validation.</li> <li>Supports AsyncIO for asynchronous programming leveraging the AsyncOpenAI() client</li> </ul>"},{"location":"concepts/fastapi/#code-example-starting-a-fastapi-app-with-a-post-request","title":"Code Example: Starting a FastAPI App with a POST Request","text":"<p>The following code snippet demonstrates how to start a <code>FastAPI</code> app with a POST endpoint. This endpoint accepts and returns data defined by a <code>Pydantic</code> model.</p> <pre><code>import instructor\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom openai import AsyncOpenAI\n\n# Enables response_model\nclient = instructor.from_openai(AsyncOpenAI())\napp = FastAPI()\n\n\nclass UserData(BaseModel):\n    # This can be the model for the input data\n    query: str\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n@app.post(\"/endpoint\", response_model=UserDetail)\nasync def endpoint_function(data: UserData) -&gt; UserDetail:\n    user_detail = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Extract: `{data.query}`\"},\n        ],\n    )\n    return user_detail\n</code></pre>"},{"location":"concepts/fastapi/#streaming-responses-with-fastapi","title":"Streaming Responses with FastAPI","text":"<p><code>FastAPI</code> supports streaming responses, which is useful for returning large amounts of data. This feature is particularly useful when working with large language models (LLMs) that generate a large amount of data.</p> <pre><code>from fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n\nclass UserData(BaseModel):\n    query: str\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\n# Route to handle SSE events and return users\n@app.post(\"/extract\", response_class=StreamingResponse)\nasync def extract(data: UserData):\n    users = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Iterable[UserDetail],\n        stream=True,\n        messages=[\n            {\"role\": \"user\", \"content\": data.query},\n        ],\n    )\n\n    async def generate():\n        async for user in users:\n            resp_json = user.model_dump_json()\n            yield f\"data: {resp_json}\"\n        yield \"data: [DONE]\"\n\n    return StreamingResponse(generate(), media_type=\"text/event-stream\")\n</code></pre>"},{"location":"concepts/fastapi/#automatic-documentation-with-fastapi","title":"Automatic Documentation with FastAPI","text":"<p>FastAPI leverages the OpenAPI specification to automatically generate a dynamic and interactive documentation page, commonly referred to as the <code>/docs</code> page. This feature is incredibly useful for developers, as it offers a live environment to test API endpoints directly through the browser.</p> <p>To explore the capabilities of your API, follow these steps:</p> <ol> <li>Run the API using the Uvicorn command: <code>uvicorn main:app --reload</code>.</li> <li>Open your web browser and navigate to <code>http://127.0.0.1:8000/docs</code>.</li> <li>You will find an interactive UI where you can send different requests to your API and see the responses in real-time.</li> </ol> <p></p>"},{"location":"concepts/fields/","title":"Fields","text":"<p>The <code>pydantic.Field</code> function is used to customize and add metadata to fields of models. To learn more, check out the Pydantic documentation as this is a near replica of that documentation that is relevant to prompting.</p>"},{"location":"concepts/fields/#default-values","title":"Default values","text":"<p>The <code>default</code> parameter is used to define a default value for a field.</p> <pre><code>from pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    name: str = Field(default='John Doe')\n\n\nuser = User()\nprint(user)\n#&gt; name='John Doe'\n</code></pre> <p>You can also use <code>default_factory</code> to define a callable that will be called to generate a default value.</p> <pre><code>from uuid import uuid4\n\nfrom pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    id: str = Field(default_factory=lambda: uuid4().hex)\n</code></pre> <p>Info</p> <p>The <code>default</code> and <code>default_factory</code> parameters are mutually exclusive.</p> <p>Note</p> <p>If you use <code>typing.Optional</code>, it doesn't mean that the field has a default value of <code>None</code> you must use <code>default</code> or <code>default_factory</code> to define a default value. Then it will be considered <code>not required</code> when sent to the language model.</p>"},{"location":"concepts/fields/#using-annotated","title":"Using <code>Annotated</code>","text":"<p>The <code>Field</code> function can also be used together with <code>Annotated</code>.</p> <pre><code>from uuid import uuid4\nfrom typing_extensions import Annotated\nfrom pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    id: Annotated[str, Field(default_factory=lambda: uuid4().hex)]\n</code></pre>"},{"location":"concepts/fields/#exclude","title":"Exclude","text":"<p>The <code>exclude</code> parameter can be used to control which fields should be excluded from the model when exporting the model. This is helpful when you want to exclude fields that are not relevant to the model generation like <code>scratch_pad</code> or <code>chain_of_thought</code></p> <p>See the following example:</p> <pre><code>from pydantic import BaseModel, Field\nfrom datetime import date\n\n\nclass DateRange(BaseModel):\n    chain_of_thought: str = Field(\n        description=\"Reasoning behind the date range.\", exclude=True\n    )\n    start_date: date\n    end_date: date\n\n\ndate_range = DateRange(\n    chain_of_thought=\"\"\"\n        I want to find the date range for the last 30 days.\n        Today is 2021-01-30 therefore the start date\n        should be 2021-01-01 and the end date is 2021-01-30\"\"\",\n    start_date=date(2021, 1, 1),\n    end_date=date(2021, 1, 30),\n)\nprint(date_range.model_dump_json())\n#&gt; {\"start_date\":\"2021-01-01\",\"end_date\":\"2021-01-30\"}\n</code></pre>"},{"location":"concepts/fields/#omitting-fields-from-schema-sent-to-the-language-model","title":"Omitting fields from schema sent to the language model","text":"<p>In some cases, you may wish to have the language model ignore certain fields in your model. You can do this by using Pydantic's <code>SkipJsonSchema</code> annotation. This omits a field from the JSON schema emitted by Pydantic (which <code>instructor</code> uses for constructing its prompts and tool definitions). For example:</p> <pre><code>from pydantic import BaseModel\nfrom pydantic.json_schema import SkipJsonSchema\nfrom typing import Union\n\n\nclass Response(BaseModel):\n    question: str\n    answer: str\n    private_field: SkipJsonSchema[Union[str, None]] = None\n\n\nassert \"private_field\" not in Response.model_json_schema()[\"properties\"]\n</code></pre> <p>Note that because the language model will never return a value for <code>private_field</code>, you'll need a default value (this can be a generator via a declared Pydantic <code>Field</code>). </p>"},{"location":"concepts/fields/#customizing-json-schema","title":"Customizing JSON Schema","text":"<p>There are some fields that are exclusively used to customise the generated JSON Schema:</p> <ul> <li><code>title</code>: The title of the field.</li> <li><code>description</code>: The description of the field.</li> <li><code>examples</code>: The examples of the field.</li> <li><code>json_schema_extra</code>: Extra JSON Schema properties to be added to the field.</li> </ul> <p>These all work as great opportunities to add more information to the JSON schema as part of your prompt engineering.</p> <p>Here's an example:</p> <pre><code>from pydantic import BaseModel, Field, SecretStr\n\n\nclass User(BaseModel):\n    age: int = Field(description='Age of the user')\n    name: str = Field(title='Username')\n    password: SecretStr = Field(\n        json_schema_extra={\n            'title': 'Password',\n            'description': 'Password of the user',\n            'examples': ['123456'],\n        }\n    )\n\n\nprint(User.model_json_schema())\n\"\"\"\n{\n    'properties': {\n        'age': {'description': 'Age of the user', 'title': 'Age', 'type': 'integer'},\n        'name': {'title': 'Username', 'type': 'string'},\n        'password': {\n            'description': 'Password of the user',\n            'examples': ['123456'],\n            'format': 'password',\n            'title': 'Password',\n            'type': 'string',\n            'writeOnly': True,\n        },\n    },\n    'required': ['age', 'name', 'password'],\n    'title': 'User',\n    'type': 'object',\n}\n\"\"\"\n</code></pre>"},{"location":"concepts/fields/#general-notes-on-json-schema-generation","title":"General notes on JSON schema generation","text":"<ul> <li>The JSON schema for Optional fields indicates that the value null is allowed.</li> <li>The Decimal type is exposed in JSON schema (and serialized) as a string.</li> <li>The JSON schema does not preserve namedtuples as namedtuples.</li> <li>When they differ, you can specify whether you want the JSON schema to represent the inputs to validation or the outputs from serialization.</li> <li>Sub-models used are added to the <code>$defs</code> JSON attribute and referenced, as per the spec.</li> <li>Sub-models with modifications (via the Field class) like a custom title, description, or default value, are recursively included instead of referenced.</li> <li>The description for models is taken from either the docstring of the class or the argument description to the Field class.</li> </ul>"},{"location":"concepts/lists/","title":"Multi-task and Streaming","text":"<p>A common use case of structured extraction is defining a single schema class and then making another schema to create a list to do multiple extraction</p> <pre><code>from typing import List\nfrom pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nclass Users(BaseModel):\n    users: List[User]\n\n\nprint(Users.model_json_schema())\n\"\"\"\n{\n    '$defs': {\n        'User': {\n            'properties': {\n                'name': {'title': 'Name', 'type': 'string'},\n                'age': {'title': 'Age', 'type': 'integer'},\n            },\n            'required': ['name', 'age'],\n            'title': 'User',\n            'type': 'object',\n        }\n    },\n    'properties': {\n        'users': {'items': {'$ref': '#/$defs/User'}, 'title': 'Users', 'type': 'array'}\n    },\n    'required': ['users'],\n    'title': 'Users',\n    'type': 'object',\n}\n\"\"\"\n</code></pre> <p>Defining a task and creating a list of classes is a common enough pattern that we make this convenient by making use of <code>Iterable[T]</code>. This lets us dynamically create a new class that:</p> <ol> <li>Has dynamic docstrings and class name based on the task</li> <li>Support streaming by collecting tokens until a task is received back out.</li> </ol>"},{"location":"concepts/lists/#extracting-tasks-using-iterable","title":"Extracting Tasks using Iterable","text":"<p>By using <code>Iterable</code> you get a very convenient class with prompts and names automatically defined:</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.function_calls.Mode.JSON)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nusers = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",\n    temperature=0.1,\n    response_model=Iterable[User],\n    stream=False,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Consider this data: Jason is 10 and John is 30.\\\n                         Correctly segment it into entitites\\\n                        Make sure the JSON is correct\",\n        },\n    ],\n)\nfor user in users:\n    print(user)\n    #&gt; name='Jason' age=10\n    #&gt; name='John' age=30\n</code></pre>"},{"location":"concepts/lists/#streaming-tasks","title":"Streaming Tasks","text":"<p>We can also generate tasks as the tokens are streamed in by defining an <code>Iterable[T]</code> type.</p> <p>Lets look at an example in action with the same class</p> <pre><code>import instructor\nimport openai\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI(), mode=instructor.Mode.TOOLS)\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nusers = client.chat.completions.create(\n    model=\"gpt-4\",\n    temperature=0.1,\n    stream=True,\n    response_model=Iterable[User],\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a perfect entity extraction system\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": (f\"Extract `Jason is 10 and John is 10`\"),\n        },\n    ],\n    max_tokens=1000,\n)\n\nfor user in users:\n    print(user)\n    #&gt; name='Jason' age=10\n    #&gt; name='John' age=10\n</code></pre>"},{"location":"concepts/lists/#asynchronous-streaming","title":"Asynchronous Streaming","text":"<p>I also just want to call out in this example that <code>instructor</code> also supports asynchronous streaming. This is useful when you want to stream a response model and process the results as they come in, but you'll need to use the <code>async for</code> syntax to iterate over the results.</p> <pre><code>import instructor\nimport openai\nfrom typing import Iterable\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.AsyncOpenAI(), mode=instructor.Mode.TOOLS)\n\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n\nasync def print_iterable_results():\n    model = await client.chat.completions.create(\n        model=\"gpt-4\",\n        response_model=Iterable[UserExtract],\n        max_retries=2,\n        stream=True,\n        messages=[\n            {\"role\": \"user\", \"content\": \"Make two up people\"},\n        ],\n    )\n    async for m in model:\n        print(m)\n        #&gt; name='John Doe' age=25\n        #&gt; name='Jane Doe' age=28\n\n\nimport asyncio\n\nasyncio.run(print_iterable_results())\n</code></pre>"},{"location":"concepts/logging/","title":"Logging","text":"<p>In order to see the requests made to OpenAI and the responses, you can set logging to DEBUG. This will show the requests and responses made to OpenAI. This can be useful for debugging and understanding the requests and responses made to OpenAI. I would love some contributions that make this a lot cleaner, but for now this is the fastest way to see the prompts. </p> <pre><code>import instructor\nimport openai\nimport logging\n\nfrom pydantic import BaseModel\n\n\n# Set logging to DEBUG\nlogging.basicConfig(level=logging.DEBUG)\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\nuser = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"},\n    ],\n)  # type: ignore\n\n\"\"\" \n...\nDEBUG:instructor:Patching `client.chat.completions.create` with mode=&lt;Mode.TOOLS: 'tool_call'&gt;\nDEBUG:instructor:Instructor Request: mode.value='tool_call', response_model=&lt;class '__main__.UserDetail'&gt;, new_kwargs={'model': 'gpt-3.5-turbo', 'messages': [{'role': 'user', 'content': 'Extract Jason is 25 years old'}], 'tools': [{'type': 'function', 'function': {'name': 'UserDetail', 'description': 'Correctly extracted `UserDetail` with all the required parameters with correct types', 'parameters': {'properties': {'name': {'title': 'Name', 'type': 'string'}, 'age': {'title': 'Age', 'type': 'integer'}}, 'required': ['age', 'name'], 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'UserDetail'}}}\nDEBUG:instructor:max_retries: 1\n...\nDEBUG:instructor:Instructor Pre-Response: ChatCompletion(id='chatcmpl-8zBxMxsOqm5Sj6yeEI38PnU2r6ncC', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_E1cftF5U0zEjzIbWt3q0ZLbN', function=Function(arguments='{\"name\":\"Jason\",\"age\":25}', name='UserDetail'), type='function')]))], created=1709594660, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_2b778c6b35', usage=CompletionUsage(completion_tokens=9, prompt_tokens=81, total_tokens=90))\nDEBUG:httpcore.connection:close.started\nDEBUG:httpcore.connection:close.complete\n\"\"\"\n</code></pre>"},{"location":"concepts/maybe/","title":"Handling Missing Data","text":"<p>The <code>Maybe</code> pattern is a concept in functional programming used for error handling. Instead of raising exceptions or returning <code>None</code>, you can use a <code>Maybe</code> type to encapsulate both the result and potential errors.</p> <p>This pattern is particularly useful when making LLM calls, as providing language models with an escape hatch can effectively reduce hallucinations.</p>"},{"location":"concepts/maybe/#defining-the-model","title":"Defining the Model","text":"<p>Using Pydantic, we'll first define the <code>UserDetail</code> and <code>MaybeUser</code> classes.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import Optional\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n\n\nclass MaybeUser(BaseModel):\n    result: Optional[UserDetail] = Field(default=None)\n    error: bool = Field(default=False)\n    message: Optional[str] = Field(default=None)\n\n    def __bool__(self):\n        return self.result is not None\n</code></pre> <p>Notice that <code>MaybeUser</code> has a <code>result</code> field that is an optional <code>UserDetail</code> instance where the extracted data will be stored. The <code>error</code> field is a boolean that indicates whether an error occurred, and the <code>message</code> field is an optional string that contains the error message.</p>"},{"location":"concepts/maybe/#defining-the-function","title":"Defining the function","text":"<p>Once we have the model defined, we can create a function that uses the <code>Maybe</code> pattern to extract the data.</p> <pre><code>import instructor\nimport openai\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\n\n# This enables the `response_model` keyword\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n\n\nclass MaybeUser(BaseModel):\n    result: Optional[UserDetail] = Field(default=None)\n    error: bool = Field(default=False)\n    message: Optional[str] = Field(default=None)\n\n    def __bool__(self):\n        return self.result is not None\n\n\ndef extract(content: str) -&gt; MaybeUser:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=MaybeUser,\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Extract `{content}`\"},\n        ],\n    )\n\n\nuser1 = extract(\"Jason is a 25-year-old scientist\")\nprint(user1.model_dump_json(indent=2))\n\"\"\"\n{\n  \"result\": {\n    \"age\": 25,\n    \"name\": \"Jason\",\n    \"role\": \"scientist\"\n  },\n  \"error\": false,\n  \"message\": null\n}\n\"\"\"\n\nuser2 = extract(\"Unknown user\")\nprint(user2.model_dump_json(indent=2))\n\"\"\"\n{\n  \"result\": null,\n  \"error\": true,\n  \"message\": \"User details could not be extracted\"\n}\n\"\"\"\n</code></pre> <p>As you can see, when the data is extracted successfully, the <code>result</code> field contains the <code>UserDetail</code> instance. When an error occurs, the <code>error</code> field is set to <code>True</code>, and the <code>message</code> field contains the error message.</p> <p>If you want to learn more about pattern matching, check out Pydantic's docs on Structural Pattern Matching</p>"},{"location":"concepts/models/","title":"Response Model","text":"<p>Defining LLM output schemas in Pydantic is done via <code>pydantic.BaseModel</code>. To learn more about models in Pydantic, check out their documentation.</p> <p>After defining a Pydantic model, we can use it as the <code>response_model</code> in your client <code>create</code> calls to OpenAI or any other supported model. The job of the <code>response_model</code> parameter is to:</p> <ul> <li>Define the schema and prompts for the language model</li> <li>Validate the response from the API</li> <li>Return a Pydantic model instance.</li> </ul>"},{"location":"concepts/models/#prompting","title":"Prompting","text":"<p>When defining a response model, we can use docstrings and field annotations to define the prompt that will be used to generate the response.</p> <pre><code>from pydantic import BaseModel, Field\n\n\nclass User(BaseModel):\n    \"\"\"\n    This is the prompt that will be used to generate the response.\n    Any instructions here will be passed to the language model.\n    \"\"\"\n\n    name: str = Field(description=\"The name of the user.\")\n    age: int = Field(description=\"The age of the user.\")\n</code></pre> <p>Here all docstrings, types, and field annotations will be used to generate the prompt. The prompt will be generated by the <code>create</code> method of the client and will be used to generate the response.</p>"},{"location":"concepts/models/#optional-values","title":"Optional Values","text":"<p>If we use <code>Optional</code> and <code>default</code>, they will be considered not required when sent to the language model.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import Optional\n\n\nclass User(BaseModel):\n    name: str = Field(description=\"The name of the user.\")\n    age: int = Field(description=\"The age of the user.\")\n    email: Optional[str] = Field(description=\"The email of the user.\", default=None)\n</code></pre> <p>Note that fields can also be omitted entirely from being sent to the language model by using Pydantic's <code>SkipJsonSchema</code> annotation. See Fields for additional details.</p>"},{"location":"concepts/models/#dynamic-model-creation","title":"Dynamic model creation","text":"<p>There are some occasions where it is desirable to create a model using runtime information to specify the fields. For this, Pydantic provides the create_model function to allow models to be created on the fly:</p> <pre><code>from pydantic import BaseModel, create_model\n\n\nclass FooModel(BaseModel):\n    foo: str\n    bar: int = 123\n\n\nBarModel = create_model(\n    'BarModel',\n    apple=(str, 'russet'),\n    banana=(str, 'yellow'),\n    __base__=FooModel,\n)\nprint(BarModel)\n#&gt; &lt;class '__main__.BarModel'&gt;\nprint(BarModel.model_fields.keys())\n#&gt; dict_keys(['foo', 'bar', 'apple', 'banana'])\n</code></pre> When would I use this? <p>Consider a situation where the model is dynamically defined, based on some configuration or database. For example, we could have a database table that stores the properties of a model for some model name or id. We could then query the database for the properties of the model and use that to create the model.</p> <pre><code>SELECT property_name, property_type, description\nFROM prompt\nWHERE model_name = {model_name}\n</code></pre> <p>We can then use this information to create the model.</p> <pre><code>from pydantic import BaseModel, create_model\nfrom typing import List\n\ntypes = {\n    'string': str,\n    'integer': int,\n    'boolean': bool,\n    'number': float,\n    'List[str]': List[str],\n}\n\n# Mocked cursor.fetchall()\ncursor = [\n    ('name', 'string', 'The name of the user.'),\n    ('age', 'integer', 'The age of the user.'),\n    ('email', 'string', 'The email of the user.'),\n]\n\nBarModel = create_model(\n    'User',\n    **{\n        property_name: (types[property_type], description)\n        for property_name, property_type, description in cursor\n    },\n    __base__=BaseModel,\n)\n\nprint(BarModel.model_json_schema())\n\"\"\"\n{\n    'properties': {\n        'name': {'default': 'The name of the user.', 'title': 'Name', 'type': 'string'},\n        'age': {'default': 'The age of the user.', 'title': 'Age', 'type': 'integer'},\n        'email': {\n            'default': 'The email of the user.',\n            'title': 'Email',\n            'type': 'string',\n        },\n    },\n    'title': 'User',\n    'type': 'object',\n}\n\"\"\"\n</code></pre> <p>This would be useful when different users have different descriptions for the same model. We can use the same model but have different prompts for each user.</p>"},{"location":"concepts/models/#adding-behavior","title":"Adding Behavior","text":"<p>We can add methods to our Pydantic models, just as any plain Python class. We might want to do this to add some custom logic to our models.</p> <pre><code>from pydantic import BaseModel\nfrom typing import Literal\n\nfrom openai import OpenAI\n\nimport instructor\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass SearchQuery(BaseModel):\n    query: str\n    query_type: Literal[\"web\", \"image\", \"video\"]\n\n    def execute(self):\n        print(f\"Searching for {self.query} of type {self.query_type}\")\n        #&gt; Searching for cat of type image\n        return \"Results for cat\"\n\n\nquery = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"user\", \"content\": \"Search for a picture of a cat\"}],\n    response_model=SearchQuery,\n)\n\nresults = query.execute()\nprint(results)\n#&gt; Results for cat\n</code></pre> <p>Now we can call <code>execute</code> on our model instance after extracting it from a language model. If you want to see more examples of this checkout our post on RAG is more than embeddings</p>"},{"location":"concepts/parallel/","title":"Parallel Tools","text":"<p>One of the latest capabilities that OpenAI has recently introduced is parallel function calling. To learn more you can read up on this</p> <p>Experimental Feature</p> <p>This feature is currently in preview and is subject to change. only supported by the <code>gpt-4-turbo-preview</code> model.</p>"},{"location":"concepts/parallel/#understanding-parallel-function-calling","title":"Understanding Parallel Function Calling","text":"<p>By using parallel function callings that allow you to call multiple functions in a single request, you can significantly reduce the latency of your application without having to use tricks with now one builds a schema.</p> <pre><code>from __future__ import annotations\n\nimport openai\nimport instructor\n\nfrom typing import Iterable, Literal\nfrom pydantic import BaseModel\n\n\nclass Weather(BaseModel):\n    location: str\n    units: Literal[\"imperial\", \"metric\"]\n\n\nclass GoogleSearch(BaseModel):\n    query: str\n\n\nclient = instructor.from_openai(\n    openai.OpenAI(), mode=instructor.Mode.PARALLEL_TOOLS\n)  # (1)!\n\nfunction_calls = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You must always use tools\"},\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the weather in toronto and dallas and who won the super bowl?\",\n        },\n    ],\n    response_model=Iterable[Weather | GoogleSearch],  # (2)!\n)\n\nfor fc in function_calls:\n    print(fc)\n    #&gt; location='Toronto' units='metric'\n    #&gt; location='Dallas' units='imperial'\n    #&gt; query='super bowl winner'\n</code></pre> <ol> <li>Set the mode to <code>PARALLEL_TOOLS</code> to enable parallel function calling.</li> <li>Set the response model to <code>Iterable[Weather | GoogleSearch]</code> to indicate that the response will be a list of <code>Weather</code> and <code>GoogleSearch</code> objects. This is necessary because the response will be a list of objects, and we need to specify the types of the objects in the list.</li> </ol> <p>Noticed that the <code>response_model</code> Must be in the form <code>Iterable[Type1 | Type2 | ...]</code> or <code>Iterable[Type1]</code> where <code>Type1</code> and <code>Type2</code> are the types of the objects that will be returned in the response.</p>"},{"location":"concepts/partial/","title":"Streaming Partial Responses","text":"<p>Field level streaming provides incremental snapshots of the current state of the response model that are immediately useable. This approach is particularly relevant in contexts like rendering UI components.</p> <p>Instructor supports this pattern by making use of <code>create_partial</code>. This lets us dynamically create a new class that treats all of the original model's fields as <code>Optional</code>.</p>"},{"location":"concepts/partial/#understanding-partial-responses","title":"Understanding Partial Responses","text":"<p>Consider what happens whene we define a response model:</p> <pre><code>from pydantic import BaseModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n</code></pre> <p>If we streamed json out from OpenAI, we would only be able to parse when the object is completed returned!</p> <pre><code>{\"name\": \"Jo\n{\"name\": \"John\", \"ag\n{\"name\": \"John\", \"age\":\n{\"name\": \"John\", \"age\": 25} # Completed\n</code></pre> <p>When specifying a <code>create_partial</code> and setting <code>stream=True</code>, the response from <code>instructor</code> becomes a <code>Generator[T]</code>. As the generator yields results, you can iterate over these incremental updates. The last value yielded by the generator represents the completed extraction!</p> <pre><code>{\"name\": \"Jo                 =&gt; User(name=\"Jo\", age=None)\n{\"name\": \"John\", \"ag         =&gt; User(name=\"John\", age=None)\n{\"name\": \"John\", \"age:       =&gt; User(name=\"John\", age=None)\n{\"name\": \"John\", \"age\": 25}  =&gt; User(name=\"John\", age=25)\n</code></pre> <p>Limited Validator Support</p> <p>Due to the streaming nature of the response model, we do not support validators since they would not be able to be applied to the streaming response.</p> <p>Let's look at an example of streaming an extraction of conference information, that would be used to stream in an react component.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nfrom typing import List\nfrom rich.console import Console\n\nclient = instructor.from_openai(OpenAI())\n\ntext_block = \"\"\"\nIn our recent online meeting, participants from various backgrounds joined to discuss the upcoming tech conference. The names and contact details of the participants were as follows:\n\n- Name: John Doe, Email: johndoe@email.com, Twitter: @TechGuru44\n- Name: Jane Smith, Email: janesmith@email.com, Twitter: @DigitalDiva88\n- Name: Alex Johnson, Email: alexj@email.com, Twitter: @CodeMaster2023\n\nDuring the meeting, we agreed on several key points. The conference will be held on March 15th, 2024, at the Grand Tech Arena located at 4521 Innovation Drive. Dr. Emily Johnson, a renowned AI researcher, will be our keynote speaker.\n\nThe budget for the event is set at $50,000, covering venue costs, speaker fees, and promotional activities. Each participant is expected to contribute an article to the conference blog by February 20th.\n\nA follow-up meetingis scheduled for January 25th at 3 PM GMT to finalize the agenda and confirm the list of speakers.\n\"\"\"\n\n\nclass User(BaseModel):\n    name: str\n    email: str\n    twitter: str\n\n\nclass MeetingInfo(BaseModel):\n    users: List[User]\n    date: str\n    location: str\n    budget: int\n    deadline: str\n\n\nextraction_stream = client.chat.completions.create_partial(\n    model=\"gpt-4\",\n    response_model=MeetingInfo,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"Get the information about the meeting and the users {text_block}\",\n        },\n    ],\n    stream=True,\n)\n\n\nconsole = Console()\n\nfor extraction in extraction_stream:\n    obj = extraction.model_dump()\n    console.clear()\n    console.print(obj)\n\nprint(extraction.model_dump_json(indent=2))\n\"\"\"\n{\n  \"users\": [\n    {\n      \"name\": \"John Doe\",\n      \"email\": \"johndoe@email.com\",\n      \"twitter\": \"@TechGuru44\"\n    },\n    {\n      \"name\": \"Jane Smith\",\n      \"email\": \"janesmith@email.com\",\n      \"twitter\": \"@DigitalDiva88\"\n    },\n    {\n      \"name\": \"Alex Johnson\",\n      \"email\": \"alexj@email.com\",\n      \"twitter\": \"@CodeMaster2023\"\n    }\n  ],\n  \"date\": \"2024-03-15\",\n  \"location\": \"Grand Tech Arena located at 4521 Innovation Drive\",\n  \"budget\": 50000,\n  \"deadline\": \"2024-02-20\"\n}\n\"\"\"\n</code></pre> <p>This will output the following:</p> <p></p>"},{"location":"concepts/partial/#asynchronous-streaming","title":"Asynchronous Streaming","text":"<p>I also just want to call out in this example that <code>instructor</code> also supports asynchronous streaming. This is useful when you want to stream a response model and process the results as they come in, but you'll need to use the <code>async for</code> syntax to iterate over the results.</p> <pre><code>import instructor\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(AsyncOpenAI())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nasync def print_partial_results():\n    user = client.chat.completions.create_partial(\n        model=\"gpt-4-turbo-preview\",\n        response_model=User,\n        max_retries=2,\n        stream=True,\n        messages=[\n            {\"role\": \"user\", \"content\": \"Jason is 12 years old\"},\n        ],\n    )\n    async for m in user:\n        print(m)\n        #&gt; name=None age=None\n        #&gt; name=None age=None\n        #&gt; name=None age=None\n        #&gt; name=None age=None\n        #&gt; name=None age=12\n        #&gt; name=None age=12\n        #&gt; name=None age=12\n        #&gt; name=None age=12\n        #&gt; name=None age=12\n        #&gt; name='Jason' age=12\n\n\nimport asyncio\n\nasyncio.run(print_partial_results())\n</code></pre>"},{"location":"concepts/patching/","title":"Patching","text":"<p>Instructor enhances client functionality with three new keywords for backwards compatibility. This allows use of the enhanced client as usual, with structured output benefits.</p> <ul> <li><code>response_model</code>: Defines the response type for <code>chat.completions.create</code>.</li> <li><code>max_retries</code>: Determines retry attempts for failed <code>chat.completions.create</code> validations.</li> <li><code>validation_context</code>: Provides extra context to the validation process.</li> </ul> <p>The default mode is <code>instructor.Mode.TOOLS</code> which is the recommended mode for OpenAI clients. This mode is the most stable and is the most recommended for OpenAI clients. The other modes are for other clients and are not recommended for OpenAI clients.</p>"},{"location":"concepts/patching/#tool-calling","title":"Tool Calling","text":"<p>This is the recommended method for OpenAI clients. It is the most stable as functions is being deprecated soon.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.Mode.TOOLS)\n</code></pre>"},{"location":"concepts/patching/#gemini-tool-calling","title":"Gemini Tool Calling","text":"<p>Gemini supports tool calling for stuctured data extraction. Gemini tool calling requires <code>jsonref</code> to be installed.</p> <p>Limitations</p> <p>Gemini tool calling comes with some known limitations:</p> <pre><code>- `strict` Pydantic validation can fail for integer/float and enum validations\n- Gemini tool calling is incompatible with Pydantic schema customizations such as examples due to API limitations and may result in errors\n- Gemini can sometimes call the wrong function name, resulting in malformed or invalid json\n- Gemini tool calling could fail with enum and literal field types\n</code></pre> <pre><code>import instructor\nimport google.generativeai as genai\n\nclient = instructor.from_gemini(\n    genai.GenerativeModel(), mode=instructor.Mode.GEMINI_TOOLS\n)\n</code></pre>"},{"location":"concepts/patching/#gemini-vertex-ai-tool-callin","title":"Gemini Vertex AI Tool Callin","text":"<p>This method allows us to get structured output from Gemini via tool calling with the Vertex AI SDK.</p> <p>Note: Gemini Tool Calling is in preview and there are some limitations, you can learn more in the Vertex AI examples notebook.</p> <pre><code>import instructor\nfrom vertexai.generative_models import GenerativeModel  # type: ignore\nimport vertexai\n\nvertexai.init(project=\"vertexai-generative-models\")\n\n\nclient = instructor.from_vertexai(\n    client=GenerativeModel(\"gemini-1.5-pro-preview-0409\"),\n    mode=instructor.Mode.VERTEXAI_TOOLS,\n)\n</code></pre>"},{"location":"concepts/patching/#parallel-tool-calling","title":"Parallel Tool Calling","text":"<p>Parallel tool calling is also an option but you must set <code>response_model</code> to be <code>Iterable[Union[...]]</code> types since we expect an array of results. Check out Parallel Tool Calling for more information.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.Mode.PARALLEL_TOOLS)\n</code></pre>"},{"location":"concepts/patching/#function-calling","title":"Function Calling","text":"<p>Note that function calling is soon to be deprecated in favor of TOOL mode for OpenAI. But will still be supported for other clients.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.Mode.TOOLS)\n</code></pre>"},{"location":"concepts/patching/#json-mode","title":"JSON Mode","text":"<p>JSON mode uses OpenAI's JSON format for responses by setting <code>response_format={\"type\": \"json_object\"}</code> in the <code>chat.completions.create</code> method.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.Mode.JSON)\n</code></pre> <p>JSON mode is also required for the Gemini Models via OpenAI's SDK.</p> <pre><code>pip install google-auth\n</code></pre> <pre><code>import google.auth\nimport google.auth.transport.requests\nimport instructor\nfrom openai import OpenAI\n\ncreds, project = google.auth.default()\nauth_req = google.auth.transport.requests.Request()\ncreds.refresh(auth_req)\n\n# Pass the Vertex endpoint and authentication to the OpenAI SDK\nPROJECT = 'PROJECT_ID'\nLOCATION = 'LOCATION'\n\nbase_url = f'https://{LOCATION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT}/locations/{LOCATION}/endpoints/openapi'\nclient = instructor.from_openai(\n    OpenAI(base_url=base_url, api_key=creds.token), mode=instructor.Mode.JSON\n)\n</code></pre>"},{"location":"concepts/patching/#gemini-json-mode","title":"Gemini JSON Mode","text":"<p>This mode uses Gemini's response mimetype field to generate a response in JSON format using the schema provided.</p> <pre><code>import instructor\nimport google.generativeai as genai\n\nclient = instructor.from_gemini(\n    genai.GenerativeModel(), mode=instructor.Mode.GEMINI_JSON\n)\n</code></pre>"},{"location":"concepts/patching/#markdown-json-mode","title":"Markdown JSON Mode","text":"<p>This just asks for the response in JSON format, but it is not recommended, and may not be supported in the future, this is just left to support vision models and models provided by Databricks and will not give you the full benefits of instructor.</p> <p>Experimental</p> <p>This is not recommended, and may not be supported in the future, this is just left to support vision models and models provided by Databricks.</p> <p>General syntax:</p> <pre><code>import instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI(), mode=instructor.Mode.MD_JSON)\n</code></pre> <p>Databricks syntax:</p> <pre><code>import instructor\nimport os\nfrom openai import OpenAI\n\nDATABRICKS_TOKEN = os.environ.get(\"DATABRICKS_TOKEN\", \"\")\nDATABRICKS_HOST = os.environ.get(\"DATABRICKS_HOST\", \"\")\n\n# Assuming Databricks environment variables are set\nclient = instructor.from_openai(\n    OpenAI(\n        api_key=DATABRICKS_TOKEN,\n        base_url=f\"{DATABRICKS_HOST}/serving-endpoints\",\n    ),\n    mode=instructor.Mode.MD_JSON,\n)\n</code></pre>"},{"location":"concepts/philosophy/","title":"Philosophy","text":"<p>The instructor values simplicity and flexibility in leveraging language models (LLMs). It offers a streamlined approach for structured output, avoiding unnecessary dependencies or complex abstractions. Let Pydantic do the heavy lifting.</p> <p>\u201cSimplicity is a great virtue but it requires hard work to achieve it and education to appreciate it. And to make matters worse: complexity sells better.\u201d \u2014 Edsger Dijkstra</p>"},{"location":"concepts/philosophy/#proof-that-its-simple","title":"Proof that its simple","text":"<ol> <li>Most users will only need to learn <code>response_model</code> and <code>patch</code> to get started.</li> <li>No new prompting language to learn, no new abstractions to learn.</li> </ol>"},{"location":"concepts/philosophy/#proof-that-its-transparent","title":"Proof that its transparent","text":"<ol> <li>We write very little prompts, and we don't try to hide the prompts from you.</li> <li>We'll do better in the future to give you config over the 2 prompts we do write, Reasking and JSON_MODE prompts.</li> </ol>"},{"location":"concepts/philosophy/#proof-that-its-flexible","title":"Proof that its flexible","text":"<ol> <li>If you build a system with OpenAI directly, it is easy to incrementally adopt instructor.</li> <li>Add <code>response_model</code> and if you want to revert, just remove it.</li> </ol>"},{"location":"concepts/philosophy/#the-zen-of-instructor","title":"The zen of <code>instructor</code>","text":"<p>Maintain the flexibility and power of Python, without unnecessary constraints.</p> <p>Begin with a function and a return type hint \u2013 simplicity is key. With my experience maintaining a large enterprise framework at my previous job over many years I've learned that the goal of making a useful framework is minimizing regret, both for the author and hopefully for the user.</p> <ol> <li>Define a Schema <code>class StructuredData(BaseModel):</code></li> <li>Define validators and methods on your schema.</li> <li>Encapsulate all your LLM logic into a function <code>def extract(a) -&gt; StructuredData:</code></li> <li>Define typed computations against your data with <code>def compute(data: StructuredData):</code> or call methods on your schema <code>data.compute()</code></li> </ol> <p>It should be that simple.</p>"},{"location":"concepts/philosophy/#my-goals","title":"My Goals","text":"<p>The goal for the library, documentation, and blog, is to help you be a better python programmer and as a result a better AI engineer.</p> <ul> <li>The library is a result of my desire for simplicity.</li> <li>The library should help maintain simplicity in your codebase.</li> <li>I won't try to write prompts for you,</li> <li>I don't try to create indirections or abstractions that make it hard to debug in the future</li> </ul> <p>Please note that the library is designed to be adaptable and open-ended, allowing you to customize and extend its functionality based on your specific requirements. If you have any further questions or ideas hit me up on twitter</p> <p>Cheers!</p>"},{"location":"concepts/prompting/","title":"General Tips for Prompt Engineering","text":"<p>The overarching theme of using Instructor and Pydantic for function calling is to make the models as self-descriptive, modular, and flexible as possible, while maintaining data integrity and ease of use.</p> <ul> <li>Modularity: Design self-contained components for reuse.</li> <li>Self-Description: Use Pydantic's <code>Field</code> for clear field descriptions.</li> <li>Optionality: Use Python's <code>Optional</code> type for nullable fields and set sensible defaults.</li> <li>Standardization: Employ enumerations for fields with a fixed set of values; include a fallback option.</li> <li>Dynamic Data: Use key-value pairs for arbitrary properties and limit list lengths.</li> <li>Entity Relationships: Define explicit identifiers and relationship fields.</li> <li>Contextual Logic: Optionally add a \"chain of thought\" field in reusable components for extra context.</li> </ul>"},{"location":"concepts/prompting/#modular-chain-of-thought","title":"Modular Chain of Thought","text":"<p>This approach to \"chain of thought\" improves data quality but can have modular components rather than global CoT.</p> <pre><code>from pydantic import BaseModel, Field\n\n\nclass Role(BaseModel):\n    chain_of_thought: str = Field(\n        ..., description=\"Think step by step to determine the correct title\"\n    )\n    title: str\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role\n</code></pre>"},{"location":"concepts/prompting/#utilize-optional-attributes","title":"Utilize Optional Attributes","text":"<p>Use Python's Optional type and set a default value to prevent undesired defaults like empty strings.</p> <pre><code>from typing import Optional\nfrom pydantic import BaseModel, Field\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n</code></pre>"},{"location":"concepts/prompting/#handling-errors-within-function-calls","title":"Handling Errors Within Function Calls","text":"<p>You can create a wrapper class to hold either the result of an operation or an error message. This allows you to remain within a function call even if an error occurs, facilitating better error handling without breaking the code flow.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import Optional\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Optional[str] = Field(default=None)\n\n\nclass MaybeUser(BaseModel):\n    result: Optional[UserDetail] = Field(default=None)\n    error: bool = Field(default=False)\n    message: Optional[str]\n\n    def __bool__(self):\n        return self.result is not None\n</code></pre> <p>With the <code>MaybeUser</code> class, you can either receive a <code>UserDetail</code> object in result or get an error message in message.</p>"},{"location":"concepts/prompting/#simplification-with-the-maybe-pattern","title":"Simplification with the Maybe Pattern","text":"<p>You can further simplify this using instructor to create the <code>Maybe</code> pattern dynamically from any <code>BaseModel</code>.</p> <pre><code>import instructor\nfrom pydantic import BaseModel\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n\n\nMaybeUser = instructor.Maybe(UserDetail)\n</code></pre> <p>This allows you to quickly create a Maybe type for any class, streamlining the process.</p>"},{"location":"concepts/prompting/#tips-for-enumerations","title":"Tips for Enumerations","text":"<p>To prevent data misalignment, use Enums for standardized fields. Always include an \"Other\" option as a fallback so the model can signal uncertainty.</p> <pre><code>from enum import Enum, auto\nfrom pydantic import BaseModel, Field\n\n\nclass Role(Enum):\n    PRINCIPAL = auto()\n    TEACHER = auto()\n    STUDENT = auto()\n    OTHER = auto()\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role = Field(\n        description=\"Correctly assign one of the predefined roles to the user.\"\n    )\n</code></pre> <p>If you're having a hard time with <code>Enum</code> an alternative is to use <code>Literal</code></p> <pre><code>from typing import Literal\nfrom pydantic import BaseModel\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Literal[\"PRINCIPAL\", \"TEACHER\", \"STUDENT\", \"OTHER\"]\n</code></pre> <p>If you'd like to improve performance more you can reiterate the requirements in the field descriptions or in the docstrings.</p>"},{"location":"concepts/prompting/#reiterate-long-instructions","title":"Reiterate Long Instructions","text":"<p>For complex attributes, it helps to reiterate the instructions in the field's description.</p> <pre><code>from pydantic import BaseModel, Field\n\n\nclass Role(BaseModel):\n    \"\"\"\n    Extract the role based on the following rules ...\n    \"\"\"\n\n    instructions: str = Field(\n        ...,\n        description=\"Restate the instructions and rules to correctly determine the title.\",\n    )\n    title: str\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    role: Role\n</code></pre>"},{"location":"concepts/prompting/#handle-arbitrary-properties","title":"Handle Arbitrary Properties","text":"<p>When you need to extract undefined attributes, use a list of key-value pairs.</p> <pre><code>from typing import List\nfrom pydantic import BaseModel, Field\n\n\nclass Property(BaseModel):\n    key: str\n    value: str\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Property] = Field(\n        ..., description=\"Extract any other properties that might be relevant.\"\n    )\n</code></pre>"},{"location":"concepts/prompting/#limiting-the-length-of-lists","title":"Limiting the Length of Lists","text":"<p>When dealing with lists of attributes, especially arbitrary properties, it's crucial to manage the length. You can use prompting and enumeration to limit the list length, ensuring a manageable set of properties.</p> <pre><code>from typing import List\nfrom pydantic import BaseModel, Field\n\n\nclass Property(BaseModel):\n    index: str = Field(..., description=\"Monotonically increasing ID\")\n    key: str\n    value: str\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Property] = Field(\n        ...,\n        description=\"Numbered list of arbitrary extracted properties, should be less than 6\",\n    )\n</code></pre> <p>Using Tuples for Simple Types</p> <p>For simple types, tuples can be a more compact alternative to custom classes, especially when the properties don't require additional descriptions.</p> <pre><code>from typing import List, Tuple\nfrom pydantic import BaseModel, Field\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: str\n    properties: List[Tuple[int, str]] = Field(\n        ...,\n        description=\"Numbered list of arbitrary extracted properties, should be less than 6\",\n    )\n</code></pre>"},{"location":"concepts/prompting/#advanced-arbitrary-properties","title":"Advanced Arbitrary Properties","text":"<p>For multiple users, aim to use consistent key names when extracting properties.</p> <pre><code>from typing import List\nfrom pydantic import BaseModel\n\n\nclass UserDetail(BaseModel):\n    id: int\n    age: int\n    name: str\n\n\nclass UserDetails(BaseModel):\n    \"\"\"\n    Extract information for multiple users.\n    Use consistent key names for properties across users.\n    \"\"\"\n\n    users: List[UserDetail]\n</code></pre> <p>This refined guide should offer a cleaner and more organized approach to structure engineering in Python.</p>"},{"location":"concepts/prompting/#defining-relationships-between-entities","title":"Defining Relationships Between Entities","text":"<p>In cases where relationships exist between entities, it's vital to define them explicitly in the model. The following example demonstrates how to define relationships between users by incorporating an id and a friends field:</p> <pre><code>from typing import List\nfrom pydantic import BaseModel, Field\n\n\nclass UserDetail(BaseModel):\n    id: int = Field(..., description=\"Unique identifier for each user.\")\n    age: int\n    name: str\n    friends: List[int] = Field(\n        ...,\n        description=\"Correct and complete list of friend IDs, representing relationships between users.\",\n    )\n\n\nclass UserRelationships(BaseModel):\n    users: List[UserDetail] = Field(\n        ...,\n        description=\"Collection of users, correctly capturing the relationships among them.\",\n    )\n</code></pre>"},{"location":"concepts/prompting/#reusing-components-with-different-contexts","title":"Reusing Components with Different Contexts","text":"<p>You can reuse the same component for different contexts within a model. In this example, the TimeRange component is used for both work_time and leisure_time.</p> <pre><code>from pydantic import BaseModel, Field\n\n\nclass TimeRange(BaseModel):\n    start_time: int = Field(..., description=\"The start time in hours.\")\n    end_time: int = Field(..., description=\"The end time in hours.\")\n\n\nclass UserDetail(BaseModel):\n    id: int = Field(..., description=\"Unique identifier for each user.\")\n    age: int\n    name: str\n    work_time: TimeRange = Field(\n        ..., description=\"Time range during which the user is working.\"\n    )\n    leisure_time: TimeRange = Field(\n        ..., description=\"Time range reserved for leisure activities.\"\n    )\n</code></pre> <p>Sometimes, a component like TimeRange may require some context or additional logic to be used effectively. Employing a \"chain of thought\" field within the component can help in understanding or optimizing the time range allocations.</p> <pre><code>from pydantic import BaseModel, Field\n\n\nclass TimeRange(BaseModel):\n    chain_of_thought: str = Field(\n        ..., description=\"Step by step reasoning to get the correct time range\"\n    )\n    start_time: int = Field(..., description=\"The start time in hours.\")\n    end_time: int = Field(..., description=\"The end time in hours.\")\n</code></pre>"},{"location":"concepts/raw_response/","title":"Creating a model with completions","text":"<p>In instructor&gt;1.0.0 we have a custom client, if you wish to use the raw response you can do the following</p> <pre><code>import instructor\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n\nuser, completion = client.chat.completions.create_with_completion(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserExtract,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nprint(user)\n#&gt; name='Jason' age=25\n\nprint(completion)\n\"\"\"\nChatCompletion(\n    id='chatcmpl-9TzoQBCR4ljqayllBj0U1PKfjGqiL',\n    choices=[\n        Choice(\n            finish_reason='stop',\n            index=0,\n            logprobs=None,\n            message=ChatCompletionMessage(\n                content=None,\n                role='assistant',\n                function_call=None,\n                tool_calls=[\n                    ChatCompletionMessageToolCall(\n                        id='call_Q5iev31AA2vUVoAdpBapGWK5',\n                        function=Function(\n                            arguments='{\"name\":\"Jason\",\"age\":25}', name='UserExtract'\n                        ),\n                        type='function',\n                    )\n                ],\n            ),\n        )\n    ],\n    created=1716936146,\n    model='gpt-3.5-turbo-0125',\n    object='chat.completion',\n    system_fingerprint=None,\n    usage=CompletionUsage(completion_tokens=9, prompt_tokens=82, total_tokens=91),\n)\n\"\"\"\n</code></pre>"},{"location":"concepts/reask_validation/","title":"Validation and Reasking","text":"<p>Instead of framing \"self-critique\" or \"self-reflection\" in AI as new concepts, we can view them as validation errors with clear error messages that the system can use to self-correct.</p>"},{"location":"concepts/reask_validation/#pydantic","title":"Pydantic","text":"<p>Pydantic offers an customizable and expressive validation framework for Python. Instructor leverages Pydantic's validation framework to provide a uniform developer experience for both code-based and LLM-based validation, as well as a reasking mechanism for correcting LLM outputs based on validation errors. To learn more check out the Pydantic docs on validators.</p> <p>Good llm validation is just good validation</p> <p>If you want to see some more examples on validators checkout our blog post Good LLM validation is just good validation</p>"},{"location":"concepts/reask_validation/#code-based-validation-example","title":"Code-based Validation Example","text":"<p>First define a Pydantic model with a validator using the <code>Annotation</code> class from <code>typing_extensions</code>.</p> <p>Enforce a naming rule using Pydantic's built-in validation:</p> <pre><code>from pydantic import BaseModel, ValidationError\nfrom typing_extensions import Annotated\nfrom pydantic import AfterValidator\n\n\ndef name_must_contain_space(v: str) -&gt; str:\n    if \" \" not in v:\n        raise ValueError(\"Name must contain a space.\")\n    return v.lower()\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: Annotated[str, AfterValidator(name_must_contain_space)]\n\n\ntry:\n    person = UserDetail(age=29, name=\"Jason\")\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserDetail\n    name\n      Value error, Name must contain a space. [type=value_error, input_value='Jason', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.7/v/value_error\n    \"\"\"\n</code></pre>"},{"location":"concepts/reask_validation/#output-for-code-based-validation","title":"Output for Code-Based Validation","text":"<pre><code>1 validation error for UserDetail\nname\n   Value error, name must contain a space (type=value_error)\n</code></pre> <p>As we can see, Pydantic raises a validation error when the name attribute does not contain a space. This is a simple example, but it demonstrates how Pydantic can be used to validate attributes of a model.</p>"},{"location":"concepts/reask_validation/#llm-based-validation-example","title":"LLM-Based Validation Example","text":"<p>LLM-based validation can also be plugged into the same Pydantic model. Here, if the answer attribute contains content that violates the rule \"don't say objectionable things,\" Pydantic will raise a validation error.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom instructor import llm_validator\nfrom pydantic import BaseModel, ValidationError, BeforeValidator\nfrom typing_extensions import Annotated\n\n\n# Apply the patch to the OpenAI client\nclient = instructor.from_openai(OpenAI())\n\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: Annotated[\n        str,\n        BeforeValidator(llm_validator(\"don't say objectionable things\", client=client)),\n    ]\n\n\ntry:\n    qa = QuestionAnswer(\n        question=\"What is the meaning of life?\",\n        answer=\"The meaning of life is to be evil and steal\",\n    )\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for QuestionAnswer\n    answer\n      Assertion failed, The statement promotes objectionable behavior by encouraging evil and stealing. [type=assertion_error, input_value='The meaning of life is to be evil and steal', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.7/v/assertion_error\n    \"\"\"\n</code></pre>"},{"location":"concepts/reask_validation/#output-for-llm-based-validation","title":"Output for LLM-Based Validation","text":"<p>It is important to note here that the error message is generated by the LLM, not the code, so it'll be helpful for re-asking the model.</p> <pre><code>1 validation error for QuestionAnswer\nanswer\n   Assertion failed, The statement is objectionable. (type=assertion_error)\n</code></pre>"},{"location":"concepts/reask_validation/#using-reasking-logic-to-correct-outputs","title":"Using Reasking Logic to Correct Outputs","text":"<p>Validators are a great tool for ensuring some property of the outputs. When you use the <code>patch()</code> method with the <code>openai</code> client, you can use the <code>max_retries</code> parameter to set the number of times you can reask the model to correct the output.</p> <p>It is a great layer of defense against bad outputs of two forms:</p> <ol> <li>Pydantic Validation Errors (code or llm based)</li> <li>JSON Decoding Errors (when the model returns a bad response)</li> </ol>"},{"location":"concepts/reask_validation/#step-1-define-the-response-model-with-validators","title":"Step 1: Define the Response Model with Validators","text":"<p>Notice that the field validator wants the name in uppercase, but the user input is lowercase. The validator will raise a <code>ValueError</code> if the name is not in uppercase.</p> <pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel, field_validator\n\n# Apply the patch to the OpenAI client\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"name\")\n    @classmethod\n    def validate_name(cls, v):\n        if v.upper() != v:\n            raise ValueError(\"Name must be in uppercase.\")\n        return v\n</code></pre>"},{"location":"concepts/reask_validation/#step-2-using-the-client-with-retries","title":"Step 2. Using the Client with Retries","text":"<p>Here, the <code>UserDetails</code> model is passed as the <code>response_model</code>, and <code>max_retries</code> is set to 2.</p> <pre><code>import instructor\nimport openai\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI(), mode=instructor.Mode.TOOLS)\n\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\n\nmodel = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserDetails,\n    max_retries=2,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nprint(model.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"Jason\",\n  \"age\": 25\n}\n\"\"\"\n</code></pre>"},{"location":"concepts/reask_validation/#what-happens-behind-the-scenes","title":"What happens behind the scenes?","text":"<p>Behind the scenes, the <code>instructor.from_openai()</code> method adds a <code>max_retries</code> parameter to the <code>openai.ChatCompletion.create()</code> method. The <code>max_retries</code> parameter will trigger up to 2 reattempts if the <code>name</code> attribute fails the uppercase validation in <code>UserDetails</code>.</p> <pre><code>from pydantic import ValidationError\n\n\ntry:\n    ...\nexcept ValidationError as e:\n    kwargs[\"messages\"].append(response.choices[0].message)\n    kwargs[\"messages\"].append(\n        {\n            \"role\": \"user\",\n            \"content\": f\"Please correct the function call; errors encountered:\\n{e}\",\n        }\n    )\n</code></pre>"},{"location":"concepts/reask_validation/#advanced-validation-techniques","title":"Advanced Validation Techniques","text":"<p>The docs are currently incomplete, but we have a few advanced validation techniques that we're working on documenting better such as model level validation, and using a validation context. Check out our example on verifying citations which covers:</p> <ol> <li>Validate the entire object with all attributes rather than one attribute at a time</li> <li>Using some 'context' to validate the object: In this case, we use the <code>context</code> to check if the citation existed in the original text.</li> </ol>"},{"location":"concepts/reask_validation/#optimizing-token-usage","title":"Optimizing Token usage","text":"<p>Pydantic automatically includes a URL within the error message itself when an error is thrown so that users can learn more about the specific error that was thrown. Some users might want to remove this URL since it adds extra tokens that otherwise might not add much value to the validation process.</p> <p>We've created a small helper function that you can use below which removes this url in the error message</p> <pre><code>from instructor.utils import disable_pydantic_error_url\nfrom pydantic import BaseModel, ValidationError\nfrom typing_extensions import Annotated\nfrom pydantic import AfterValidator\n\ndisable_pydantic_error_url()  # (1)!\n\n\ndef name_must_contain_space(v: str) -&gt; str:\n    if \" \" not in v:\n        raise ValueError(\"Name must contain a space.\")\n    return v.lower()\n\n\nclass UserDetail(BaseModel):\n    age: int\n    name: Annotated[str, AfterValidator(name_must_contain_space)]\n\n\ntry:\n    person = UserDetail(age=29, name=\"Jason\")\nexcept ValidationError as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserDetail\n    name\n        Value error, Name must contain a space. [type=value_error, input_value='Jason', input_type=str]\n    \"\"\"\n</code></pre> <ol> <li>We disable the error by setting an environment variable <code>PYDANTIC_ERRORS_INCLUDE_URL</code> to <code>0</code>. This is valid only for the duration that the script is executed for, once the function is not called, the original behaviour is restored.</li> </ol>"},{"location":"concepts/reask_validation/#takeaways","title":"Takeaways","text":"<p>By integrating these advanced validation techniques, we not only improve the quality and reliability of LLM-generated content, but also pave the way for more autonomous and effective systems.</p>"},{"location":"concepts/retrying/","title":"Retrying","text":"<p>One of the benefits of having Pydantic is the ease with which we can define validators. We cover this topic in many articles, like Reasking Validation and in our blog post Good LLM validation is just good validation.</p> <p>This post will mostly describe how to use simple and more complex retry and logic.</p>"},{"location":"concepts/retrying/#example-of-a-validator","title":"Example of a Validator","text":"<p>Before we begin, we'll use a simple example of a validator. One that checks that the name is in all caps. While we could obviously prompt that we want the name in all caps, this serves as an example of how we can build in additional logic without changing our prompts.</p> <p>To use simple retry, we just need to set `max_retries`` as an integer. In this example.</p> <pre><code>from typing import Annotated\nfrom pydantic import AfterValidator, BaseModel\n\n\ndef uppercase_validator(v):\n    if v.islower():\n        raise ValueError(\"Name must be ALL CAPS\")\n    return v\n\n\nclass UserDetail(BaseModel):\n    name: Annotated[str, AfterValidator(uppercase_validator)]\n    age: int\n\n\ntry:\n    UserDetail(name=\"jason\", age=12)\nexcept Exception as e:\n    print(e)\n    \"\"\"\n    1 validation error for UserDetail\n    name\n      Value error, Name must be ALL CAPS [type=value_error, input_value='jason', input_type=str]\n        For further information visit https://errors.pydantic.dev/2.7/v/value_error\n    \"\"\"\n</code></pre>"},{"location":"concepts/retrying/#simple-max-retries","title":"Simple: Max Retries","text":"<p>The simplest way of defining a retry is just defining the maximum number of retries.</p> <pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\nclient = instructor.from_openai(openai.OpenAI(), mode=instructor.Mode.TOOLS)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract `jason is 12`\"},\n    ],\n    max_retries=3,  # (1)!\n)\nprint(response.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"jason\",\n  \"age\": 12\n}\n\"\"\"\n# (2)!\n</code></pre> <ol> <li>We set the maximum number of retries to 3. This means that if the model returns an error, we'll reask the model up to 3 times.</li> <li>We assert that the name is in all caps.</li> </ol>"},{"location":"concepts/retrying/#catching-retry-exceptions","title":"Catching Retry Exceptions","text":"<p>If you want to catch the retry exceptions, you can do so and access the <code>last_completion</code>, <code>n_attempts</code> and <code>messages</code> attributes.</p> <pre><code>from pydantic import BaseModel, field_validator\nimport openai\nimport instructor\nfrom instructor.exceptions import InstructorRetryException\nfrom tenacity import Retrying, retry_if_not_exception_type, stop_after_attempt\n\n# Patch the OpenAI client to enable response_model\nclient = instructor.from_openai(openai.OpenAI())\n\n\n# Define a Pydantic model for the user details\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"age\")\n    def validate_age(cls, v: int):\n        raise ValueError(f\"You will never succeed with {str(v)}\")\n\n\nretries = Retrying(\n    retry=retry_if_not_exception_type(ZeroDivisionError), stop=stop_after_attempt(3)\n)\n# Use the client to create a user detail\ntry:\n    user = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserDetail,\n        messages=[{\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"}],\n        max_retries=retries,\n    )\nexcept InstructorRetryException as e:\n    print(e.messages[-1][\"content\"])  # type: ignore\n    \"\"\"\n    1 validation error for UserDetail\n    age\n    Value error, You will never succeed with 25 [type=value_error, input_value=25, input_type=int]\n        For further information visit https://errors.pydantic.dev/2.7/v/value_error\n    \"\"\"\n\n    print(e.n_attempts)\n    #&gt; 3\n\n    print(e.last_completion)\n    \"\"\"\n    ChatCompletion(id='chatcmpl-9FaHq4dL4SszLAbErGlpD3a0TYxi0', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_XidgLpIu1yfaq876L65k91RM', function=Function(arguments='{\"name\":\"Jason\",\"age\":25}', name='UserDetail'), type='function')]))], created=1713501434, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_d9767fc5b9', usage=CompletionUsage(completion_tokens=27, prompt_tokens=513, total_tokens=540))\n    \"\"\"\n</code></pre>"},{"location":"concepts/retrying/#advanced-retry-logic","title":"Advanced: Retry Logic","text":"<p>If you want more control over how we define retries such as back-offs and additional retry logic we can use a library called Tenacity. To learn more, check out the documentation on the Tenacity website.</p> <p>Rather than using the decorator <code>@retry</code>, we can use the <code>Retrying</code> and <code>AsyncRetrying</code> classes to define our own retry logic.</p> <pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel\nfrom tenacity import Retrying, stop_after_attempt, wait_fixed\n\nclient = instructor.from_openai(openai.OpenAI(), mode=instructor.Mode.TOOLS)\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract `jason is 12`\"},\n    ],\n    max_retries=Retrying(\n        stop=stop_after_attempt(2),  # (1)!\n        wait=wait_fixed(1),  # (2)!\n    ),  # (3)!\n)\nprint(response.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"jason\",\n  \"age\": 12\n}\n\"\"\"\n</code></pre> <ol> <li>We stop after 2 attempts</li> <li>We wait 1 second between each attempt</li> <li>We can now define our own retry logic</li> </ol>"},{"location":"concepts/retrying/#asynchronous-retries","title":"asynchronous retries","text":"<p>If you're using asynchronous code, you can use <code>AsyncRetrying</code> instead.</p> <pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel\nfrom tenacity import AsyncRetrying, stop_after_attempt, wait_fixed\n\nclient = instructor.from_openai(openai.AsyncOpenAI(), mode=instructor.Mode.TOOLS)\n\n\nclass UserDetail(BaseModel):\n    name: str\n    age: int\n\n\ntask = client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    response_model=UserDetail,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract `jason is 12`\"},\n    ],\n    max_retries=AsyncRetrying(\n        stop=stop_after_attempt(2),\n        wait=wait_fixed(1),\n    ),\n)\n\nimport asyncio\n\nresponse = asyncio.run(task)\nprint(response.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"jason\",\n  \"age\": 12\n}\n\"\"\"\n</code></pre>"},{"location":"concepts/retrying/#other-features-of-tenacity","title":"Other Features of Tenacity","text":"<p>Tenacity features a huge number of different retrying capabilities. A few of them are listed below.</p> <ul> <li><code>Retrying(stop=stop_after_attempt(2))</code>: Stop after 2 attempts</li> <li><code>Retrying(stop=stop_after_delay(10))</code>: Stop after 10 seconds</li> <li><code>Retrying(wait=wait_fixed(1))</code>: Wait 1 second between each attempt</li> <li><code>Retrying(wait=wait_random(0, 1))</code>: Wait a random amount of time between 0 and 1 seconds</li> <li><code>Retrying(wait=wait_exponential(multiplier=1, min=4, max=10))</code>: Wait an exponential amount of time between 4 and 10 seconds</li> <li><code>Retrying(wait=(stop_after_attempt(2) | stop_after_delay(10)))</code>: Stop after 2 attempts or 10 seconds</li> <li><code>Retrying(wait=(wait_fixed(1) + wait_random(0.2)))</code>: Wait at least 1 second and add up to 0.2 seconds</li> </ul> <p>Remember that for async clients you need to use <code>AsyncRetrying</code> instead of <code>Retrying</code>!</p>"},{"location":"concepts/retrying/#retry-callbacks","title":"Retry Callbacks","text":"<p>You can also define callbacks to be called before and after each attempt. This is useful for logging or debugging.</p> <pre><code>from pydantic import BaseModel, field_validator\nimport instructor\nimport tenacity\nimport openai\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"name\")\n    def name_is_uppercase(cls, v: str):\n        assert v.isupper(), \"Name must be uppercase\"\n        return v\n\n\nresp = client.messages.create(\n    model=\"gpt-3.5-turbo\",\n    max_tokens=1024,\n    max_retries=tenacity.Retrying(\n        stop=tenacity.stop_after_attempt(3),\n        before=lambda _: print(\"before:\", _),\n        # \"\"\"\n        # before:\n        # &lt;RetryCallState 4682490016: attempt #1; slept for 0.0; last result: none yet&gt;\n        # \"\"\"\n        after=lambda _: print(\"after:\", _),\n    ),  # type: ignore\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract John is 18 years old.\",\n        }\n    ],\n    response_model=User,\n)\n\nassert isinstance(resp, User)\nassert resp.name == \"JOHN\"  # due to validation\nassert resp.age == 18\nprint(resp)\n#&gt; name='JOHN' age=18\n\n\"\"\"\nbefore: &lt;RetryCallState 4421908816: attempt #1; slept for 0.0; last result: none yet&gt;\nafter: &lt;RetryCallState 4421908816: attempt #1; slept for 0.0; last result: failed (ValidationError 1 validation error for User\nname\n  Assertion failed, Name must be uppercase [type=assertion_error, input_value='John', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.6/v/assertion_error)&gt;\n\nbefore: &lt;RetryCallState 4421908816: attempt #2; slept for 0.0; last result: none yet&gt;\nname='JOHN' age=18\n\"\"\"\n</code></pre>"},{"location":"concepts/typeadapter/","title":"Type Adapter","text":"<p>This page is a work in progress</p> <p>This page is a work in progress. Check out Pydantic's documentation</p>"},{"location":"concepts/typeddicts/","title":"TypedDicts","text":"<p>We also support typed dicts.</p> <pre><code>from typing_extensions import TypedDict\nfrom openai import OpenAI\nimport instructor\n\n\nclass User(TypedDict):\n    name: str\n    age: int\n\n\nclient = instructor.from_openai(OpenAI())\n\n\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=User,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Timothy is a man from New York who is turning 32 this year\",\n        }\n    ],\n)\n</code></pre>"},{"location":"concepts/types/","title":"Support for Simple Types","text":"<p>Aside from the recommended <code>pydantic.BaseModel</code>, and Iterable, and Partial,</p> <p>Instructor supports simple types like <code>str</code>, <code>int</code>, <code>float</code>, <code>bool</code>, <code>Union</code>, <code>Literal</code>, out of the box. You can use these types directly in your response models.</p> <p>To add more descriptions you can also use <code>typing.Annotated</code> to include more information about the type.</p>"},{"location":"concepts/types/#what-happens-behind-the-scenes","title":"What happens behind the scenes?","text":"<p>We will actually wrap the response model with a <code>pydantic.BaseModel</code> of the following form:</p> <pre><code>from typing import Annotated\nfrom pydantic import create_model, Field, BaseModel\n\ntypehint = Annotated[bool, Field(description=\"Sample Description\")]\n\nmodel = create_model(\"Response\", content=(typehint, ...), __base__=BaseModel)\n\nprint(model.model_json_schema())\n\"\"\"\n{\n    'properties': {\n        'content': {\n            'description': 'Sample Description',\n            'title': 'Content',\n            'type': 'boolean',\n        }\n    },\n    'required': ['content'],\n    'title': 'Response',\n    'type': 'object',\n}\n\"\"\"\n</code></pre>"},{"location":"concepts/types/#primitive-types-str-int-float-bool","title":"Primitive Types (str, int, float, bool)","text":"<pre><code>import instructor\nimport openai\n\nclient = instructor.from_openai(openai.OpenAI())\n\n# Response model with simple types like str, int, float, bool\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=bool,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Is it true that Paris is the capital of France?\",\n        },\n    ],\n)\nassert resp is True, \"Paris is the capital of France\"\nprint(resp)\n#&gt; True\n</code></pre>"},{"location":"concepts/types/#annotated","title":"Annotated","text":"<p>Annotations can be used to add more information about the type. This can be useful for adding descriptions to the type, along with more complex information like field names, and more.</p> <pre><code>import instructor\nimport openai\nfrom typing import Annotated\nfrom pydantic import Field\n\nclient = instructor.from_openai(openai.OpenAI())\n\nUpperCaseStr = Annotated[str, Field(description=\"string must be upper case\")]\n\n# Response model with simple types like str, int, float, bool\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=UpperCaseStr,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the capital of france?\",\n        },\n    ],\n)\nassert resp == \"PARIS\", \"Paris is the capital of France\"\nprint(resp)\n#&gt; PARIS\n</code></pre>"},{"location":"concepts/types/#literal","title":"Literal","text":"<p>When doing simple classification Literals go quite well, they support literal of string, int, bool.</p> <pre><code>import instructor\nimport openai\nfrom typing import Literal\n\nclient = instructor.from_openai(openai.OpenAI())\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Literal[\"BILLING\", \"SHIPPING\"],\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Classify the following messages: 'I am having trouble with my billing'\",\n        },\n    ],\n)\nassert resp == \"BILLING\"\nprint(resp)\n#&gt; BILLING\n</code></pre>"},{"location":"concepts/types/#enum","title":"Enum","text":"<p>Enums are harder to get right without some addition promping but are useful if these are values that are shared across the application.</p> <pre><code>import instructor\nimport openai\nfrom enum import Enum\n\n\nclass Label(str, Enum):\n    BILLING = \"BILLING\"\n    SHIPPING = \"SHIPPING\"\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Label,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Classify the following messages: 'I am having trouble with my billing'\",\n        },\n    ],\n)\nassert resp == Label.BILLING\nprint(resp)\n#&gt; BILLING\n</code></pre>"},{"location":"concepts/types/#list","title":"List","text":"<pre><code>import instructor\nimport openai\nfrom typing import List\n\nclient = instructor.from_openai(openai.OpenAI())\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=List[int],\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Give me the first 5 prime numbers\",\n        },\n    ],\n)\n\nassert resp == [2, 3, 5, 7, 11]\nprint(resp)\n#&gt; [2, 3, 5, 7, 11]\n</code></pre>"},{"location":"concepts/types/#union","title":"Union","text":"<p>Union is a great way to handle multiple types of responses, similar to multiple function calls but not limited to the function calling api, like in JSON_SCHEMA modes.</p> <pre><code>import instructor\nimport openai\nfrom pydantic import BaseModel\nfrom typing import Union\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass Add(BaseModel):\n    a: int\n    b: int\n\n\nclass Weather(BaseModel):\n    location: str\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Union[Add, Weather],\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"What is 5 + 5?\",\n        },\n    ],\n)\n\nassert resp == Add(a=5, b=5)\nprint(resp)\n#&gt; a=5 b=5\n</code></pre>"},{"location":"concepts/types/#complex-types","title":"Complex Types","text":""},{"location":"concepts/types/#pandas-dataframe","title":"Pandas DataFrame","text":"<p>This is a more complex example, where we use a custom type to convert markdown to a pandas DataFrame.</p> <pre><code>from io import StringIO\nfrom typing import Annotated, Any\nfrom pydantic import BeforeValidator, PlainSerializer, InstanceOf, WithJsonSchema\nimport pandas as pd\nimport instructor\nimport openai\n\n\ndef md_to_df(data: Any) -&gt; Any:\n    # Convert markdown to DataFrame\n    if isinstance(data, str):\n        return (\n            pd.read_csv(\n                StringIO(data),  # Process data\n                sep=\"|\",\n                index_col=1,\n            )\n            .dropna(axis=1, how=\"all\")\n            .iloc[1:]\n            .applymap(lambda x: x.strip())\n        )\n    return data\n\n\nMarkdownDataFrame = Annotated[\n    # Validates final type\n    InstanceOf[pd.DataFrame],\n    # Converts markdown to DataFrame\n    BeforeValidator(md_to_df),\n    # Converts DataFrame to markdown on model_dump_json\n    PlainSerializer(lambda df: df.to_markdown()),\n    # Adds a description to the type\n    WithJsonSchema(\n        {\n            \"type\": \"string\",\n            \"description\": \"\"\"\n            The markdown representation of the table,\n            each one should be tidy, do not try to join\n            tables that should be seperate\"\"\",\n        }\n    ),\n]\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=MarkdownDataFrame,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Jason is 20, Sarah is 30, and John is 40\",\n        },\n    ],\n)\n\nassert isinstance(resp, pd.DataFrame)\nprint(resp)\n\"\"\"\n        Age\n Name\nJason     20\nSarah     30\nJohn      40\n\"\"\"\n</code></pre>"},{"location":"concepts/types/#lists-of-unions","title":"Lists of Unions","text":"<p>Just like Unions we can use List of Unions to represent multiple types of responses. This will feel similar to the parallel function calls but not limited to the function calling api, like in JSON_SCHEMA modes.</p> <pre><code>import instructor\nimport openai\nfrom pydantic import BaseModel\nfrom typing import Union, List\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass Weather(BaseModel, frozen=True):\n    location: str\n\n\nclass Add(BaseModel, frozen=True):\n    a: int\n    b: int\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=List[Union[Add, Weather]],\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Add 5 and 5, and also whats the weather in Toronto?\",\n        },\n    ],\n)\n\nassert resp == [Add(a=5, b=5), Weather(location=\"Toronto\")]\nprint(resp)\n#&gt; [Add(a=5, b=5), Weather(location='Toronto')]\n</code></pre>"},{"location":"concepts/union/","title":"Union","text":"<p>Pydantic models also support <code>Union</code> types, which are used to represent a value that can be one of several types.</p> <p>While many libraries support multiple function calls, and tool calls support multiple returns, the goal is to provide only one way to do things.</p>"},{"location":"concepts/union/#unions-for-multiple-types","title":"Unions for Multiple Types","text":"<p>You can use <code>Union</code> types to write agents that can dynamically choose actions - by choosing an output class. For example, in a search and lookup function, the LLM can determine whether to execute another search, lookup or other action.</p> <pre><code>from pydantic import BaseModel\nfrom typing import Union\n\n\nclass Search(BaseModel):\n    query: str\n\n    def execute(self):\n        return ...\n\n\nclass Lookup(BaseModel):\n    key: str\n\n    def execute(self):\n        return ...\n\n\nclass Action(BaseModel):\n    action: Union[Search, Lookup]\n\n    def execute(self):\n        return self.action.execute()\n</code></pre> <p>See 'examples/union/run.py' for a working example.</p>"},{"location":"concepts/usage/","title":"Usage Tokens","text":"<p>The easiest way to get usage for non streaming requests is to access the raw response.</p> <pre><code>import instructor\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n\nuser, completion = client.chat.completions.create_with_completion(\n    model=\"gpt-3.5-turbo\",\n    response_model=UserExtract,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nprint(completion.usage)\n#&gt; CompletionUsage(completion_tokens=9, prompt_tokens=82, total_tokens=91)\n</code></pre> <p>You can catch an IncompleteOutputException whenever the context length is exceeded and react accordingly, such as by trimming your prompt by the number of exceeding tokens.</p> <pre><code>from instructor.exceptions import IncompleteOutputException\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n\ntry:\n    client.chat.completions.create_with_completion(\n        model=\"gpt-3.5-turbo\",\n        response_model=UserExtract,\n        messages=[\n            {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n        ],\n    )\nexcept IncompleteOutputException as e:\n    token_count = e.last_completion.usage.total_tokens  # type: ignore\n    # your logic here\n</code></pre>"},{"location":"examples/","title":"Cookbooks: Leveraging Structured Outputs","text":""},{"location":"examples/#quick-links","title":"Quick Links","text":"<ol> <li>How are single and multi-label classifications done using enums?</li> <li>How is AI self-assessment implemented with <code>llm_validator</code>?</li> <li>How to do classification in batch from user provided classes.</li> <li>How are exact citations retrieved using regular expressions and smart prompting?</li> <li>How are search queries segmented through function calling and multi-task definitions?</li> <li>How are knowledge graphs generated from questions?</li> <li>How are complex queries decomposed into subqueries in a single request?</li> <li>How are entities extracted and resolved from documents?</li> <li>How is Personally Identifiable Information sanitized from documents?</li> <li>How are action items and dependencies generated from transcripts?</li> <li>How to enable OpenAI's moderation</li> <li>How to extract tables using GPT-Vision?</li> <li>How to generate advertising copy from image inputs</li> <li>How to use local models from Ollama</li> <li>How to store responses in a database with SQLModel</li> <li>How to use groqcloud api</li> <li>How to do document segmentation using LLMs?</li> <li>How to save 50% of API costs with OpenAI's Batch API using Instructor</li> </ol> <p>Explore more!</p>"},{"location":"examples/batch_job_oai/","title":"Bulk Generation of Synthetic Data","text":"<p>This tutorial shows how to use <code>instructor</code> to generate large quantities of synthetic data at scale using Open AI's new Batch API. In this example, we'll be generating synthetic questions using the <code>ms-marco</code> dataset to evaluate RAG retrieval.</p> Why use the batch API? <p>There are a few reasons why you might want to use the Batch API</p> <ol> <li> <p>Batch Jobs are 50% cheaper than running an inference job on demand ( see Open AI's pricing page here )</p> </li> <li> <p>Batch Jobs have higher rate limits than normal api calls</p> </li> <li> <p>Batch Jobs support both normal models and fine-tuned models</p> </li> </ol> <p>This makes them perfect for non time-sensitive tasks that involve large quantities of data.</p>"},{"location":"examples/batch_job_oai/#getting-started","title":"Getting Started","text":"<p>Let's first see how we can generate a Question and Answer Pair using Instructor with a normal OpenAI function call.</p> <pre><code>from pydantic import BaseModel, Field\nfrom openai import OpenAI\nfrom instructor import from_openai\n\nclient = from_openai(OpenAI())\n\n\nclass QuestionAnswerPair(BaseModel):\n    \"\"\"\n    This model represents a pair of a question generated from a text chunk, its corresponding answer,\n    and the chain of thought leading to the answer. The chain of thought provides insight into how the answer\n    was derived from the question.\n    \"\"\"\n\n    chain_of_thought: str = Field(\n        description=\"The reasoning process leading to the answer.\"\n    )\n    question: str = Field(description=\"The generated question from the text chunk.\")\n    answer: str = Field(description=\"The answer to the generated question.\")\n\n\ndef generate_question(chunk: str) -&gt; QuestionAnswerPair:\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a world class AI that excels at generating hypothethical search queries. You're about to be given a text snippet and asked to generate a search query which is specific to the specific text chunk that you'll be given. Make sure to use information from the text chunk.\",\n            },\n            {\"role\": \"user\", \"content\": f\"Here is the text chunk: {chunk}\"},\n        ],\n        response_model=QuestionAnswerPair,\n    )\n\n\ntext_chunk = \"\"\"\nThe Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\n\"\"\"\nprint(generate_question(text_chunk).model_dump_json(indent=2))\n\"\"\"\n{\n  \"chain_of_thought\": \"The text mentions that the Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia\u2019s central bank and banknote issuing authority.\",\n  \"question\": \"When was the Reserve Bank of Australia (RBA) established?\",\n  \"answer\": \"14 January 1960\"\n}\n\"\"\"\n</code></pre> <p>As the number of chunks we'd like to generate these synthetic questions for increases, the cost will grow proportionally.</p> <p>Let's see how we can use the <code>BatchJob</code> object to create a <code>.jsonl</code> file which is compatible with the Batch API.</p> <pre><code>from datasets import load_dataset\nfrom instructor.batch import BatchJob\nfrom pydantic import BaseModel, Field\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"ms_marco\", \"v1.1\", split=\"train\", streaming=True).take(200)\n\n\ndef get_messages(dataset):  # (1)!\n    for row in dataset:\n        for passage in row['passages']['passage_text']:\n            yield [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a world class AI that excels at generating hypothethical search queries. You're about to be given a text snippet and asked to generate a search query which is specific to the specific text chunk that you'll be given. Make sure to use information from the text chunk.\",\n                },\n                {\"role\": \"user\", \"content\": f\"Here is the text chunk: {passage}\"},\n            ]\n\n\nclass QuestionAnswerPair(BaseModel):\n    \"\"\"\n    This model represents a pair of a question generated from a text chunk, its corresponding answer,\n    and the chain of thought leading to the answer. The chain of thought provides insight into how the answer\n    was derived from the question.\n    \"\"\"\n\n    chain_of_thought: str = Field(\n        description=\"The reasoning process leading to the answer.\"\n    )\n    question: str = Field(description=\"The generated question from the text chunk.\")\n    answer: str = Field(description=\"The answer to the generated question.\")\n\n\nBatchJob.create_from_messages(\n    messages_batch=get_messages(dataset),\n    model=\"gpt-4o\",\n    file_path=\"./test.jsonl\",\n    response_model=QuestionAnswerPair,\n)  # (2)!\n</code></pre> <ol> <li> <p>We first define a generator which generates a list of messages which we would have made in a normal <code>openai</code> api call</p> </li> <li> <p>We then use the <code>create_from_messages</code> class method to specify the model and response_model that we want. <code>instructor</code> will handle the generation of the openai schema behind the scenes as well as write the output to the file path you specify</p> </li> </ol> <p>Once we've got this new <code>.jsonl</code> file, we can then use the new <code>instructor</code> cli's <code>batch</code> command to create a new batch job.</p> <pre><code>&gt; % ls -a | grep test.jsonl\ntest.jsonl\n\n&gt; % instructor batch create-from-file --file-path test.jsonl\n</code></pre> <p>This will create a table like what you see below. In my case, my batch job took around 6 minutes to complete and cost me $2.72 to run.</p> Batch ID Created At Status Failed Completed Total batch_Z8XUudoweH43R9c4sr4wRYub 2024-07-16 12:45:22 in_progress 0 483 1627 <p>Once our batch job is complete, the status will change to <code>completed</code>.</p> Cancelling A Job <p>If you'd like to cancel a batch job midway, you can do so too with the instructor <code>batch</code> cli command</p> <pre><code>instructor batch cancel --batch-id &lt;batch id here&gt;\n</code></pre> <p>We can then download the file generated by the batch job using the cli command</p> <pre><code>instructor batch download-file --download-file-path output.jsonl --batch-id batch_Z8XUudoweH43R9c4sr4wRYub\n</code></pre> <p>This will then create a <code>.jsonl</code> file with the generated content at the path that you specify.</p>"},{"location":"examples/batch_job_oai/#parsing-the-generated-response","title":"Parsing the generated response","text":"<p>We can then parse the generated response by using the <code>.parse_from_file</code> command provided by the <code>BatchJob</code> class.</p> <pre><code>from instructor.batch import BatchJob\nfrom pydantic import BaseModel, Field\n\n\nclass QuestionAnswerPair(BaseModel):\n    \"\"\"\n    This model represents a pair of a question generated from a text chunk, its corresponding answer,\n    and the chain of thought leading to the answer. The chain of thought provides insight into how the answer\n    was derived from the question.\n    \"\"\"\n\n    chain_of_thought: str = Field(\n        description=\"The reasoning process leading to the answer.\"\n    )\n    question: str = Field(description=\"The generated question from the text chunk.\")\n    answer: str = Field(description=\"The answer to the generated question.\")\n\n\nparsed, unparsed = BatchJob.parse_from_file(  # (1)!\n    file_path=\"./output.jsonl\", response_model=QuestionAnswerPair\n)\n\nprint(len(parsed))\n#&gt; 1626\nprint(len(unparsed))\n#&gt; 1\n</code></pre> <ol> <li>We can then use a generic <code>Pydantic</code> schema to parse the generated function calls back</li> </ol> <p>This will then return a list of two elements</p> <ul> <li><code>parsed</code> is a list of responses that have been succesfully parsed into the <code>QuestionAnswerPair</code> Base Model class</li> <li><code>unparsed</code> is a second list which contains responses which were not able to be parsed into the <code>QuestionAnswerPair</code> Base Model class</li> </ul>"},{"location":"examples/bulk_classification/","title":"Bulk Classification from User-Provided Tags.","text":"<p>This tutorial shows how to do classification from user provided tags. This is valuable when you want to provide services that allow users to do some kind of classification.</p> <p>Motivation</p> <p>Imagine allowing the user to upload documents as part of a RAG application. Oftentimes, we might want to allow the user to specify an existing set of tags, give descriptions, and do the classification for them.</p>"},{"location":"examples/bulk_classification/#defining-the-structures","title":"Defining the Structures","text":"<p>One of the easy things to do is to allow users to define a set of tags in some kind of schema and save that in a database. Here's an example of a schema that we might use:</p> tag_id name instructions 0 personal Personal information 1 phone Phone number 2 email Email address 3 address Address 4 Other Other information <ol> <li>tag_id \u2014 The unique identifier for the tag.</li> <li>name \u2014 The name of the tag.</li> <li>instructions \u2014 A description of the tag, which can be used as a prompt to describe the tag.</li> </ol>"},{"location":"examples/bulk_classification/#implementing-the-classification","title":"Implementing the Classification","text":"<p>In order to do this we'll do a couple of things:</p> <ol> <li>We'll use the <code>instructor</code> library to patch the <code>openai</code> library to use the <code>AsyncOpenAI</code> client.</li> <li>Implement a <code>Tag</code> model that will be used to validate the tags from the context. (This will allow us to avoid hallucinating tags that are not in the context.)</li> <li>Helper models for the request and response.</li> <li>An async function to do the classification.</li> <li>A main function to run the classification using the <code>asyncio.gather</code> function to run the classification in parallel.</li> </ol> <p>If you want to learn more about how to do bad computations, check out our post on AsyncIO here.</p> <pre><code>import openai\nimport instructor\n\nclient = instructor.from_openai(\n    openai.AsyncOpenAI(),\n)\n</code></pre> <p>First, we'll need to import all of our Pydantic and instructor code and use the AsyncOpenAI client. Then, we'll define the tag model along with the tag instructions to provide input and output.</p> <p>This is very helpful because once we use something like FastAPI to create endpoints, the Pydantic functions will serve as multiple tools:</p> <ol> <li>A description for the developer</li> <li>Type hints for the IDE</li> <li>OpenAPI documentation for the FastAPI endpoint</li> <li>Schema and Response Model for the language model.</li> </ol> <pre><code>from typing import List\nfrom pydantic import BaseModel, ValidationInfo, model_validator\n\nclass Tag(BaseModel):\n    id: int\n    name: str\n\n    @model_validator(mode=\"after\")\n    def validate_ids(self, info: ValidationInfo):\n        context = info.context\n        if context:\n            tags: List[Tag] = context.get(\"tags\")\n            assert self.id in {\n                tag.id for tag in tags\n            }, f\"Tag ID {self.id} not found in context\"\n            assert self.name in {\n                tag.name for tag in tags\n            }, f\"Tag name {self.name} not found in context\"\n        return self\n\n\nclass TagWithInstructions(Tag):\n    instructions: str\n\n\nclass TagRequest(BaseModel):\n    texts: List[str]\n    tags: List[TagWithInstructions]\n\n\nclass TagResponse(BaseModel):\n    texts: List[str]\n    predictions: List[Tag]\n</code></pre> <p>Let's delve deeper into what the <code>validate_ids</code> function does. Notice that its purpose is to extract tags from the context and ensure that each ID and name exists in the set of tags. This approach helps minimize hallucinations. If we mistakenly identify either the ID or the tag, an error will be thrown, and the instructor will prompt the language model to retry until the correct item is successfully extracted.</p> <pre><code>@model_validator(mode=\"after\")\ndef validate_ids(self, info: ValidationInfo):\n    context = info.context\n    if context:\n        tags: List[Tag] = context.get(\"tags\")\n        assert self.id in {\n            tag.id for tag in tags\n        }, f\"Tag ID {self.id} not found in context\"\n        assert self.name in {\n            tag.name for tag in tags\n        }, f\"Tag name {self.name} not found in context\"\n    return self\n</code></pre> <p>Now, let's implement the function to do the classification. This function will take a single text and a list of tags and return the predicted tag.</p> <pre><code>async def tag_single_request(text: str, tags: List[Tag]) -&gt; Tag:\n    allowed_tags = [(tag.id, tag.name) for tag in tags]\n    allowed_tags_str = \", \".join([f\"`{tag}`\" for tag in allowed_tags])\n\n    return await client.chat.completions.create(\n        model=\"gpt-4-turbo-preview\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a world-class text tagging system.\",\n            },\n            {\"role\": \"user\", \"content\": f\"Describe the following text: `{text}`\"},\n            {\n                \"role\": \"user\",\n                \"content\": f\"Here are the allowed tags: {allowed_tags_str}\",\n            },\n        ],\n        response_model=Tag,  # Minimizes the hallucination of tags that are not in the allowed tags.\n        validation_context={\"tags\": tags},\n    )\n\n\nasync def tag_request(request: TagRequest) -&gt; TagResponse:\n    predictions = await asyncio.gather(\n        *[tag_single_request(text, request.tags) for text in request.texts]\n    )\n    return TagResponse(\n        texts=request.texts,\n        predictions=predictions,\n    )\n</code></pre> <p>Notice that we first define a single async function that makes a prediction of a tag, and we pass it into the validation context in order to minimize hallucinations.</p> <p>Finally, we'll implement the main function to run the classification using the <code>asyncio.gather</code> function to run the classification in parallel.</p> <pre><code>tags = [\n    TagWithInstructions(id=0, name=\"personal\", instructions=\"Personal information\"),\n    TagWithInstructions(id=1, name=\"phone\", instructions=\"Phone number\"),\n    TagWithInstructions(id=2, name=\"email\", instructions=\"Email address\"),\n    TagWithInstructions(id=3, name=\"address\", instructions=\"Address\"),\n    TagWithInstructions(id=4, name=\"Other\", instructions=\"Other information\"),\n]\n\n# Texts will be a range of different questions.\n# Such as \"How much does it cost?\", \"What is your privacy policy?\", etc.\ntexts = [\n    \"What is your phone number?\",\n    \"What is your email address?\",\n    \"What is your address?\",\n    \"What is your privacy policy?\",\n]\n\n# The request will contain the texts and the tags.\nrequest = TagRequest(texts=texts, tags=tags)\n\n# The response will contain the texts, the predicted tags, and the confidence.\nresponse = asyncio.run(tag_request(request))\nprint(response.model_dump_json(indent=2))\n</code></pre> <p>Which would result in:</p> <pre><code>{\n  \"texts\": [\n    \"What is your phone number?\",\n    \"What is your email address?\",\n    \"What is your address?\",\n    \"What is your privacy policy?\"\n  ],\n  \"predictions\": [\n    {\n      \"id\": 1,\n      \"name\": \"phone\"\n    },\n    {\n      \"id\": 2,\n      \"name\": \"email\"\n    },\n    {\n      \"id\": 3,\n      \"name\": \"address\"\n    },\n    {\n      \"id\": 4,\n      \"name\": \"Other\"\n    }\n  ]\n}\n</code></pre>"},{"location":"examples/bulk_classification/#what-happens-in-production","title":"What happens in production?","text":"<p>If we were to use this in production, we might expect to have some kind of fast API endpoint.</p> <pre><code>from fastapi import FastAPI\n\napp = FastAPI()\n\n\n@app.post(\"/tag\", response_model=TagResponse)\nasync def tag(request: TagRequest) -&gt; TagResponse:\n    return await tag_request(request)\n</code></pre> <p>Since everything is already annotated with Pydantic, this code is very simple to write!</p> <p>Where do tags come from?</p> <p>I just want to call out that here you can also imagine the tag spec IDs and names and instructions for example could come from a database or somewhere else. I'll leave this as an exercise to the reader, but I hope this gives us a clear understanding of how we can do something like user-defined classification.</p>"},{"location":"examples/bulk_classification/#improving-the-model","title":"Improving the Model","text":"<p>There's a couple things we could do to make this system a little bit more robust.</p> <ol> <li>Use confidence score:</li> </ol> <pre><code>class TagWithConfidence(Tag):\n    confidence: float = Field(\n        ...,\n        ge=0,\n        le=1,\n        description=\"The confidence of the prediction, 0 is low, 1 is high\",\n    )\n</code></pre> <ol> <li>Use multiclass classification:</li> </ol> <p>Notice in the example we use Iterable[Tag] vs Tag. This is because we might want to use a multiclass classification model that returns multiple tag!</p> <pre><code>await client.chat.completions.create(\n    model=\"gpt-4-turbo-preview\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a world-class text tagging system.\",\n        },\n        {\"role\": \"user\", \"content\": f\"Describe the following text: `{text}`\"},\n        {\n            \"role\": \"user\",\n            \"content\": f\"Here are the allowed tags: {allowed_tags_str}\",\n        },\n    ],\n    response_model=Iterable[Tag],\n    validation_context={\"tags\": tags},\n)\n</code></pre>"},{"location":"examples/classification/","title":"Example: Text Classification using OpenAI and Pydantic","text":"<p>This tutorial showcases how to implement text classification tasks\u2014specifically, single-label and multi-label classifications\u2014using the OpenAI API, Python's <code>enum</code> module, and Pydantic models.</p> <p>Motivation</p> <p>Text classification is a common problem in many NLP applications, such as spam detection or support ticket categorization. The goal is to provide a systematic way to handle these cases using OpenAI's GPT models in combination with Python data structures.</p>"},{"location":"examples/classification/#single-label-classification","title":"Single-Label Classification","text":""},{"location":"examples/classification/#defining-the-structures","title":"Defining the Structures","text":"<p>For single-label classification, we first define an <code>enum</code> for possible labels and a Pydantic model for the output.</p> <pre><code>import enum\nfrom pydantic import BaseModel\n\n\nclass Labels(str, enum.Enum):\n    \"\"\"Enumeration for single-label text classification.\"\"\"\n\n    SPAM = \"spam\"\n    NOT_SPAM = \"not_spam\"\n\n\nclass SinglePrediction(BaseModel):\n    \"\"\"\n    Class for a single class label prediction.\n    \"\"\"\n\n    class_label: Labels\n</code></pre>"},{"location":"examples/classification/#classifying-text","title":"Classifying Text","text":"<p>The function <code>classify</code> will perform the single-label classification.</p> <pre><code>from openai import OpenAI\nimport instructor\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.from_openai(OpenAI())\n\n\ndef classify(data: str) -&gt; SinglePrediction:\n    \"\"\"Perform single-label classification on the input text.\"\"\"\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo-0613\",\n        response_model=SinglePrediction,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Classify the following text: {data}\",\n            },\n        ],\n    )  # type: ignore\n</code></pre>"},{"location":"examples/classification/#testing-and-evaluation","title":"Testing and Evaluation","text":"<p>Let's run an example to see if it correctly identifies a spam message.</p> <pre><code># Test single-label classification\nprediction = classify(\"Hello there I'm a Nigerian prince and I want to give you money\")\nassert prediction.class_label == Labels.SPAM\n</code></pre>"},{"location":"examples/classification/#multi-label-classification","title":"Multi-Label Classification","text":""},{"location":"examples/classification/#defining-the-structures_1","title":"Defining the Structures","text":"<p>For multi-label classification, we introduce a new enum class and a different Pydantic model to handle multiple labels.</p> <pre><code>from typing import List\nimport enum\n\n# Define Enum class for multiple labels\nclass MultiLabels(str, enum.Enum):\n    TECH_ISSUE = \"tech_issue\"\n    BILLING = \"billing\"\n    GENERAL_QUERY = \"general_query\"\n\n\n# Define the multi-class prediction model\nclass MultiClassPrediction(BaseModel):\n    \"\"\"\n    Class for a multi-class label prediction.\n    \"\"\"\n\n    class_labels: List[MultiLabels]\n</code></pre>"},{"location":"examples/classification/#classifying-text_1","title":"Classifying Text","text":"<p>The function <code>multi_classify</code> is responsible for multi-label classification.</p> <pre><code>def multi_classify(data: str) -&gt; MultiClassPrediction:\n    \"\"\"Perform multi-label classification on the input text.\"\"\"\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo-0613\",\n        response_model=MultiClassPrediction,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Classify the following support ticket: {data}\",\n            },\n        ],\n    )  # type: ignore\n</code></pre>"},{"location":"examples/classification/#testing-and-evaluation_1","title":"Testing and Evaluation","text":"<p>Finally, we test the multi-label classification function using a sample support ticket.</p> <pre><code># Test multi-label classification\nticket = \"My account is locked and I can't access my billing info.\"\nprediction = multi_classify(ticket)\nassert MultiLabels.TECH_ISSUE in prediction.class_labels\nassert MultiLabels.BILLING in prediction.class_labels\n</code></pre>"},{"location":"examples/document_segmentation/","title":"Document Segmentation","text":"<p>In this guide, we demonstrate how to do document segmentation using structured output from an LLM. We'll be using command-r-plus - one of Cohere's latest LLMs with 128k context length and testing the approach on an article explaining the Transformer architecture. Same approach to document segmentation can be applied to any other domain where we need to break down a complex long document into smaller chunks.</p> <p>Motivation</p> <p>Sometimes we need a way to split the document into meaningful parts that center around a single key concept/idea. Simple length-based / rule-based text-splitters are not reliable enough. Consider the cases where documents contain code snippets or math equations - we don't want to split those on <code>'\\n\\n'</code> or have to write extensive rules for different types of documents. It turns out that LLMs with sufficiently long context length are well suited for this task.</p>"},{"location":"examples/document_segmentation/#defining-the-data-structures","title":"Defining the Data Structures","text":"<p>First, we need to define a <code>Section</code> class for each of the document's segments.  <code>StructuredDocument</code> class will then encapsulate a list of these sections.</p> <p>Note that in order to avoid LLM regenerating the content of each section, we can simply enumerate each line of the input document and then ask LLM to segment it by providing start-end line numbers for each section.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List, Dict, Any\n\nclass Section(BaseModel):\n    title: str = Field(description=\"main topic of this section of the document\")\n    start_index: int = Field(description=\"line number where the section begins\")\n    end_index: int = Field(description=\"line number where the section ends\")\n\n\nclass StructuredDocument(BaseModel):\n    \"\"\"obtains meaningful sections, each centered around a single concept/topic\"\"\"\n    sections: List[Section] = Field(description=\"a list of sections of the document\")\n</code></pre>"},{"location":"examples/document_segmentation/#document-preprocessing","title":"Document Preprocessing","text":"<p>Preprocess the input <code>document</code> by prepending each line with its number.</p> <pre><code>def doc_with_lines(document):\n    document_lines = document.split(\"\\n\")\n    document_with_line_numbers = \"\"\n    line2text = {}\n    for i, line in enumerate(document_lines):\n        document_with_line_numbers += f\"[{i}] {line}\\n\"\n        line2text[i] = line\n    return document_with_line_numbers, line2text\n</code></pre>"},{"location":"examples/document_segmentation/#segmentation","title":"Segmentation","text":"<p>Next use a Cohere client to extract <code>StructuredDocument</code> from the preprocessed doc.</p> <pre><code>import instructor\nimport cohere\n\n# Apply the patch to the cohere client\n# enables response_model keyword\nclient = instructor.from_cohere(cohere.Client())\n\n\nsystem_prompt = f\"\"\"\\\nYou are a world class educator working on organizing your lecture notes.\nRead the document below and extract a StructuredDocument object from it where each section of the document is centered around a single concept/topic that can be taught in one lesson.\nEach line of the document is marked with its line number in square brackets (e.g. [1], [2], [3], etc). Use the line numbers to indicate section start and end.\n\"\"\"\n\n\ndef get_structured_document(document_with_line_numbers) -&gt; StructuredDocument:\n    return client.chat.completions.create(\n        model=\"command-r-plus\",\n        response_model=StructuredDocument,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt,\n            },\n            {\n                \"role\": \"user\",\n                \"content\": document_with_line_numbers,\n            },\n        ],\n    )  # type: ignore\n</code></pre> <p>Next, we need to get back the section text based on the start/end indices and our <code>line2text</code> dict from the preprocessing step.</p> <pre><code>def get_sections_text(structured_doc, line2text):\n    segments = []\n    for s in structured_doc.sections:\n        contents = []\n        for line_id in range(s.start_index, s.end_index):\n                contents.append(line2text.get(line_id, ''))\n        segments.append({\n            \"title\": s.title,\n            \"content\": \"\\n\".join(contents),\n            \"start\": s.start_index,\n            \"end\": s.end_index\n        })\n    return segments\n</code></pre>"},{"location":"examples/document_segmentation/#example","title":"Example","text":"<p>Here's an example of using these classes and functions to segment a tutorial on Transformers from Sebastian Raschka. We can use <code>trafilatura</code> package to scrape the web page content of the article.</p> <pre><code>from trafilatura import fetch_url, extract\n\n\nurl='https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html'\ndownloaded = fetch_url(url)\ndocument = extract(downloaded)\n\n\ndocument_with_line_numbers, line2text = doc_with_lines(document)\nstructured_doc = get_structured_document(document_with_line_numbers)\nsegments = get_sections_text(structured_doc, line2text)\n</code></pre> <pre><code>print(segments[5]['title'])\n\"\"\"\nIntroduction to Multi-Head Attention\n\"\"\"\nprint(segments[5]['content'])\n\"\"\"\nMulti-Head Attention\nIn the very first figure, at the top of this article, we saw that transformers use a module called multi-head attention. How does that relate to the self-attention mechanism (scaled-dot product attention) we walked through above?\nIn the scaled dot-product attention, the input sequence was transformed using three matrices representing the query, key, and value. These three matrices can be considered as a single attention head in the context of multi-head attention. The figure below summarizes this single attention head we covered previously:\nAs its name implies, multi-head attention involves multiple such heads, each consisting of query, key, and value matrices. This concept is similar to the use of multiple kernels in convolutional neural networks.\nTo illustrate this in code, suppose we have 3 attention heads, so we now extend the \\(d' \\times d\\) dimensional weight matrices so \\(3 \\times d' \\times d\\):\nIn:\nh = 3\nmultihead_W_query = torch.nn.Parameter(torch.rand(h, d_q, d))\nmultihead_W_key = torch.nn.Parameter(torch.rand(h, d_k, d))\nmultihead_W_value = torch.nn.Parameter(torch.rand(h, d_v, d))\nConsequently, each query element is now \\(3 \\times d_q\\) dimensional, where \\(d_q=24\\) (here, let\u2019s keep the focus on the 3rd element corresponding to index position 2):\nIn:\nmultihead_query_2 = multihead_W_query.matmul(x_2)\nprint(multihead_query_2.shape)\nOut:\ntorch.Size([3, 24])\n\"\"\"\n</code></pre>"},{"location":"examples/entity_resolution/","title":"Entity Resolution and Visualization for Legal Documents","text":"<p>In this guide, we demonstrate how to extract and resolve entities from a sample legal contract. Then, we visualize these entities and their dependencies as an entity graph. This approach can be invaluable for legal tech applications, aiding in the understanding of complex documents.</p> <p>Motivation</p> <p>Legal contracts are full of intricate details and interconnected clauses. Automatically extracting and visualizing these elements can make it easier to understand the document's overall structure and terms.</p>"},{"location":"examples/entity_resolution/#defining-the-data-structures","title":"Defining the Data Structures","text":"<p>The <code>Entity</code> and <code>Property</code> classes model extracted entities and their attributes. <code>DocumentExtraction</code> encapsulates a list of these entities.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List\n\n\nclass Property(BaseModel):\n    key: str\n    value: str\n    resolved_absolute_value: str\n\n\nclass Entity(BaseModel):\n    id: int = Field(\n        ...,\n        description=\"Unique identifier for the entity, used for deduplication, design a scheme allows multiple entities\",\n    )\n    subquote_string: List[str] = Field(\n        ...,\n        description=\"Correctly resolved value of the entity, if the entity is a reference to another entity, this should be the id of the referenced entity, include a few more words before and after the value to allow for some context to be used in the resolution\",\n    )\n    entity_title: str\n    properties: List[Property] = Field(\n        ..., description=\"List of properties of the entity\"\n    )\n    dependencies: List[int] = Field(\n        ...,\n        description=\"List of entity ids that this entity depends  or relies on to resolve it\",\n    )\n\n\nclass DocumentExtraction(BaseModel):\n    entities: List[Entity] = Field(\n        ...,\n        description=\"Body of the answer, each fact should be a separate object with a body and a list of sources\",\n    )\n</code></pre>"},{"location":"examples/entity_resolution/#entity-extraction-and-resolution","title":"Entity Extraction and Resolution","text":"<p>The <code>ask_ai</code> function utilizes OpenAI's API to extract and resolve entities from the input content.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.from_openai(OpenAI())\n\n\ndef ask_ai(content) -&gt; DocumentExtraction:\n    return client.chat.completions.create(\n        model=\"gpt-4\",\n        response_model=DocumentExtraction,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Extract and resolve a list of entities from the following document:\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": content,\n            },\n        ],\n    )  # type: ignore\n</code></pre>"},{"location":"examples/entity_resolution/#graph-visualization","title":"Graph Visualization","text":"<p><code>generate_graph</code> takes the extracted entities and visualizes them using Graphviz. It creates nodes for each entity and edges for their dependencies.</p> <pre><code>from graphviz import Digraph\n\n\ndef generate_html_label(entity: Entity) -&gt; str:\n    rows = [\n        f\"&lt;tr&gt;&lt;td&gt;{prop.key}&lt;/td&gt;&lt;td&gt;{prop.resolved_absolute_value}&lt;/td&gt;&lt;/tr&gt;\"\n        for prop in entity.properties\n    ]\n    table_rows = \"\".join(rows)\n    return f\"&lt;&lt;table border='0' cellborder='1' cellspacing='0'&gt;&lt;tr&gt;&lt;td colspan='2'&gt;&lt;b&gt;{entity.entity_title}&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;{table_rows}&lt;/table&gt;&gt;\"\n\n\ndef generate_graph(data: DocumentExtraction):\n    dot = Digraph(comment=\"Entity Graph\", node_attr={\"shape\": \"plaintext\"})\n\n    for entity in data.entities:\n        label = generate_html_label(entity)\n        dot.node(str(entity.id), label)\n\n    for entity in data.entities:\n        for dep_id in entity.dependencies:\n            dot.edge(str(entity.id), str(dep_id))\n\n    dot.render(\"entity.gv\", view=True)\n</code></pre>"},{"location":"examples/entity_resolution/#execution","title":"Execution","text":"<p>Finally, execute the code to visualize the entity graph for the sample legal contract.</p> <pre><code>content = \"\"\"\nSample Legal Contract\nAgreement Contract\n\nThis Agreement is made and entered into on 2020-01-01 by and between Company A (\"the Client\") and Company B (\"the Service Provider\").\n\nArticle 1: Scope of Work\n\nThe Service Provider will deliver the software product to the Client 30 days after the agreement date.\n\nArticle 2: Payment Terms\n\nThe total payment for the service is $50,000.\nAn initial payment of $10,000 will be made within 7 days of the the signed date.\nThe final payment will be due 45 days after [SignDate].\n\nArticle 3: Confidentiality\n\nThe parties agree not to disclose any confidential information received from the other party for 3 months after the final payment date.\n\nArticle 4: Termination\n\nThe contract can be terminated with a 30-day notice, unless there are outstanding obligations that must be fulfilled after the [DeliveryDate].\n\"\"\"  # Your legal contract here\nmodel = ask_ai(content)\ngenerate_graph(model)\n</code></pre> <p>This will produce a graphical representation of the entities and their dependencies, stored as \"entity.gv\".</p> <p></p>"},{"location":"examples/exact_citations/","title":"Example: Answering Questions with Validated Citations","text":"<p>For the full code example check out examples/citation_fuzzy_match.py</p>"},{"location":"examples/exact_citations/#overview","title":"Overview","text":"<p>This example shows how to use Instructor with validators to not only add citations to answers generated but also prevent hallucinations by ensuring that every statement made by the LLM is backed up by a direct quote from the context provided, and that those quotes exist!.Two Python classes, <code>Fact</code> and <code>QuestionAnswer</code>, are defined to encapsulate the information of individual facts and the entire answer, respectively.</p>"},{"location":"examples/exact_citations/#data-structures","title":"Data Structures","text":""},{"location":"examples/exact_citations/#the-fact-class","title":"The <code>Fact</code> Class","text":"<p>The <code>Fact</code> class encapsulates a single statement or fact. It contains two fields:</p> <ul> <li><code>fact</code>: A string representing the body of the fact or statement.</li> <li><code>substring_quote</code>: A list of strings. Each string is a direct quote from the context that supports the <code>fact</code>.</li> </ul>"},{"location":"examples/exact_citations/#validation-method-validate_sources","title":"Validation Method: <code>validate_sources</code>","text":"<p>This method validates the sources (<code>substring_quote</code>) in the context. It utilizes regex to find the span of each substring quote in the given context. If the span is not found, the quote is removed from the list.</p> <pre><code>from pydantic import Field, BaseModel, model_validator, ValidationInfo\nfrom typing import List\n\n\nclass Fact(BaseModel):\n    fact: str = Field(...)\n    substring_quote: List[str] = Field(...)\n\n    @model_validator(mode=\"after\")\n    def validate_sources(self, info: ValidationInfo) -&gt; \"Fact\":\n        text_chunks = info.context.get(\"text_chunk\", None)\n        spans = list(self.get_spans(text_chunks))\n        self.substring_quote = [text_chunks[span[0] : span[1]] for span in spans]\n        return self\n\n    def get_spans(self, context):\n        for quote in self.substring_quote:\n            yield from self._get_span(quote, context)\n\n    def _get_span(self, quote, context):\n        for match in re.finditer(re.escape(quote), context):\n            yield match.span()\n</code></pre>"},{"location":"examples/exact_citations/#the-questionanswer-class","title":"The <code>QuestionAnswer</code> Class","text":"<p>This class encapsulates the question and its corresponding answer. It contains two fields:</p> <ul> <li><code>question</code>: The question asked.</li> <li><code>answer</code>: A list of <code>Fact</code> objects that make up the answer.</li> </ul>"},{"location":"examples/exact_citations/#validation-method-validate_sources_1","title":"Validation Method: <code>validate_sources</code>","text":"<p>This method checks that each <code>Fact</code> object in the <code>answer</code> list has at least one valid source. If a <code>Fact</code> object has no valid sources, it is removed from the <code>answer</code> list.</p> <pre><code>class QuestionAnswer(BaseModel):\n    question: str = Field(...)\n    answer: List[Fact] = Field(...)\n\n    @model_validator(mode=\"after\")\n    def validate_sources(self) -&gt; \"QuestionAnswer\":\n        self.answer = [fact for fact in self.answer if len(fact.substring_quote) &gt; 0]\n        return self\n</code></pre>"},{"location":"examples/exact_citations/#function-to-ask-ai-a-question","title":"Function to Ask AI a Question","text":""},{"location":"examples/exact_citations/#the-ask_ai-function","title":"The <code>ask_ai</code> Function","text":"<p>This function takes a string <code>question</code> and a string <code>context</code> and returns a <code>QuestionAnswer</code> object. It uses the OpenAI API to fetch the answer and then validates the sources using the defined classes.</p> <p>To understand the validation context work from pydantic check out pydantic's docs</p> <pre><code>from openai import OpenAI\nimport instructor\n\n# Apply the patch to the OpenAI client\n# enables response_model, validation_context keyword\nclient = instructor.from_openai(OpenAI())\n\n\ndef ask_ai(question: str, context: str) -&gt; QuestionAnswer:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo-0613\",\n        temperature=0,\n        response_model=QuestionAnswer,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a world class algorithm to answer questions with correct and exact citations.\",\n            },\n            {\"role\": \"user\", \"content\": f\"{context}\"},\n            {\"role\": \"user\", \"content\": f\"Question: {question}\"},\n        ],\n        validation_context={\"text_chunk\": context},\n    )\n</code></pre>"},{"location":"examples/exact_citations/#example","title":"Example","text":"<p>dd Here's an example of using these classes and functions to ask a question and validate the answer.</p> <pre><code>question = \"What did the author do during college?\"\ncontext = \"\"\"\nMy name is Jason Liu, and I grew up in Toronto Canada but I was born in China.\nI went to an arts high school but in university I studied Computational Mathematics and physics.\nAs part of coop I worked at many companies including Stitchfix, Facebook.\nI also started the Data Science club at the University of Waterloo and I was the president of the club for 2 years.\n\"\"\"\n</code></pre> <p>The output would be a <code>QuestionAnswer</code> object containing validated facts and their sources.</p> <pre><code>{\n    \"question\": \"where did he go to school?\",\n    \"answer\": [\n        {\n            \"statement\": \"Jason Liu went to an arts highschool.\",\n            \"substring_phrase\": [\"arts highschool\"],\n        },\n        {\n            \"statement\": \"Jason Liu studied Computational Mathematics and physics in university.\",\n            \"substring_phrase\": [\"university\"],\n        },\n    ],\n}\n</code></pre> <p>This ensures that every piece of information in the answer has been validated against the context.</p>"},{"location":"examples/examples/","title":"How should I include examples?","text":"<p>To enhance the clarity and usability of your model and prompt, incorporating examples directly into the JSON schema extra of your Pydantic model is highly recommended. This approach not only streamlines the integration of practical examples but also ensures that they are easily accessible and understandable within the context of your model's schema.</p> <pre><code>import openai\nimport instructor\nfrom typing import Iterable\nfrom pydantic import BaseModel, Field, ConfigDict\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass SyntheticQA(BaseModel):\n    question: str\n    answer: str\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n                {\n                    \"question\": \"What is the largest planet in our solar system?\",\n                    \"answer\": \"Jupiter\",\n                },\n                {\n                    \"question\": \"Who wrote 'To Kill a Mockingbird'?\",\n                    \"answer\": \"Harper Lee\",\n                },\n                {\n                    \"question\": \"What element does 'O' represent on the periodic table?\",\n                    \"answer\": \"Oxygen\",\n                },\n            ]\n        }\n    )\n\n\ndef get_synthetic_data() -&gt; Iterable[SyntheticQA]:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"Generate synthetic examples\"},\n            {\n                \"role\": \"user\",\n                \"content\": \"Generate the exact examples you see in the examples of this prompt. \",\n            },\n        ],\n        response_model=Iterable[SyntheticQA],\n    )  # type: ignore\n\n\nif __name__ == \"__main__\":\n    for example in get_synthetic_data():\n        print(example)\n        \"\"\"\n        question='What is the capital of France?' answer='Paris'\n        question='What is the largest planet in our solar system?' answer='Jupiter'\n        question=\"Who wrote 'To Kill a Mockingbird'?\" answer='Harper Lee'\n        question=\"What element does 'O' represent on the periodic table?\" answer='Oxygen'\n        \"\"\"\n</code></pre>"},{"location":"examples/extract_slides/","title":"Data extraction from slides","text":"<p>In this guide, we demonstrate how to extract data from slides.</p> <p>Motivation</p> <p>When we want to translate key information from slides into structured data, simply isolating the text and running extraction might not be enough. Sometimes the important data is in the images on the slides, so we should consider including them in our extraction pipeline.</p>"},{"location":"examples/extract_slides/#defining-the-necessary-data-structures","title":"Defining the necessary Data Structures","text":"<p>Let's say we want to extract the competitors from various presentations and categorize them according to their respective industries.</p> <p>Our data model will have <code>Industry</code> which will be a list of <code>Competitor</code>'s for a specific industry, and <code>Competition</code> which will aggregate the competitors for all the industries.</p> <pre><code>from openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, List\n\nclass Competitor(BaseModel):\n    name: str\n    features: Optional[List[str]]\n\n\n# Define models\nclass Industry(BaseModel):\n    \"\"\"\n    Represents competitors from a specific industry extracted from an image using AI.\n    \"\"\"\n\n    name: str = Field(\n        description=\"The name of the industry\"\n    )\n    competitor_list: List[Competitor] = Field(\n        description=\"A list of competitors for this industry\"\n    )\n\nclass Competition(BaseModel):\n    \"\"\"\n    This class serves as a structured representation of \n    competitors and their qualities.\n    \"\"\"\n\n    industry_list: List[IndustryCompetition] = Field(\n        description=\"A list of industries and their competitors\"\n    )\n</code></pre>"},{"location":"examples/extract_slides/#competitors-extraction","title":"Competitors extraction","text":"<p>To extract competitors from slides we will define a function which will read images from urls and extract the relevant information from them.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.from_openai(\n    OpenAI()\n)\n\n# Define functions\ndef read_images(image_urls: List[str]) -&gt; Competition:\n    \"\"\"\n    Given a list of image URLs, identify the competitors in the images.\n    \"\"\"\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Competition,\n        max_tokens=2048,\n        temperature=0,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"Identify competitors and generate key features for each competitor.\",\n                    },\n                    *[\n                        {\"type\": \"image_url\", \"image_url\": {\"url\": url}}\n                        for url in image_urls\n                    ],\n                ],\n            }\n        ],\n    )\n</code></pre>"},{"location":"examples/extract_slides/#execution","title":"Execution","text":"<p>Finally, we will run the previous function with a few sample slides to see the data extractor in action.</p> <p>As we can see, our model extracted the relevant information for each competitor regardless of how this information was formatted in the original presentations.</p> <p><pre><code>url = [\n    'https://miro.medium.com/v2/resize:fit:1276/0*h1Rsv-fZWzQUyOkt', \n    'https://earlygame.vc/wp-content/uploads/2020/06/startup-pitch-deck-5.jpg'\n    ]\nmodel = read_images(url)\nprint(model.model_json_dump(indent=2))\n</code></pre>     industry_list=[</p> <pre><code>Industry(name='Accommodation and Hospitality', competitor_list=[Competitor(name='CouchSurfing', features=['Affordable', 'Online Transaction']), Competitor(name='Craigslist', features=['Affordable', 'Offline Transaction']), Competitor(name='BedandBreakfast.com', features=['Affordable', 'Offline Transaction']), Competitor(name='AirBed&amp;Breakfast', features=['Affordable', 'Online Transaction']), Competitor(name='Hostels.com', features=['Affordable', 'Online Transaction']), Competitor(name='VRBO', features=['Expensive', 'Offline Transaction']), Competitor(name='Rentahome', features=['Expensive', 'Online Transaction']), Competitor(name='Orbitz', features=['Expensive', 'Online Transaction']), Competitor(name='Hotels.com', features=['Expensive', 'Online Transaction'])]),\n\nIndustry(name='Wine E-commerce', competitor_list=[Competitor(name='WineSimple', features=['Ecommerce Retailers', 'True Personalized Selections', 'Brand Name Wine', 'No Inventory Cost', 'Target Mass Market']), Competitor(name='NakedWines', features=['Ecommerce Retailers', 'Target Mass Market']), Competitor(name='Club W', features=['Ecommerce Retailers', 'Brand Name Wine', 'Target Mass Market']), Competitor(name='Tasting Room', features=['Ecommerce Retailers', 'True Personalized Selections', 'Brand Name Wine']), Competitor(name='Drync', features=['Ecommerce Retailers', 'True Personalized Selections', 'No Inventory Cost']), Competitor(name='Hello Vino', features=['Ecommerce Retailers', 'Brand Name Wine', 'Target Mass Market'])])\n\n]\n</code></pre> <p><code></code></p>"},{"location":"examples/extracting_receipts/","title":"Extracting Receipt Data using GPT-4 and Python","text":"<p>This post demonstrates how to use Python's Pydantic library and OpenAI's GPT-4 model to extract receipt data from images and validate the total amount. This method is particularly useful for automating expense tracking and financial analysis tasks.</p>"},{"location":"examples/extracting_receipts/#defining-the-item-and-receipt-classes","title":"Defining the Item and Receipt Classes","text":"<p>First, we define two Pydantic models, <code>Item</code> and <code>Receipt</code>, to structure the extracted data. The <code>Item</code> class represents individual items on the receipt, with fields for name, price, and quantity. The <code>Receipt</code> class contains a list of <code>Item</code> objects and the total amount.</p> <pre><code>class Item(BaseModel):\n    name: str\n    price: float\n    quantity: int\n\nclass Receipt(BaseModel):\n    items: list[Item]\n    total: float\n</code></pre>"},{"location":"examples/extracting_receipts/#validating-the-total-amount","title":"Validating the Total Amount","text":"<p>To ensure the accuracy of the extracted data, we use Pydantic's <code>model_validator</code> decorator to define a custom validation function, <code>check_total</code>. This function calculates the sum of item prices and compares it to the extracted total amount. If there's a discrepancy, it raises a <code>ValueError</code>.</p> <pre><code>@model_validator(mode=\"after\")\ndef check_total(cls, values: \"Receipt\"):\n    items = values.items\n    total = values.total\n    calculated_total = sum(item.price * item.quantity for item in items)\n    if calculated_total != total:\n        raise ValueError(\n            f\"Total {total} does not match the sum of item prices {calculated_total}\"\n        )\n    return values\n</code></pre>"},{"location":"examples/extracting_receipts/#extracting-receipt-data-from-images","title":"Extracting Receipt Data from Images","text":"<p>The <code>extract_receipt</code> function uses OpenAI's GPT-4 model to process an image URL and extract receipt data. We utilize the <code>instructor</code> library to configure the OpenAI client for this purpose.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(\n    client=OpenAI(),\n    mode=instructor.Mode.TOOLS,\n)\n\ndef extract(url: str) -&gt; Receipt:\n    return client.chat.completions.create(\n        model=\"gpt-4\",\n        max_tokens=4000,\n        response_model=Receipt,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": url},\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"Analyze the image and return the items in the receipt and the total amount.\",\n                    },\n                ],\n            }\n        ],\n    )\n</code></pre>"},{"location":"examples/extracting_receipts/#practical-examples","title":"Practical Examples","text":"<p>In these examples, we apply the method to extract receipt data from two different images. The custom validation function ensures that the extracted total amount matches the sum of item prices.</p> <pre><code>urls = [\n    \"https://templates.mediamodifier.com/645124ff36ed2f5227cbf871/supermarket-receipt-template.jpg\",\n    \"https://ocr.space/Content/Images/receipt-ocr-original.jpg\",\n]\n\nfor url in urls:\n    receipt = extract(url)\n    print(receipt)\n</code></pre> <p>By combining the power of GPT-4 and Python's Pydantic library, we can accurately extract and validate receipt data from images, streamlining expense tracking and financial analysis tasks.</p>"},{"location":"examples/extracting_tables/","title":"Extracting Tables using GPT-Vision","text":"<p>This post demonstrates how to use Python's type annotations and OpenAI's new vision model to extract tables from images and convert them into markdown format. This method is particularly useful for data analysis and automation tasks.</p> <p>The full code is available on GitHub</p>"},{"location":"examples/extracting_tables/#building-the-custom-type-for-markdown-tables","title":"Building the Custom Type for Markdown Tables","text":"<p>First, we define a custom type, <code>MarkdownDataFrame</code>, to handle pandas DataFrames formatted in markdown. This type uses Python's <code>Annotated</code> and <code>InstanceOf</code> types, along with decorators <code>BeforeValidator</code> and <code>PlainSerializer</code>, to process and serialize the data.</p> <pre><code>from io import StringIO\nfrom typing import Annotated, Any\nfrom pydantic import BaseModel, Field, BeforeValidator, PlainSerializer, InstanceOf, WithJsonSchema\nfrom typing import Iterable\nimport pandas as pd\n\n\ndef md_to_df(data: Any) -&gt; Any:\n    # Convert markdown to DataFrame\n    if isinstance(data, str):\n        return (\n            pd.read_csv(\n                StringIO(data),  # Process data\n                sep=\"|\",\n                index_col=1,\n            )\n            .dropna(axis=1, how=\"all\")\n            .iloc[1:]\n            .applymap(lambda x: x.strip())\n        )\n    return data\n\n\nMarkdownDataFrame = Annotated[\n    InstanceOf[pd.DataFrame],\n    BeforeValidator(md_to_df),\n    PlainSerializer(lambda df: df.to_markdown()),\n    WithJsonSchema(\n        {\n            \"type\": \"string\",\n            \"description\": \"The markdown representation of the table, each one should be tidy, do not try to join tables that should be seperate\",\n        }\n    ),\n]\n</code></pre>"},{"location":"examples/extracting_tables/#defining-the-table-class","title":"Defining the Table Class","text":"<p>The <code>Table</code> class is essential for organizing the extracted data. It includes a caption and a dataframe, processed as a markdown table. Since most of the complexity is handled by the <code>MarkdownDataFrame</code> type, the <code>Table</code> class is straightforward!</p> <pre><code>class Table(BaseModel):\n    caption: str\n    dataframe: MarkdownDataFrame\n</code></pre>"},{"location":"examples/extracting_tables/#extracting-tables-from-images","title":"Extracting Tables from Images","text":"<p>The <code>extract_table</code> function uses OpenAI's vision model to process an image URL and extract tables in markdown format. We utilize the <code>instructor</code> library to patch the OpenAI client for this purpose.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Apply the patch to the OpenAI client to support response_model\n# Also use MD_JSON mode since the visino model does not support any special structured output mode\nclient = instructor.from_openai(OpenAI(), mode=instructor.function_calls.Mode.MD_JSON)\n\n\ndef extract_table(url: str) -&gt; Iterable[Table]:\n    return client.chat.completions.create(\n        model=\"gpt-4-vision-preview\",\n        response_model=Iterable[Table],\n        max_tokens=1800,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": \"Extract table from image.\"},\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": url}},\n                ],\n            }\n        ],\n    )\n</code></pre>"},{"location":"examples/extracting_tables/#practical-example","title":"Practical Example","text":"<p>In this example, we apply the method to extract data from an image showing the top grossing apps in Ireland for October 2023.</p> <pre><code>url = \"https://a.storyblok.com/f/47007/2400x2000/bf383abc3c/231031_uk-ireland-in-three-charts_table_v01_b.png\"\ntables = extract_table(url)\nfor table in tables:\n    print(table.caption, end=\"\\n\")\n    print(table.dataframe)\n</code></pre> Expand to see the output <p></p>"},{"location":"examples/extracting_tables/#top-10-grossing-apps-in-october-2023-ireland-for-android-platforms","title":"Top 10 Grossing Apps in October 2023 (Ireland) for Android Platforms","text":"Rank App Name Category 1 Google One Productivity 2 Disney+ Entertainment 3 TikTok - Videos, Music &amp; LIVE Entertainment 4 Candy Crush Saga Games 5 Tinder: Dating, Chat &amp; Friends Social networking 6 Coin Master Games 7 Roblox Games 8 Bumble - Dating &amp; Make Friends Dating 9 Royal Match Games 10 Spotify: Music and Podcasts Music &amp; Audio"},{"location":"examples/extracting_tables/#top-10-grossing-apps-in-october-2023-ireland-for-ios-platforms","title":"Top 10 Grossing Apps in October 2023 (Ireland) for iOS Platforms","text":"Rank App Name Category 1 Tinder: Dating, Chat &amp; Friends Social networking 2 Disney+ Entertainment 3 YouTube: Watch, Listen, Stream Entertainment 4 Audible: Audio Entertainment Entertainment 5 Candy Crush Saga Games 6 TikTok - Videos, Music &amp; LIVE Entertainment 7 Bumble - Dating &amp; Make Friends Dating 8 Roblox Games 9 LinkedIn: Job Search &amp; News Business 10 Duolingo - Language Lessons Education"},{"location":"examples/groq/","title":"Structured Outputs using Groq","text":"<p>Instead of using openai or antrophic you can now also use groq for inference by using from_groq.</p> <p>The examples are using mixtral-8x7b model.</p>"},{"location":"examples/groq/#groqcloud-api","title":"GroqCloud API","text":"<p>To use groq you need to obtain a groq API key. Goto groqcloud and login. Select API Keys from the left menu and then select Create API key to create a new key.</p>"},{"location":"examples/groq/#use-example","title":"Use example","text":"<p>Some pip packages need to be installed to use the example: <pre><code>pip install instructor groq pydantic openai anthropic\n</code></pre> You need to export the groq API key: <pre><code>export GROQ_API_KEY=&lt;your-api-key&gt;\n</code></pre></p> <p>An example: <pre><code>import os\nfrom pydantic import BaseModel, Field\nfrom typing import List\nfrom groq import Groq\nimport instructor\n\nclass Character(BaseModel):\n    name: str\n    fact: List[str] = Field(..., description=\"A list of facts about the subject\")\n\n\nclient = Groq(\n    api_key=os.environ.get('GROQ_API_KEY'),\n)\n\nclient = instructor.from_groq(client, mode=instructor.Mode.TOOLS)\n\nresp = client.chat.completions.create(\n    model=\"mixtral-8x7b-32768\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Tell me about the company Tesla\",\n        }\n    ],\n    response_model=Character,\n)\nprint(resp.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"Tesla\",\n  \"fact\": [\n    \"An American electric vehicle and clean energy company.\",\n    \"Co-founded by Elon Musk, JB Straubel, Martin Eberhard, Marc Tarpenning, and Ian Wright in 2003.\",\n    \"Headquartered in Austin, Texas.\",\n    \"Produces electric vehicles, energy storage solutions, and more recently, solar energy products.\",\n    \"Known for its premium electric vehicles, such as the Model S, Model 3, Model X, and Model Y.\",\n    \"One of the world's most valuable car manufacturers by market capitalization.\",\n    \"Tesla's CEO, Elon Musk, is also the CEO of SpaceX, Neuralink, and The Boring Company.\",\n    \"Tesla operates the world's largest global network of electric vehicle supercharging stations.\",\n    \"The company aims to accelerate the world's transition to sustainable transport and energy through innovative technologies and products.\"\n  ]\n}\n\"\"\"\n</code></pre> You can find another example called groq_example2.py under examples/groq of this repository.</p>"},{"location":"examples/image_to_ad_copy/","title":"Use Vision API to detect products and generate advertising copy","text":"<p>This post demonstrates how to use GPT-4 Vision API and the Chat API to automatically generate advertising copy from product images. This method can be useful for marketing and advertising teams, as well as for e-commerce platforms.</p> <p>The full code is available on GitHub.</p>"},{"location":"examples/image_to_ad_copy/#building-the-models","title":"Building the models","text":""},{"location":"examples/image_to_ad_copy/#product","title":"Product","text":"<p>For the <code>Product</code> model, we define a class that represents a product extracted from an image and store the name, key features, and description. The product attributes are dynamically determined based on the content of the image.</p> <p>Note that it is easy to add Validators and other Pydantic features to the model to ensure that the data is valid and consistent.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List, Optional\n\nclass Product(BaseModel):\n    \"\"\"\n    Represents a product extracted from an image using AI.\n\n    The product attributes are dynamically determined based on the content\n    of the image and the AI's interpretation. This class serves as a structured\n    representation of the identified product characteristics.\n    \"\"\"\n\n    name: str = Field(\n        description=\"A generic name for the product.\", example=\"Headphones\"\n    )\n    key_features: Optional[List[str]] = Field(\n        description=\"A list of key features of the product that stand out.\",\n        default=None,\n    )\n\n    description: Optional[str] = Field(\n        description=\"A description of the product.\",\n        default=None,\n    )\n\n    # Can be customized and automatically generated\n    def generate_prompt(self):\n        prompt = f\"Product: {self.name}\\n\"\n        if self.description:\n            prompt += f\"Description: {self.description}\\n\"\n        if self.key_features:\n            prompt += f\"Key Features: {', '.join(self.key_features)}\\n\"\n        return prompt\n</code></pre>"},{"location":"examples/image_to_ad_copy/#identified-product","title":"Identified Product","text":"<p>We also define a class that represents a list of products identified in the images. We also add an error flag and message to indicate if there was an error in the processing of the image.</p> <pre><code>class IdentifiedProduct(BaseModel):\n    \"\"\"\n    Represents a list of products identified in the images.\n    \"\"\"\n\n    products: Optional[List[Product]] = Field(\n        description=\"A list of products identified by the AI.\",\n        example=[\n            Product(\n                name=\"Headphones\",\n                description=\"Wireless headphones with noise cancellation.\",\n                key_features=[\"Wireless\", \"Noise Cancellation\"],\n            )\n        ],\n        default=None,\n    )\n\n    error: bool = Field(default=False)\n    message: Optional[str] = Field(default=None)\n</code></pre>"},{"location":"examples/image_to_ad_copy/#advertising-copy","title":"Advertising Copy","text":"<p>Finally, the <code>AdCopy</code> models stores the output in a structured format with a headline and the text.</p> <pre><code>class AdCopy(BaseModel):\n    \"\"\"\n    Represents a generated ad copy.\n    \"\"\"\n\n    headline: str = Field(\n        description=\"A short, catchy, and memorable headline for the given product. The headline should invoke curiosity and interest in the product.\",\n    )\n    ad_copy: str = Field(\n        description=\"A long-form advertisement copy for the given product. This will be used in campaigns to promote the product with a persuasive message and a call-to-action with the objective of driving sales.\",\n    )\n    name: str = Field(description=\"The name of the product being advertised.\")\n</code></pre>"},{"location":"examples/image_to_ad_copy/#calling-the-api","title":"Calling the API","text":""},{"location":"examples/image_to_ad_copy/#product-detection","title":"Product Detection","text":"<p>The <code>read_images</code> function uses OpenAI's vision model to process a list of image URLs and identify products in each of them. We utilize the <code>instructor</code> library to patch the OpenAI client for this purpose.</p> <pre><code>def read_images(image_urls: List[str]) -&gt; IdentifiedProduct:\n    \"\"\"\n    Given a list of image URLs, identify the products in the images.\n    \"\"\"\n\n    logger.info(f\"Identifying products in images... {len(image_urls)} images\")\n\n    return client_image.chat.completions.create(\n        model=\"gpt-4-vision-preview\",\n        response_model=IdentifiedProduct,\n        max_tokens=1024,  # can be changed\n        temperature=0,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"Identify products using the given images and generate key features for each product.\",\n                    },\n                    *[\n                        {\"type\": \"image_url\", \"image_url\": {\"url\": url}}\n                        for url in image_urls\n                    ],\n                ],\n            }\n        ],\n    )\n</code></pre> <p>This gives us a list of products identified in all the images.</p>"},{"location":"examples/image_to_ad_copy/#generate-advertising-copy","title":"Generate advertising copy","text":"<p>Then, we can use the <code>generate_ad_copy</code> function to generate advertising copy for each of the products identified in the images.</p> <p>Two clients are defined for the two different models. This is because the <code>gpt-4-vision-preview</code> model is not compatible with the <code>gpt-4-1106-preview</code> model in terms of their response format.</p> <pre><code>def generate_ad_copy(product: Product) -&gt; AdCopy:\n    \"\"\"\n    Given a product, generate an ad copy for the product.\n    \"\"\"\n\n    logger.info(f\"Generating ad copy for product: {product.name}\")\n\n    return client_copy.chat.completions.create(\n        model=\"gpt-4-1106-preview\",\n        response_model=AdCopy,\n        temperature=0.3,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are an expert marketing assistant for all products. Your task is to generate an advertisement copy for a product using the name, description, and key features.\",\n            },\n            {\"role\": \"user\", \"content\": product.generate_prompt()},\n        ],\n    )\n</code></pre>"},{"location":"examples/image_to_ad_copy/#putting-it-all-together","title":"Putting it all together","text":"<p>Finally, we can put it all together in a single function that takes a list of image URLs and generates advertising copy for the products identified in the images. Please refer to the full code for the complete implementation.</p>"},{"location":"examples/image_to_ad_copy/#input-file","title":"Input file","text":"<p>The input file is currently a list of image URLs, but this trivial to change to any required format.</p> <pre><code>https://contents.mediadecathlon.com/p1279823/9a1c59ad97a4084a346c014740ae4d3ff860ea70b485ee65f34017ff5e9ae5f7/recreational-ice-skates-fit-50-black.jpg?format=auto\nhttps://contents.mediadecathlon.com/p1279822/a730505231dbd6747c14ee93e8f89e824d3fa2a5b885ec26de8d7feb5626638a/recreational-ice-skates-fit-50-black.jpg?format=auto\nhttps://contents.mediadecathlon.com/p2329893/1ed75517602a5e00245b89ab6a1c6be6d8968a5a227c932b10599f857f3ed4cd/mens-hiking-leather-boots-sh-100-x-warm.jpg?format=auto\nhttps://contents.mediadecathlon.com/p2047870/8712c55568dd9928c83b19c6a4067bf161811a469433dc89244f0ff96a50e3e9/men-s-winter-hiking-boots-sh-100-x-warm-grey.jpg?format=auto\n</code></pre> Expand to see the output <p> </p> <pre><code>{\n    \"products\":\n    [\n        {\n            \"name\": \"Ice Skates\",\n            \"key_features\": [\n                \"Lace-up closure\",\n                \"Durable blade\",\n                \"Ankle support\"\n            ],\n            \"description\": \"A pair of ice skates with lace-up closure for secure fit, durable blade for ice skating, and reinforced ankle support.\"\n        },\n        {\n            \"name\": \"Hiking Boots\",\n            \"key_features\": [\n                \"High-top design\",\n                \"Rugged outsole\",\n                \"Water-resistant\"\n            ],\n            \"description\": \"Sturdy hiking boots featuring a high-top design for ankle support, rugged outsole for grip on uneven terrain, and water-resistant construction.\"\n        },\n        {\n            \"name\": \"Winter Boots\",\n            \"key_features\": [\n                \"Insulated lining\",\n                \"Waterproof lower\",\n                \"Slip-resistant sole\"\n            ],\n            \"description\": \"Warm winter boots with insulated lining for cold weather, waterproof lower section to keep feet dry, and a slip-resistant sole for stability.\"\n        }\n    ],\n    \"ad_copies\": [\n        {\n            \"headline\": \"Glide with Confidence - Discover the Perfect Ice Skates!\",\n            \"ad_copy\": \"Step onto the ice with poise and precision with our premium Ice Skates. Designed for both beginners and seasoned skaters, these skates offer a perfect blend of comfort and performance. The lace-up closure ensures a snug fit that keeps you stable as you carve through the ice. With a durable blade that withstands the test of time, you can focus on perfecting your moves rather than worrying about your equipment. The reinforced ankle support provides the necessary protection and aids in preventing injuries, allowing you to skate with peace of mind. Whether you're practicing your spins, jumps, or simply enjoying a leisurely glide across the rink, our Ice Skates are the ideal companion for your ice adventures. Lace up and get ready to experience the thrill of ice skating like never before!\",\n            \"name\": \"Ice Skates\"\n        },\n        {\n            \"headline\": \"Conquer Every Trail with Confidence!\",\n            \"ad_copy\": \"Embark on your next adventure with our top-of-the-line Hiking Boots! Designed for the trail-blazing spirits, these boots boast a high-top design that provides unparalleled ankle support to keep you steady on any path. The rugged outsole ensures a firm grip on the most uneven terrains, while the water-resistant construction keeps your feet dry as you traverse through streams and muddy trails. Whether you're a seasoned hiker or just starting out, our Hiking Boots are the perfect companion for your outdoor escapades. Lace up and step into the wild with confidence - your journey awaits!\",\n            \"name\": \"Hiking Boots\"\n        },\n        {\n            \"headline\": \"Conquer the Cold with Comfort!\",\n            \"ad_copy\": \"Step into the season with confidence in our Winter Boots, the ultimate ally against the chill. Designed for those who don't let the cold dictate their moves, these boots feature an insulated lining that wraps your feet in a warm embrace, ensuring that the biting cold is a worry of the past. But warmth isn't their only virtue. With a waterproof lower section, your feet will remain dry and cozy, come rain, snow, or slush. And let's not forget the slip-resistant sole that stands between you and the treacherous ice, offering stability and peace of mind with every step you take. Whether you're braving a blizzard or just nipping out for a coffee, our Winter Boots are your trusty companions, keeping you warm, dry, and upright. Don't let winter slow you down. Lace up and embrace the elements!\",\n            \"name\": \"Winter Boots\"\n        }\n    ]\n}\n</code></pre>"},{"location":"examples/knowledge_graph/","title":"Visualizing Knowledge Graphs for Complex Topics","text":"<p>In this guide, you'll discover how to visualise a detailed knowledge graph when dealing with complex topics. We'll then move on to iteratively updating our knowledge graph with new information through a series of sequential api calls using only the Instructor library, Pydantic and Graphviz to visualise our graph.</p> <p>Motivation</p> <p>Knowledge graphs offer a visually appealing and coherent way to understand complicated topics like quantum mechanics. By generating these graphs automatically, you can accelerate the learning process and make it easier to digest complex information.</p>"},{"location":"examples/knowledge_graph/#defining-the-structures","title":"Defining the Structures","text":"<p>Let's model a knowledge graph with <code>Node</code> and <code>Edge</code> objects. <code>Node</code> objects represent key concepts or entities, while <code>Edge</code> objects indicate the relationships between them.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List\n\n\nclass Node(BaseModel):\n    id: int\n    label: str\n    color: str\n\n\nclass Edge(BaseModel):\n    source: int\n    target: int\n    label: str\n    color: str = \"black\"\n\n\nclass KnowledgeGraph(BaseModel):\n    nodes: List[Node] = Field(..., default_factory=list)\n    edges: List[Edge] = Field(..., default_factory=list)\n</code></pre>"},{"location":"examples/knowledge_graph/#generating-knowledge-graphs","title":"Generating Knowledge Graphs","text":"<p>The <code>generate_graph</code> function leverages OpenAI's API to generate a knowledge graph based on the input query.</p> <pre><code>from openai import OpenAI\nimport instructor\n\n# Adds response_model to ChatCompletion\n# Allows the return of Pydantic model rather than raw JSON\nclient = instructor.from_openai(OpenAI())\n\n\ndef generate_graph(input) -&gt; KnowledgeGraph:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Help me understand the following by describing it as a detailed knowledge graph: {input}\",\n            }\n        ],\n        response_model=KnowledgeGraph,\n    )  # type: ignore\n</code></pre>"},{"location":"examples/knowledge_graph/#visualizing-the-graph","title":"Visualizing the Graph","text":"<p>The <code>visualize_knowledge_graph</code> function uses the Graphviz library to render the generated knowledge graph.</p> <pre><code>from graphviz import Digraph\n\n\ndef visualize_knowledge_graph(kg: KnowledgeGraph):\n    dot = Digraph(comment=\"Knowledge Graph\")\n\n    # Add nodes\n    for node in kg.nodes:\n        dot.node(str(node.id), node.label, color=node.color)\n\n    # Add edges\n    for edge in kg.edges:\n        dot.edge(str(edge.source), str(edge.target), label=edge.label, color=edge.color)\n\n    # Render the graph\n    dot.render(\"knowledge_graph.gv\", view=True)\n\ngraph = generate_graph(\"Teach me about quantum mechanics\")\nvisualize_knowledge_graph(graph)\n</code></pre> <p></p> <p>This will produce a visual representation of the knowledge graph, stored as \"knowledge_graph.gv\". You can open this file to explore the key concepts and their relationships in quantum mechanics.</p>"},{"location":"examples/knowledge_graph/#iterative-updates","title":"Iterative Updates","text":"<p>Now that we've seen how to generate a knowledge graph from a single input, let's see how we can iteratively update our knowledge graph with new information, or when information does not fit into a single prompt.</p> <p>Let's take an easy example where we want to visualise the combined knowledge graph that the following sentences represent.</p> <pre><code>text_chunks = [\n    \"Jason knows a lot about quantum mechanics. He is a physicist. He is a professor\",\n    \"Professors are smart.\",\n    \"Sarah knows Jason and is a student of his.\",\n    \"Sarah is a student at the University of Toronto. and UofT is in Canada\",\n]\n</code></pre>"},{"location":"examples/knowledge_graph/#updating-our-data-model","title":"Updating Our Data Model","text":"<p>To support our new iterative approach, we need to update our data model. We can do this by adding helper methods <code>update</code> and <code>draw</code> to our Pydantic models. These methods will simplify our code and allow us to easily visualize the knowledge graph.</p> <p>In the <code>KnowledgeGraph</code> class, we have migrated the code from the <code>visualize_knowledge_graph</code> method and added new lists for nodes and edges.</p> <pre><code>class KnowledgeGraph(BaseModel):\n    nodes: Optional[List[Node]] = Field(..., default_factory=list)\n    edges: Optional[List[Edge]] = Field(..., default_factory=list)\n\n    def update(self, other: \"KnowledgeGraph\") -&gt; \"KnowledgeGraph\":\n        \"\"\"Updates the current graph with the other graph, deduplicating nodes and edges.\"\"\"\n        return KnowledgeGraph(\n            nodes=list(set(self.nodes + other.nodes)),\n            edges=list(set(self.edges + other.edges)),\n        )\n\n    def draw(self, prefix: str = None):\n        dot = Digraph(comment=\"Knowledge Graph\")\n\n        for node in self.nodes:  # (1)!\n            dot.node(str(node.id), node.label, color=node.color)\n\n        for edge in self.edges:  # (2)!\n            dot.edge(\n                str(edge.source), str(edge.target), label=edge.label, color=edge.color\n            )\n        dot.render(prefix, format=\"png\", view=True)\n</code></pre> <ol> <li>We iterate through all the nodes in our graph and add them to the graph</li> <li>We iterate through all the edges in our graph and add them to the graph</li> </ol> <p>We can modify our <code>generate_graph</code> function to now take in a list of strings. At each step, it'll extract out the key insights from the sentences in the form of edges and nodes like we've seen before. We can then combine these new edges and nodes with our existing knowledge graph through iterative updates to our graph before arriving at our final result.</p> <pre><code>def generate_graph(input: List[str]) -&gt; KnowledgeGraph:\n    cur_state = KnowledgeGraph()  # (1)!\n    num_iterations = len(input)\n    for i, inp in enumerate(input):\n        new_updates = client.chat.completions.create(\n            model=\"gpt-3.5-turbo-16k\",\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"You are an iterative knowledge graph builder.\n                    You are given the current state of the graph, and you must append the nodes and edges\n                    to it Do not procide any duplcates and try to reuse nodes as much as possible.\"\"\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"Extract any new nodes and edges from the following:\n                    # Part {i}/{num_iterations} of the input:\n\n                    {inp}\"\"\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"Here is the current state of the graph:\n                    {cur_state.model_dump_json(indent=2)}\"\"\",\n                },  # (2)!\n            ],\n            response_model=KnowledgeGraph,\n        )  # type: ignore\n\n        # Update the current state\n        cur_state = cur_state.update(new_updates)  # (3)!\n        cur_state.draw(prefix=f\"iteration_{i}\")\n    return cur_state\n</code></pre> <ol> <li> <p>We first initialise an empty <code>KnowledgeGraph</code>. In this state, it has zero nodes and edges</p> </li> <li> <p>We then add in the current state of the graph into the prompt so that the model knows what new information needs to be added</p> </li> <li> <p>We then update the nodes and edges of our graph with the information that our model has returned before visualizing the new changes</p> </li> </ol> <p>Once we've done this, we can now run this new <code>generate_graph</code> function with the following two lines.</p> <pre><code>graph: KnowledgeGraph = generate_graph(text_chunks)\ngraph.draw(prefix=\"final\")\n</code></pre>"},{"location":"examples/knowledge_graph/#conclusion","title":"Conclusion","text":"<p>We've seen how we can use <code>Instructor</code> to obtain structured outputs from the OpenAI LLM API but you could use that for any of the other open-source models that the library is compatible with. If you enjoy the content or want to try out <code>Instructor</code> check out the github and don't forget to give us a star!</p>"},{"location":"examples/local_classification/","title":"Leveraging Local Models for Classifying Private Data","text":"<p>In this article, we'll show you how to use Llama-cpp-python with instructor for classification. This is a perfect use-case for users who want to ensure that confidential documents are handled securely without ever leaving your own infrastructure.</p>"},{"location":"examples/local_classification/#setup","title":"Setup","text":"<p>Let's start by installing the required libraries in your local python environment. This might take a while since we'll need to build and compile <code>llama-cpp</code> for your specific environment.</p> <pre><code>pip install instructor pydantic\n</code></pre> <p>Next, we'll install <code>llama-cpp-python</code> which is a python package that allows us to use llama-cpp with our python scripts.</p> <p>For this tutorial, we'll be using <code>Mistral-7B-Instruct-v0.2-GGUF</code> by <code>TheBloke</code> to do our function calls. This will require around 6GB of RAM and a GPU.</p> <p>We can install the package by running the following command</p> <pre><code>CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python\n</code></pre> <p>Don't have a GPU?</p> <p>If you don't have a GPU, we recommend using the <code>Qwen2-0.5B-Instruct</code> model instead and compiling llama-cpp-python to use <code>OpenBLAS</code>. This allows you to run the program using your CPU instead.</p> <p>You can compile <code>llama-cpp-python</code> with <code>OpenBLAS</code> support by running the command</p> <pre><code>CMAKE_ARGS=\"-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS\" pip install llama-cpp-python\n</code></pre>"},{"location":"examples/local_classification/#using-llama-cpp-python","title":"Using <code>LLama-cpp-python</code>","text":"<p>Here's an example of how to implement a system for handling confidential document queries using local models:</p> <pre><code>from llama_cpp import Llama\nimport instructor\nfrom pydantic import BaseModel\nfrom enum import Enum\nfrom typing import Optional\n\nllm = Llama.from_pretrained(\n    repo_id=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\", # (1)!\n    filename=\"*Q4_K_M.gguf\",\n    verbose=False, # (2)!\n    n_gpu_layers=-1, # (3)!\n)\n\ncreate = instructor.patch(\n    create=llm.create_chat_completion_openai_v1, #(4)!\n)\n\n# Define query types for document-related inquiries\nclass QueryType(str, Enum):\n    DOCUMENT_CONTENT = \"document_content\"\n    LAST_MODIFIED = \"last_modified\"\n    ACCESS_PERMISSIONS = \"access_permissions\"\n    RELATED_DOCUMENTS = \"related_documents\"\n\n# Define the structure for query responses\nclass QueryResponse(BaseModel):\n    query_type: QueryType\n    response: str\n    additional_info: Optional[str] = None\n\ndef process_confidential_query(query: str) -&gt; QueryResponse:\n    prompt = f\"\"\"Analyze the following confidential document query and provide an appropriate response:\n    Query: {query}\n\n    Determine the type of query (document content, last modified, access permissions, or related documents),\n    provide a response, and include a confidence score and any additional relevant information.\n    Remember, you're handling confidential data, so be cautious about specific details.\n    \"\"\"\n\n    return create(\n        response_model=QueryResponse, #(5)!\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a secure AI assistant trained to handle confidential document queries.\"},\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n    )\n\n\n# Sample confidential document queries\nconfidential_queries = [\n    \"What are the key findings in the Q4 financial report?\",\n    \"Who last accessed the merger proposal document?\",\n    \"What are the access permissions for the new product roadmap?\",\n    \"Are there any documents related to Project X's budget forecast?\",\n    \"When was the board meeting minutes document last updated?\",\n]\n\n# Process each query and print the results\nfor i, query in enumerate(confidential_queries, 1):\n    response:QueryResponse = process_confidential_query(query)\n    print(f\"{query} : {response.query_type}\")\n    \"\"\"\n    #&gt; What are the key findings in the Q4 financial report? : document_content\n    #&gt; Who last accessed the merger proposal document? : access_permissions\n    #&gt; What are the access permissions for the new product roadmap? : access_permissions\n    #&gt; Are there any documents related to Project X's budget forecast? : document_content\n    #&gt; When was the board meeting minutes document last updated? : last_modified\n    \"\"\"\n</code></pre> <ol> <li> <p>We load in the model from Hugging Face and cache it locally. This makes it quick and easy for us to experiment with different model configurations and types.</p> </li> <li> <p>We can set <code>verbose</code> to be <code>True</code> to log out all of the output from <code>llama.cpp</code>. This helps if you're trying to debug specific issues</p> </li> <li> <p>If you have a GPU with limited memory, set <code>n_gpu</code> to a lower number (Eg. 10 ). We've set it here to <code>-1</code> so that all of the model layers are loaded on the GPU by default.</p> </li> <li> <p>Now make sure to patch the client with the <code>create_chat_completion_openai_v1</code> api which is OpenAI compatible</p> </li> <li> <p>Pass in the response model as a parameter just like any other inference client we support</p> </li> </ol>"},{"location":"examples/local_classification/#conclusion","title":"Conclusion","text":"<p><code>instructor</code> provides a robust solution for organizations needing to handle confidential document queries locally. By processing these queries on your own hardware, you can leverage advanced AI capabilities while maintaining the highest standards of data privacy and security.</p> <p>But this goes far beyond just simple confidential documents, using local models unlocks a whole new world of interesting use-cases, fine-tuned specialist models and more!</p>"},{"location":"examples/mistral/","title":"Structured Outputs using Mistral","text":"<p>You can now also use mistralai models for inference by using from_mistral.</p> <p>The examples are using mistral-large-latest.</p>"},{"location":"examples/mistral/#mistralai-api","title":"MistralAI API","text":"<p>To use mistral you need to obtain a mistral API key. Goto mistralai click on Build Now and login. Select API Keys from the left menu and then select  Create API key to create a new key.</p>"},{"location":"examples/mistral/#use-example","title":"Use example","text":"<p>Some pip packages need to be installed to use the example: <pre><code>pip install instructor mistralai pydantic\n</code></pre> You need to export the mistral API key: <pre><code>export MISTRAL_API_KEY=&lt;your-api-key&gt;\n</code></pre></p> <p>An example: <pre><code>import os\nfrom pydantic import BaseModel, Field\nfrom typing import List\nfrom mistralai.client import MistralClient\nfrom instructor import from_mistral, Mode\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\n\n# enables `response_model` in chat call\nclient = MistralClient(api_key=os.environ.get(\"MISTRAL_API_KEY\"))\n\ninstructor_client = from_mistral(client=client, model=\"mistral-large-latest\", \n                                 mode=Mode.MISTRAL_TOOLS, max_tokens=1000)\n\nresp = instructor_client.messages.create(\n    response_model=UserDetails,\n    messages=[{\"role\": \"user\", \"content\": \"Jason is 10\"}],\n    temperature=0,\n)\n\nprint(resp)\n\n# output: UserDetails(name='Jason', age=10)\n</code></pre></p>"},{"location":"examples/moderation/","title":"OpenAI Moderation","text":"<p>This example uses OpenAI's moderation endpoint to check content compliance with OpenAI's usage policies. It can identify and filter harmful content that violates the policies.</p> <p>The model flags content and classifies it into categories including hate, harassment, self-harm, sexual content, and violence. Each category has subcategories for detailed classification.</p> <p>This validator is to be used for monitoring OpenAI API inputs and outputs, other use cases are currently not allowed.</p>"},{"location":"examples/moderation/#incorporating-openai-moderation-validator","title":"Incorporating OpenAI moderation validator","text":"<p>The following code defines a function to validate content using OpenAI's Moderation endpoint. The <code>AfterValidator</code> is used to apply OpenAI's moderation after the compute. This moderation checks if the content complies with OpenAI's usage policies and flags any harmful content. Here's how it works:</p> <ol> <li> <p>Generate the OpenAI client and patch it with the <code>instructor</code>. Patching is not strictly necessary for this example but its a good idea to always patch the client to leverage the full <code>instructor</code> functionality.</p> </li> <li> <p>Annotate our <code>message</code> field with <code>AfterValidator(openai_moderation(client=client))</code>. This means that after the <code>message</code> is computed, it will be passed to the <code>openai_moderation</code> function for validation.</p> </li> </ol> <pre><code>import instructor\n\nfrom instructor import openai_moderation\n\nfrom typing_extensions import Annotated\nfrom pydantic import BaseModel, AfterValidator\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass Response(BaseModel):\n    message: Annotated[str, AfterValidator(openai_moderation(client=client))]\n\ntry:\n    Response(message=\"I want to make them suffer the consequences\")\nexcept Exception as e:\n    print(e)\n    \"\"\"\n    1 validation error for Response\n    message\n      Value error, `I want to make them suffer the consequences` was flagged for violence, violence/threat [type=value_error, input_value='I want to make them suffer the consequences', input_type=str]\n    \"\"\"\n\ntry:\n    Response(message=\"I want to hurt myself.\")\nexcept Exception as e:\n    print(e)\n    \"\"\"\n    1 validation error for Response\n    message\n      Value error, `I want to hurt myself` was flagged for self_harm, self_harm_intent, violence, self-harm, self-harm/intent [type=value_error, input_value='I want to hurt myself', input_type=str]\n    \"\"\"\n</code></pre>"},{"location":"examples/ollama/","title":"Structured Outputs with Ollama","text":"<p>Open-source LLMS are gaining popularity, and with the release of Ollama's OpenAI compatibility layer, it has become possible to obtain structured outputs using JSON schema.</p> <p>By the end of this blog post, you will learn how to effectively utilize instructor with Ollama. But before we proceed, let's first explore the concept of patching.</p>"},{"location":"examples/ollama/#patching","title":"Patching","text":"<p>Instructor's patch enhances an openai api with the following features:</p> <ul> <li><code>response_model</code> in <code>create</code> calls that returns a pydantic model</li> <li><code>max_retries</code> in <code>create</code> calls that retries the call if it fails by using a backoff strategy</li> </ul> <p>Learn More</p> <p>To learn more, please refer to the docs. To understand the benefits of using Pydantic with Instructor, visit the tips and tricks section of the why use Pydantic page.</p>"},{"location":"examples/ollama/#ollama","title":"Ollama","text":"<p>Start by downloading Ollama, and then pull a model such as Llama 3 or Mistral.</p> <p>Make sure you update your <code>ollama</code> to the latest version!</p> <pre><code>ollama pull llama3\n</code></pre> <pre><code>from openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\nimport instructor\n\n\nclass Character(BaseModel):\n    name: str\n    age: int\n    fact: List[str] = Field(..., description=\"A list of facts about the character\")\n\n\n# enables `response_model` in create call\nclient = instructor.from_openai(\n    OpenAI(\n        base_url=\"http://localhost:11434/v1\",\n        api_key=\"ollama\",  # required, but unused\n    ),\n    mode=instructor.Mode.JSON,\n)\n\nresp = client.chat.completions.create(\n    model=\"llama3\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Tell me about the Harry Potter\",\n        }\n    ],\n    response_model=Character,\n)\nprint(resp.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"Harry James Potter\",\n  \"age\": 37,\n  \"fact\": [\n    \"He is the chosen one.\",\n    \"He has a lightning-shaped scar on his forehead.\",\n    \"He is the son of James and Lily Potter.\",\n    \"He attended Hogwarts School of Witchcraft and Wizardry.\",\n    \"He is a skilled wizard and sorcerer.\",\n    \"He fought against Lord Voldemort and his followers.\",\n    \"He has a pet owl named Snowy.\"\n  ]\n}\n\"\"\"\n</code></pre>"},{"location":"examples/open_source/","title":"Instructor with open source models","text":"<p>Instructor works with Open source model providers that support the OpenAI API chat endpoint</p> <p>See examples README here</p>"},{"location":"examples/open_source/#currently-tested-open-source-model-providers","title":"Currently tested open source model providers","text":"<ul> <li>OpenRouter</li> <li>Perplexity</li> <li>RunPod TheBloke LLMs **</li> </ul> <p>** This utilizes text-generation-webui w/ Openai plugin under the hood. </p>"},{"location":"examples/pii/","title":"PII Data Extraction and Scrubbing","text":""},{"location":"examples/pii/#overview","title":"Overview","text":"<p>This example demonstrates the usage of OpenAI's ChatCompletion model for the extraction and scrubbing of Personally Identifiable Information (PII) from a document. The code defines Pydantic models to manage the PII data and offers methods for both extraction and sanitation.</p>"},{"location":"examples/pii/#defining-the-structures","title":"Defining the Structures","text":"<p>First, Pydantic models are defined to represent the PII data and the overall structure for PII data extraction.</p> <pre><code>from typing import List\nfrom pydantic import BaseModel\n\n\n# Define Schemas for PII data\nclass Data(BaseModel):\n    index: int\n    data_type: str\n    pii_value: str\n\n\nclass PIIDataExtraction(BaseModel):\n    \"\"\"\n    Extracted PII data from a document, all data_types should try to have consistent property names\n    \"\"\"\n\n    private_data: List[Data]\n\n    def scrub_data(self, content: str) -&gt; str:\n        \"\"\"\n        Iterates over the private data and replaces the value with a placeholder in the form of\n        &lt;{data_type}_{i}&gt;\n        \"\"\"\n        for i, data in enumerate(self.private_data):\n            content = content.replace(data.pii_value, f\"&lt;{data.data_type}_{i}&gt;\")\n        return content\n</code></pre>"},{"location":"examples/pii/#extracting-pii-data","title":"Extracting PII Data","text":"<p>The OpenAI API is utilized to extract PII information from a given document.</p> <pre><code>from openai import OpenAI\nimport instructor\n\nclient = instructor.from_openai(OpenAI())\n\nEXAMPLE_DOCUMENT = \"\"\"\n# Fake Document with PII for Testing PII Scrubbing Model\n# (The content here)\n\"\"\"\n\npii_data = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=PIIDataExtraction,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a world class PII scrubbing model, Extract the PII data from the following document\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": EXAMPLE_DOCUMENT,\n        },\n    ],\n)  # type: ignore\n\nprint(\"Extracted PII Data:\")\nprint(pii_data.model_dump_json())\n</code></pre>"},{"location":"examples/pii/#output-of-extracted-pii-data","title":"Output of Extracted PII Data","text":"<pre><code>{\n  \"private_data\": [\n    {\n      \"index\": 0,\n      \"data_type\": \"date\",\n      \"pii_value\": \"01/02/1980\"\n    },\n    {\n      \"index\": 1,\n      \"data_type\": \"ssn\",\n      \"pii_value\": \"123-45-6789\"\n    },\n    {\n      \"index\": 2,\n      \"data_type\": \"email\",\n      \"pii_value\": \"john.doe@email.com\"\n    },\n    {\n      \"index\": 3,\n      \"data_type\": \"phone\",\n      \"pii_value\": \"555-123-4567\"\n    },\n    {\n      \"index\": 4,\n      \"data_type\": \"address\",\n      \"pii_value\": \"123 Main St, Springfield, IL, 62704\"\n    }\n  ]\n}\n</code></pre>"},{"location":"examples/pii/#scrubbing-pii-data","title":"Scrubbing PII Data","text":"<p>After extracting the PII data, the <code>scrub_data</code> method is used to sanitize the document.</p> <pre><code>print(\"Scrubbed Document:\")\nprint(pii_data.scrub_data(EXAMPLE_DOCUMENT))\n</code></pre>"},{"location":"examples/pii/#output-of-scrubbed-document","title":"Output of Scrubbed Document","text":"<pre><code># Fake Document with PII for Testing PII Scrubbing Model\n\n## Personal Story\n\nJohn Doe was born on &lt;date_0&gt;. His social security number is &lt;ssn_1&gt;. He has been using the email address &lt;email_2&gt; for years, and he can always be reached at &lt;phone_3&gt;.\n\n## Residence\n\nJohn currently resides at &lt;address_4&gt;. He's been living there for about 5 years now.\n</code></pre>"},{"location":"examples/planning-tasks/","title":"Example: Planning and Executing a Query Plan","text":"<p>This example demonstrates how to use the OpenAI Function Call ChatCompletion model to plan and execute a query plan in a question-answering system. By breaking down a complex question into smaller sub-questions with defined dependencies, the system can systematically gather the necessary information to answer the main question.</p> <p>Motivation</p> <p>The goal of this example is to showcase how query planning can be used to handle complex questions, facilitate iterative information gathering, automate workflows, and optimize processes. By leveraging the OpenAI Function Call model, you can design and execute a structured plan to find answers effectively.</p> <p>Use Cases:</p> <ul> <li>Complex question answering</li> <li>Iterative information gathering</li> <li>Workflow automation</li> <li>Process optimization</li> </ul> <p>With the OpenAI Function Call model, you can customize the planning process and integrate it into your specific application to meet your unique requirements.</p>"},{"location":"examples/planning-tasks/#defining-the-structures","title":"Defining the Structures","text":"<p>Let's define the necessary Pydantic models to represent the query plan and the queries.</p> <pre><code>import enum\nfrom typing import List\nfrom pydantic import Field, BaseModel\n\n\nclass QueryType(str, enum.Enum):\n    \"\"\"Enumeration representing the types of queries that can be asked to a question answer system.\"\"\"\n\n    SINGLE_QUESTION = \"SINGLE\"\n    MERGE_MULTIPLE_RESPONSES = \"MERGE_MULTIPLE_RESPONSES\"\n\n\nclass Query(BaseModel):\n    \"\"\"Class representing a single question in a query plan.\"\"\"\n\n    id: int = Field(..., description=\"Unique id of the query\")\n    question: str = Field(\n        ...,\n        description=\"Question asked using a question answering system\",\n    )\n    dependencies: List[int] = Field(\n        default_factory=list,\n        description=\"List of sub questions that need to be answered before asking this question\",\n    )\n    node_type: QueryType = Field(\n        default=QueryType.SINGLE_QUESTION,\n        description=\"Type of question, either a single question or a multi-question merge\",\n    )\n\n\nclass QueryPlan(BaseModel):\n    \"\"\"Container class representing a tree of questions to ask a question answering system.\"\"\"\n\n    query_graph: List[Query] = Field(\n        ..., description=\"The query graph representing the plan\"\n    )\n\n    def _dependencies(self, ids: List[int]) -&gt; List[Query]:\n        \"\"\"Returns the dependencies of a query given their ids.\"\"\"\n        return [q for q in self.query_graph if q.id in ids]\n</code></pre> <p>Graph Generation</p> <p>Notice that this example produces a flat list of items with dependencies that resemble a graph, while pydantic allows for recursive definitions, it's much easier and less confusing for the model to generate flat schemas rather than recursive schemas. If you want to see a recursive example, see recursive schemas</p>"},{"location":"examples/planning-tasks/#planning-a-query-plan","title":"Planning a Query Plan","text":"<p>Now, let's demonstrate how to plan and execute a query plan using the defined models and the OpenAI API.</p> <pre><code>import instructor\nfrom openai import OpenAI\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.from_openai(OpenAI())\n\n\ndef query_planner(question: str) -&gt; QueryPlan:\n    PLANNING_MODEL = \"gpt-4-0613\"\n\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a world class query planning algorithm capable ofbreaking apart questions into its dependency queries such that the answers can be used to inform the parent question. Do not answer the questions, simply provide a correct compute graph with good specific questions to ask and relevant dependencies. Before you call the function, think step-by-step to get a better understanding of the problem.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"Consider: {question}\\nGenerate the correct query plan.\",\n        },\n    ]\n\n    root = client.chat.completions.create(\n        model=PLANNING_MODEL,\n        temperature=0,\n        response_model=QueryPlan,\n        messages=messages,\n        max_tokens=1000,\n    )\n    return root\n</code></pre> <pre><code>plan = query_planner(\n    \"What is the difference in populations of Canada and the Jason's home country?\"\n)\nplan.model_dump()\n</code></pre> <p>No RAG</p> <p>While we build the query plan in this example, we do not propose a method to actually answer the question. You can implement your own answer function that perhaps makes a retrieval and calls openai for retrieval augmented generation. That step would also make use of function calls but goes beyond the scope of this example.</p> <pre><code>{\n    \"query_graph\": [\n        {\n            \"dependencies\": [],\n            \"id\": 1,\n            \"node_type\": \"SINGLE\",\n            \"question\": \"Identify Jason's home country\",\n        },\n        {\n            \"dependencies\": [],\n            \"id\": 2,\n            \"node_type\": \"SINGLE\",\n            \"question\": \"Find the population of Canada\",\n        },\n        {\n            \"dependencies\": [1],\n            \"id\": 3,\n            \"node_type\": \"SINGLE\",\n            \"question\": \"Find the population of Jason's home country\",\n        },\n        {\n            \"dependencies\": [2, 3],\n            \"id\": 4,\n            \"node_type\": \"SINGLE\",\n            \"question\": \"Calculate the difference in populations between Canada and Jasons home country\",\n        },\n    ]\n}\n</code></pre> <p>In the above code, we define a <code>query_planner</code> function that takes a question as input and generates a query plan using the OpenAI API.</p>"},{"location":"examples/planning-tasks/#conclusion","title":"Conclusion","text":"<p>In this example, we demonstrated how to use the OpenAI Function Call <code>ChatCompletion</code> model to plan and execute a query plan using a question-answering system. We defined the necessary structures using Pydantic, created a query planner function.</p> <p>If you want to see multiple versions of this style of code, please visit:</p> <ol> <li>query planning example</li> <li>task planning with topo sort</li> </ol> <p>Feel free to modify the code to fit your specific use case and explore other possibilities of using the OpenAI Function Call model to plan and execute complex workflows.</p>"},{"location":"examples/search/","title":"Example: Segmenting Search Queries","text":"<p>In this example, we will demonstrate how to leverage the <code>MultiTask</code> and <code>enum.Enum</code> features of OpenAI Function Call to segment search queries. We will define the necessary structures using Pydantic and demonstrate how segment queries into multiple sub queries and execute them in parallel with <code>asyncio</code>.</p> <p>Motivation</p> <p>Extracting a list of tasks from text is a common use case for leveraging language models. This pattern can be applied to various applications, such as virtual assistants like Siri or Alexa, where understanding user intent and breaking down requests into actionable tasks is crucial. In this example, we will demonstrate how to use OpenAI Function Call to segment search queries and execute them in parallel.</p>"},{"location":"examples/search/#structure-of-the-data","title":"Structure of the Data","text":"<p>The <code>Search</code> class is a Pydantic model that defines the structure of the search query. It has three fields: <code>title</code>, <code>query</code>, and <code>type</code>. The <code>title</code> field is the title of the request, the <code>query</code> field is the query to search for relevant content, and the <code>type</code> field is the type of search. The <code>execute</code> method is used to execute the search query.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom typing import Iterable, Literal\nfrom pydantic import BaseModel, Field\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.from_openai(OpenAI())\n\nclass Search(BaseModel):\n    query: str = Field(..., description=\"Query to search for relevant content\")\n    type: Literal[\"web\", \"image\", \"video\"] = Field(..., description=\"Type of search\")\n\n    async def execute(self):\n        print(\n            f\"Searching for `{self.title}` with query `{self.query}` using `{self.type}`\"\n        )\n\n\ndef segment(data: str) -&gt; Search:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo-0613\",\n        response_model=Iterable[Search],\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Consider the data below: '\\n{data}' and segment it into multiple search queries\",\n            },\n        ],\n        max_tokens=1000,\n    )\n\nfor search in segment(\"Search for a picture of a cat and a video of a dog\"):\n    print(search.model_dump_json())\n    \"\"\"\n    {\n        \"query\": \"a picture of a cat\",\n        \"type\": \"image\"\n    }\n    {\n        \"query\": \"a video of a dog\",\n        \"type\": \"video\"\n    }\n    \"\"\"\n    }\n</code></pre>"},{"location":"examples/self_critique/","title":"Self-Correction with <code>llm_validator</code>","text":""},{"location":"examples/self_critique/#introduction","title":"Introduction","text":"<p>This guide demonstrates how to use <code>llm_validator</code> for implementing self-healing. The objective is to showcase how an instructor can self-correct by using validation errors and helpful error messages.</p> <pre><code>from openai import OpenAI\nfrom pydantic import BaseModel\nimport instructor\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.from_openai(OpenAI())\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: str\n\nquestion = \"What is the meaning of life?\"\ncontext = \"The according to the devil the meaning of live is to live a life of sin and debauchery.\"\n\nqa: QuestionAnswer = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=QuestionAnswer,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"using the context: {context}\\n\\nAnswer the following question: {question}\",\n        },\n    ],\n)\n</code></pre>"},{"location":"examples/self_critique/#output-before-validation","title":"Output Before Validation","text":"<p>While it calls out the objectionable content, it doesn't provide any details on how to correct it.</p> <pre><code>{\n  \"question\": \"What is the meaning of life?\",\n  \"answer\": \"The meaning of life, according to the context, is to live a life of sin and debauchery.\"\n}\n</code></pre>"},{"location":"examples/self_critique/#adding-custom-validation","title":"Adding Custom Validation","text":"<p>By adding a validator to the <code>answer</code> field, we can try to catch the issue and correct it. Lets integrate <code>llm_validator</code> into the model and see the error message. Its important to note that you can use all of pydantic's validators as you would normally as long as you raise a <code>ValidationError</code> with a helpful error message as it will be used as part of the self correction prompt.</p> <pre><code>from pydantic import BaseModel, BeforeValidator\nfrom typing_extensions import Annotated\nfrom instructor import llm_validator\nfrom openai import OpenAI\nimport instructor\n\nclient = instructor.from_openai(OpenAI())\n\nclass QuestionAnswerNoEvil(BaseModel):\n    question: str\n    answer: Annotated[\n        str,\n        BeforeValidator(\n            llm_validator(\"don't say objectionable things\", client=client, allow_override=True)\n        ),\n    ]\n\n\ntry:\n    qa: QuestionAnswerNoEvil = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=QuestionAnswerNoEvil,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"using the context: {context}\\n\\nAnswer the following question: {question}\",\n            },\n        ],\n    )\nexcept Exception as e:\n    print(e)\n</code></pre>"},{"location":"examples/self_critique/#output-after-validation","title":"Output After Validation","text":"<p>Now, we throw validation error that its objectionable and provide a helpful error message.</p> <pre><code>1 validation error for QuestionAnswerNoEvil\nanswer\n  Assertion failed, The statement promotes sin and debauchery, which is objectionable.\n</code></pre>"},{"location":"examples/self_critique/#retrying-with-corrections","title":"Retrying with Corrections","text":"<p>By adding the <code>max_retries</code> parameter, we can retry the request with corrections. and use the error message to correct the output.</p> <pre><code>qa: QuestionAnswerNoEvil = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=QuestionAnswerNoEvil,\n    max_retries=2,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"using the context: {context}\\n\\nAnswer the following question: {question}\",\n        },\n    ],\n)\n</code></pre>"},{"location":"examples/self_critique/#final-output","title":"Final Output","text":"<p>Now, we get a valid response that is not objectionable!</p> <pre><code>{\n  \"question\": \"What is the meaning of life?\",\n  \"answer\": \"The meaning of life is subjective and can vary depending on individual beliefs and philosophies.\"\n}\n</code></pre>"},{"location":"examples/sqlmodel/","title":"Integrating Instructor with SQLModel","text":"<p>SQLModel is a library designed for interacting with SQL databases from Python code using Python objects. <code>SQLModel</code> is based on <code>Pydantic</code> and <code>SQLAlchemy</code> and was created by tiangolo who also developed <code>FastAPI</code>. So you can expect seamless integration across all these libraries, reducing code duplicating and improving your developer experience. </p>"},{"location":"examples/sqlmodel/#example-adding-responses-from-instructor-directly-to-your-db","title":"Example: Adding responses from Instructor directly to your DB","text":""},{"location":"examples/sqlmodel/#defining-the-models","title":"Defining the Models","text":"<p>First we'll define a model that will serve as a table for our database and the structure of our outputs from <code>Instructor</code></p> <p>Model Definition</p> <p>You'll need to subclass your models with both <code>SQLModel</code> and <code>instructor.OpenAISchema</code> for them to work with SQLModel</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom typing import Optional\nfrom sqlmodel import Field, SQLModel, create_engine\n\n\nclass Hero(SQLModel, instructor.OpenAISchema, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    name: str\n    secret_name: str\n    age: Optional[int] = None\n</code></pre>"},{"location":"examples/sqlmodel/#generating-a-record","title":"Generating a record","text":"<p>The <code>create_hero</code> function will query <code>OpenAI</code> for a <code>Hero</code> record</p> <pre><code>client = instructor.from_openai(OpenAI())\n\ndef create_hero() -&gt; Hero:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Hero,\n        messages=[\n            {\"role\": \"user\", \"content\": \"Make a new superhero\"},\n        ],\n    )\n</code></pre>"},{"location":"examples/sqlmodel/#inserting-the-response-into-the-db","title":"Inserting the response into the DB","text":"<pre><code>engine = create_engine(\"sqlite:///database.db\")\nSQLModel.metadata.create_all(engine)\n\nhero = create_hero()\nprint(hero.model_dump())\n    \"\"\"\n    {'name': 'SuperNova', 'secret_name': 'Mia Thompson', 'age': 28, 'id': None}\n    \"\"\"\n\nwith Session(engine) as session:\n    session.add(hero)\n    session.commit()\n</code></pre> <p>And there you have it! You can now use the same models for your database and <code>Instructor</code> enabling them work seamlessly! Also checkout the FastAPI guide to see how you can use these models in an API as well. </p>"},{"location":"examples/watsonx/","title":"Structured Outputs with IBM watsonx.ai","text":"<p>You can use IBM watsonx.ai for inference using LiteLLM.</p>"},{"location":"examples/watsonx/#prerequisites","title":"Prerequisites","text":"<ul> <li>IBM Cloud Account</li> <li>API Key from IBM Cloud IAM: https://cloud.ibm.com/iam/apikeys</li> <li>Project ID (from watsonx.ai instance URL: https://dataplatform.cloud.ibm.com/projects//)"},{"location":"examples/watsonx/#install","title":"Install","text":"<pre><code>poetry install instructor --with litellm\n</code></pre>"},{"location":"examples/watsonx/#example","title":"Example","text":"<pre><code>import os\n\nimport litellm\nfrom litellm import completion\nfrom pydantic import BaseModel, Field\n\nimport instructor\nfrom instructor import Mode\n\nlitellm.drop_params = True  # watsonx.ai doesn't support `json_mode`\n\nos.environ[\"WATSONX_URL\"] = \"https://us-south.ml.cloud.ibm.com\"\nos.environ[\"WATSONX_API_KEY\"] = \"\"\nos.environ[\"WATSONX_PROJECT_ID\"] = \"\"\n# Additional options: https://docs.litellm.ai/docs/providers/watsonx\n\n\nclass Company(BaseModel):\n    name: str = Field(description=\"name of the company\")\n    year_founded: int = Field(description=\"year the company was founded\")\n\n\nclient = instructor.from_litellm(completion, mode=Mode.JSON)\n\nresp = client.chat.completions.create(\n    model=\"watsonx/meta-llama/llama-3-8b-instruct\",\n    max_tokens=1024,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"\\\nGiven the following text, create a Company object:\n\nIBM was founded in 1911 as the Computing-Tabulating-Recording Company (CTR), a holding company of manufacturers of record-keeping and measuring systems.\n\"\"\",\n        }\n    ],\n    project_id=os.environ[\"WATSONX_PROJECT_ID\"],\n    response_model=Company,\n)\n\nprint(resp.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"IBM\",\n  \"year_founded\": 1911\n}\n\"\"\"\n</code></pre>"},{"location":"hub/","title":"Instructor Hub","text":"<p>Welcome to instructor hub, the goal of this project is to provide a set of tutorials and examples to help you get started, and allow you to pull in the code you need to get started with <code>instructor</code></p> <p>Make sure you're using the latest version of <code>instructor</code> by running:</p> <pre><code>pip install -U instructor\n</code></pre>"},{"location":"hub/#contributing","title":"Contributing","text":"<p>We welcome contributions to the instructor hub, if you have a tutorial or example you'd like to add, please open a pull request in <code>docs/hub</code> and we'll review it.</p> <ol> <li>The code must be in a single file</li> <li>Make sure that its referenced in the <code>mkdocs.yml</code></li> <li>Make sure that the code is unit tested.</li> </ol>"},{"location":"hub/#using-pytest_examples","title":"Using pytest_examples","text":"<p>By running the following command you can run the tests and update the examples. This ensures that the examples are always up to date. Linted correctly and that the examples are working, make sure to include a <code>if __name__ == \"__main__\":</code> block in your code and add some asserts to ensure that the code is working.</p> <pre><code>poetry run pytest tests/openai/docs/test_hub.py --update-examples\n</code></pre>"},{"location":"hub/#cli-usage","title":"CLI Usage","text":"<p>Instructor hub comes with a command line interface (CLI) that allows you to view and interact with the tutorials and examples and allows you to pull in the code you need to get started with the API.</p>"},{"location":"hub/#list-cookbooks","title":"List Cookbooks","text":"<p>By running <code>instructor hub list</code> you can see all the available tutorials and examples. By clickony (doc) you can see the full tutorial back on this website.</p> <pre><code>$ instructor hub list --sort\n</code></pre> hub_id slug title n_downloads 2 multiple_classification (doc) Multiple Classification Model 24 1 single_classification (doc) Single Classification Model 2"},{"location":"hub/#searching-for-cookbooks","title":"Searching for Cookbooks","text":"<p>You can search for a tutorial by running <code>instructor hub list -q &lt;QUERY&gt;</code>. This will return a list of tutorials that match the query.</p> <pre><code>$ instructor hub list -q multi\n</code></pre> hub_id slug title n_downloads 2 multiple_classification (doc) Multiple Classification Model 24"},{"location":"hub/#reading-a-cookbook","title":"Reading a Cookbook","text":"<p>To read a tutorial, you can run <code>instructor hub pull --id &lt;hub_id&gt; --page</code> to see the full tutorial in the terminal. You can use <code>j,k</code> to scroll up and down, and <code>q</code> to quit. You can also run it without <code>--page</code> to print the tutorial to the terminal.</p> <pre><code>$ instructor hub pull --id 2 --page\n</code></pre>"},{"location":"hub/#pulling-in-code","title":"Pulling in Code","text":"<p>You can pull in the code with <code>--py --output=&lt;filename&gt;</code> to save the code to a file, or you cal also run it without <code>--output</code> to print the code to the terminal.</p> <pre><code>$ instructor hub pull --id 2 --py --output=run.py\n$ instructor hub pull --id 2 --py &gt; run.py\n</code></pre> <p>You can run the code instantly if you <code>|</code> it to <code>python</code>:</p> <pre><code>$ instructor hub pull --id 2 --py | python\n</code></pre>"},{"location":"hub/#call-for-contributions","title":"Call for Contributions","text":"<p>We're looking for a bunch more hub examples, if you have a tutorial or example you'd like to add, please open a pull request in <code>docs/hub</code> and we'll review it.</p> <ul> <li> Converting the cookbooks to the new format</li> <li> Validator examples</li> <li> Data extraction examples</li> <li> Streaming examples (Iterable and Partial)</li> <li> Batch Parsing examples</li> <li> Query Expansion examples</li> <li> Batch Data Processing examples</li> <li> Batch Data Processing examples with Cache</li> </ul>"},{"location":"hub/action_items/","title":"Extracting Action Items from Meeting Transcripts","text":"<p>In this guide, we'll walk through how to extract action items from meeting transcripts using OpenAI's API and Pydantic. This use case is essential for automating project management tasks, such as task assignment and priority setting.</p> <p>If you want to try outs via <code>instructor hub</code>, you can pull it by running</p> <pre><code>instructor hub pull --slug action_items --py &gt; action_items.py\n</code></pre> <p>For multi-label classification, we introduce a new enum class and a different Pydantic model to handle multiple labels.</p> <p>Motivation</p> <p>Significant amount of time is dedicated to meetings, where action items are generated as the actionable outcomes of these discussions. Automating the extraction of action items can save time and guarantee that no critical tasks are overlooked.</p>"},{"location":"hub/action_items/#defining-the-structures","title":"Defining the Structures","text":"<p>We'll model a meeting transcript as a collection of <code>Ticket</code> objects, each representing an action item. Every <code>Ticket</code> can have multiple <code>Subtask</code> objects, representing smaller, manageable pieces of the main task.</p>"},{"location":"hub/action_items/#extracting-action-items","title":"Extracting Action Items","text":"<p>To extract action items from a meeting transcript, we use the <code>generate</code> function. It calls OpenAI's API, processes the text, and returns a set of action items modeled as <code>ActionItems</code>.</p>"},{"location":"hub/action_items/#evaluation-and-testing","title":"Evaluation and Testing","text":"<p>To test the <code>generate</code> function, we provide it with a sample transcript, and then print the JSON representation of the extracted action items.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom typing import Iterable, List, Optional\nfrom enum import Enum\nfrom pydantic import BaseModel\n\n\nclass PriorityEnum(str, Enum):\n    high = \"High\"\n    medium = \"Medium\"\n    low = \"Low\"\n\n\nclass Subtask(BaseModel):\n    \"\"\"Correctly resolved subtask from the given transcript\"\"\"\n\n    id: int\n    name: str\n\n\nclass Ticket(BaseModel):\n    \"\"\"Correctly resolved ticket from the given transcript\"\"\"\n\n    id: int\n    name: str\n    description: str\n    priority: PriorityEnum\n    assignees: List[str]\n    subtasks: Optional[List[Subtask]]\n    dependencies: Optional[List[int]]\n\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.from_openai(OpenAI())\n\n\ndef generate(data: str) -&gt; Iterable[Ticket]:\n    return client.chat.completions.create(\n        model=\"gpt-4\",\n        response_model=Iterable[Ticket],\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"The following is a transcript of a meeting...\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Create the action items for the following transcript: {data}\",\n            },\n        ],\n    )\n\n\nprediction = generate(\n    \"\"\"\nAlice: Hey team, we have several critical tasks we need to tackle for the upcoming release. First, we need to work on improving the authentication system. It's a top priority.\n\nBob: Got it, Alice. I can take the lead on the authentication improvements. Are there any specific areas you want me to focus on?\n\nAlice: Good question, Bob. We need both a front-end revamp and back-end optimization. So basically, two sub-tasks.\n\nCarol: I can help with the front-end part of the authentication system.\n\nBob: Great, Carol. I'll handle the back-end optimization then.\n\nAlice: Perfect. Now, after the authentication system is improved, we have to integrate it with our new billing system. That's a medium priority task.\n\nCarol: Is the new billing system already in place?\n\nAlice: No, it's actually another task. So it's a dependency for the integration task. Bob, can you also handle the billing system?\n\nBob: Sure, but I'll need to complete the back-end optimization of the authentication system first, so it's dependent on that.\n\nAlice: Understood. Lastly, we also need to update our user documentation to reflect all these changes. It's a low-priority task but still important.\n\nCarol: I can take that on once the front-end changes for the authentication system are done. So, it would be dependent on that.\n\nAlice: Sounds like a plan. Let's get these tasks modeled out and get started.\"\"\"\n)\n</code></pre>"},{"location":"hub/action_items/#visualizing-the-tasks","title":"Visualizing the tasks","text":"<p>In order to quickly visualize the data we used code interpreter to create a graphviz export of the json version of the ActionItems array.</p> <p></p> <pre><code>[\n  {\n    \"id\": 1,\n    \"name\": \"Improve Authentication System\",\n    \"description\": \"Revamp the front-end and optimize the back-end of the authentication system\",\n    \"priority\": \"High\",\n    \"assignees\": [\"Bob\", \"Carol\"],\n    \"subtasks\": [\n      {\n        \"id\": 2,\n        \"name\": \"Front-end Revamp\"\n      },\n      {\n        \"id\": 3,\n        \"name\": \"Back-end Optimization\"\n      }\n    ],\n    \"dependencies\": []\n  },\n  {\n    \"id\": 4,\n    \"name\": \"Integrate Authentication System with Billing System\",\n    \"description\": \"Integrate the improved authentication system with the new billing system\",\n    \"priority\": \"Medium\",\n    \"assignees\": [\"Bob\"],\n    \"subtasks\": [],\n    \"dependencies\": [1]\n  },\n  {\n    \"id\": 5,\n    \"name\": \"Update User Documentation\",\n    \"description\": \"Update the user documentation to reflect the changes in the authentication system\",\n    \"priority\": \"Low\",\n    \"assignees\": [\"Carol\"],\n    \"subtasks\": [],\n    \"dependencies\": [2]\n  }\n]\n</code></pre> <p>In this example, the <code>generate</code> function successfully identifies and segments the action items, assigning them priorities, assignees, subtasks, and dependencies as discussed in the meeting.</p> <p>By automating this process, you can ensure that important tasks and details are not lost in the sea of meeting minutes, making project management more efficient and effective.</p>"},{"location":"hub/anthropic/","title":"Anthropic","text":"<p>Now that we have a Anthropic client, we can use it with the <code>instructor</code> client to make requests.</p> <pre><code>pip install anthropic\n</code></pre> <pre><code>from pydantic import BaseModel\nfrom typing import List\nimport anthropic\nimport instructor\n\n# Patching the Anthropics client with the instructor for enhanced capabilities\nclient = instructor.from_anthropic(\n    anthropic.Anthropic(),\n)\n\n\nclass Properties(BaseModel):\n    name: str\n    value: str\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n    properties: List[Properties]\n\n\n# client.messages.create will also work due to the instructor client\nuser_response = client.chat.completions.create(\n    model=\"claude-3-haiku-20240307\",\n    max_tokens=1024,\n    max_retries=0,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Create a user for a model with a name, age, and properties.\",\n        }\n    ],\n    response_model=User,\n)  # type: ignore\n\nprint(user_response.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"John Doe\",\n  \"age\": 35,\n  \"properties\": [\n    {\n      \"name\": \"Occupation\",\n      \"value\": \"Software Engineer\"\n    },\n    {\n      \"name\": \"Hobbies\",\n      \"value\": \"Reading, Hiking, Cooking\"\n    },\n    {\n      \"name\": \"Location\",\n      \"value\": \"San Francisco, CA\"\n    }\n  ]\n}\n\"\"\"\n</code></pre> <p>We're encountering challenges with deeply nested types and eagerly invite the community to test, provide feedback, and suggest necessary improvements as we enhance the anthropic client's support.</p>"},{"location":"hub/anyscale/","title":"Structured Outputs with Anyscale","text":"<p>If you want to try this example using <code>instructor hub</code>, you can pull it by running</p> <pre><code>instructor hub pull --slug anyscale --py &gt; anyscale_example.py\n</code></pre> <p>Open-source LLMS are gaining popularity, and the release of Anyscale's Mistral model has made it possible to obtain structured outputs using JSON schema at any scale. Instead of relying on a model's default output mode, you can utilize JSON schema to obtain structured outputs. This approach is a time-saving alternative to extensive prompt engineering.</p> <p>By the end of this blog post, you will learn how to effectively utilize the instructor at any scale. But before we proceed, let's first explore the concept of patching.</p>","tags":["patching","open source"]},{"location":"hub/anyscale/#patching","title":"Patching","text":"<p>Instructor's patch enhances a openai api it with the following features:</p> <ul> <li><code>response_model</code> in <code>create</code> calls that returns a pydantic model</li> <li><code>max_retries</code> in <code>create</code> calls that retries the call if it fails by using a backoff strategy</li> </ul> <p>Learn More</p> <p>To learn more, please refer to the docs. To understand the benefits of using Pydantic with Instructor, visit the tips and tricks section of the why use Pydantic page.</p>","tags":["patching","open source"]},{"location":"hub/anyscale/#anyscale","title":"Anyscale","text":"<p>The good news is that Anyscale employs the same OpenAI client, and its models support some of these output modes too!</p> <p>Getting access</p> <p>If you want to try this out for yourself check out the Anyscale website. You can get started here.</p> <p>Let's explore one of the models available in Anyscale's extensive collection!</p> <pre><code>from openai import OpenAI\nfrom pydantic import BaseModel\nimport os\nimport instructor\n\n\nclass UserDetails(BaseModel):\n    name: str\n    age: int\n\n\n# enables `response_model` in create call\nclient = instructor.from_openai(\n    OpenAI(\n        base_url=\"https://api.endpoints.anyscale.com/v1\",\n        api_key=os.environ[\"ANYSCALE_API_KEY\"],\n    ),\n    # This uses Anyscale's json schema output mode\n    mode=instructor.Mode.JSON_SCHEMA,\n)\n\nresp = client.chat.completions.create(\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a world class extractor\"},\n        {\"role\": \"user\", \"content\": 'Extract the following entities: \"Jason is 20\"'},\n    ],\n    response_model=UserDetails,\n)\nprint(resp)\n#&gt; name='Jason' age=20\n# # &gt; name='Jason' age=20\n</code></pre> <p>You can find more information about Anyscale's output mode support here.</p>","tags":["patching","open source"]},{"location":"hub/batch_classification_langsmith/","title":"Seamless Support with Langsmith","text":"<p>Its a common misconception that LangChain's LangSmith is only compatible with LangChain's models. In reality, LangSmith is a unified DevOps platform for developing, collaborating, testing, deploying, and monitoring LLM applications. In this blog we will explore how LangSmith can be used to enhance the OpenAI client alongside <code>instructor</code>.</p> <p>If you want to try this example using <code>instructor hub</code>, you can pull it by running</p> <pre><code>pip install -U langsmith\ninstructor hub pull --slug batch_classification_langsmith --py &gt; langsmith_example.py\n</code></pre>"},{"location":"hub/batch_classification_langsmith/#langsmith","title":"LangSmith","text":"<p>In order to use langsmith, you first need to set your LangSmith API key.</p> <pre><code>export LANGCHAIN_API_KEY=&lt;your-api-key&gt;\n</code></pre> <p>Next, you will need to install the LangSmith SDK:</p> <pre><code>pip install -U langsmith\npip install -U instructor\n</code></pre> <p>In this example we'll use the <code>wrap_openai</code> function to wrap the OpenAI client with LangSmith. This will allow us to use LangSmith's observability and monitoring features with the OpenAI client. Then we'll use <code>instructor</code> to patch the client with the <code>TOOLS</code> mode. This will allow us to use <code>instructor</code> to add additional functionality to the client.</p> <pre><code>import instructor\nimport asyncio\n\nfrom langsmith import traceable\nfrom langsmith.wrappers import wrap_openai\n\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel, Field, field_validator\nfrom typing import List\nfrom enum import Enum\n\n# Wrap the OpenAI client with LangSmith\nclient = wrap_openai(AsyncOpenAI())\n\n# Patch the client with instructor\nclient = instructor.from_openai(client)\n\n# Rate limit the number of requests\nsem = asyncio.Semaphore(5)\n\n\n# Use an Enum to define the types of questions\nclass QuestionType(Enum):\n    CONTACT = \"CONTACT\"\n    TIMELINE_QUERY = \"TIMELINE_QUERY\"\n    DOCUMENT_SEARCH = \"DOCUMENT_SEARCH\"\n    COMPARE_CONTRAST = \"COMPARE_CONTRAST\"\n    EMAIL = \"EMAIL\"\n    PHOTOS = \"PHOTOS\"\n    SUMMARY = \"SUMMARY\"\n\n\n# You can add more instructions and examples in the description\n# or you can put it in the prompt in `messages=[...]`\nclass QuestionClassification(BaseModel):\n    \"\"\"\n    Predict the type of question that is being asked.\n    Here are some tips on how to predict the question type:\n    CONTACT: Searches for some contact information.\n    TIMELINE_QUERY: \"When did something happen?\n    DOCUMENT_SEARCH: \"Find me a document\"\n    COMPARE_CONTRAST: \"Compare and contrast two things\"\n    EMAIL: \"Find me an email, search for an email\"\n    PHOTOS: \"Find me a photo, search for a photo\"\n    SUMMARY: \"Summarize a large amount of data\"\n    \"\"\"\n\n    # If you want only one classification, just change it to\n    #   `classification: QuestionType` rather than `classifications: List[QuestionType]``\n    chain_of_thought: str = Field(\n        ..., description=\"The chain of thought that led to the classification\"\n    )\n    classification: List[QuestionType] = Field(\n        description=f\"An accuracy and correct prediction predicted class of question. Only allowed types: {[t.value for t in QuestionType]}, should be used\",\n    )\n\n    @field_validator(\"classification\", mode=\"before\")\n    def validate_classification(cls, v):\n        # sometimes the API returns a single value, just make sure it's a list\n        if not isinstance(v, list):\n            v = [v]\n        return v\n\n\n@traceable(name=\"classify-question\")\nasync def classify(data: str) -&gt; QuestionClassification:\n    \"\"\"\n    Perform multi-label classification on the input text.\n    Change the prompt to fit your use case.\n\n    Args:\n        data (str): The input text to classify.\n    \"\"\"\n    async with sem:  # some simple rate limiting\n        return data, await client.chat.completions.create(\n            model=\"gpt-4-turbo-preview\",\n            response_model=QuestionClassification,\n            max_retries=2,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Classify the following question: {data}\",\n                },\n            ],\n        )\n\n\nasync def main(questions: List[str]):\n    tasks = [classify(question) for question in questions]\n\n    for task in asyncio.as_completed(tasks):\n        question, label = await task\n        resp = {\n            \"question\": question,\n            \"classification\": [c.value for c in label.classification],\n            \"chain_of_thought\": label.chain_of_thought,\n        }\n        resps.append(resp)\n    return resps\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    questions = [\n        \"What was that ai app that i saw on the news the other day?\",\n        \"Can you find the trainline booking email?\",\n        \"what did I do on Monday?\",\n        \"Tell me about todays meeting and how it relates to the email on Monday\",\n    ]\n\n    resp = asyncio.run(main(questions))\n\n    for r in resp:\n        print(\"q:\", r[\"question\"])\n        #&gt; q: what did I do on Monday?\n        print(\"c:\", r[\"classification\"])\n        #&gt; c: ['SUMMARY']\n</code></pre> <p>If you follow what we've done is wrapped the client and proceeded to quickly use asyncio to classify a list of questions. This is a simple example of how you can use LangSmith to enhance the OpenAI client. You can use LangSmith to monitor and observe the client, and use <code>instructor</code> to add additional functionality to the client.</p> <p>To take a look at trace of this run check out this shareable link.</p>"},{"location":"hub/cohere/","title":"Structured Outputs with Cohere","text":"<p>If you want to try this example using <code>instructor hub</code>, you can pull it by running</p> <pre><code>instructor hub pull --slug cohere --py &gt; cohere_example.py\n</code></pre> <p>You can now use any of the Cohere's command models with the <code>instructor</code> library to get structured outputs.</p> <p>You'll need a cohere API key which can be obtained by signing up here and gives you free, rate-limited usage for learning and prototyping.</p>"},{"location":"hub/cohere/#setup","title":"Setup","text":"<p><pre><code>pip install cohere\n</code></pre> Export your key: <pre><code>export CO_API_KEY=&lt;YOUR_COHERE_API_KEY&gt;\n</code></pre></p>"},{"location":"hub/cohere/#example","title":"Example","text":"<pre><code>from pydantic import BaseModel, Field\nfrom typing import List\nimport cohere\nimport instructor\n\n\n# Patching the Cohere client with the instructor for enhanced capabilities\nclient = instructor.from_cohere(\n    cohere.Client(),\n    max_tokens=1000,\n    model=\"command-r-plus\",\n)\n\n\nclass Person(BaseModel):\n    name: str = Field(description=\"name of the person\")\n    country_of_origin: str = Field(description=\"country of origin of the person\")\n\n\nclass Group(BaseModel):\n    group_name: str = Field(description=\"name of the group\")\n    members: List[Person] = Field(description=\"list of members in the group\")\n\n\ntask = \"\"\"\\\nGiven the following text, create a Group object for 'The Beatles' band\n\nText:\nThe Beatles were an English rock band formed in Liverpool in 1960. With a line-up comprising John Lennon, Paul McCartney, George Harrison and Ringo Starr, they are regarded as the most influential band of all time. The group were integral to the development of 1960s counterculture and popular music's recognition as an art form.\n\"\"\"\ngroup = client.messages.create(\n    response_model=Group,\n    messages=[{\"role\": \"user\", \"content\": task}],\n    temperature=0,\n)\n\nprint(group.model_dump_json(indent=2))\n\"\"\"\n{\n  \"group_name\": \"The Beatles\",\n  \"members\": [\n    {\n      \"name\": \"John Lennon\",\n      \"country_of_origin\": \"England\"\n    },\n    {\n      \"name\": \"Paul McCartney\",\n      \"country_of_origin\": \"England\"\n    },\n    {\n      \"name\": \"George Harrison\",\n      \"country_of_origin\": \"England\"\n    },\n    {\n      \"name\": \"Ringo Starr\",\n      \"country_of_origin\": \"England\"\n    }\n  ]\n}\n\"\"\"\n</code></pre>"},{"location":"hub/extract_contact_info/","title":"Customer Information Extraction","text":"<p>In this guide, we'll walk through how to extract customer lead information using OpenAI's API and Pydantic. This use case is essential for seamlessly automating the process of extracting specific information from a context.</p> <p>If you want to try this out via <code>instructor hub</code>, you can pull it by running:</p> <pre><code>instructor hub pull --slug extract_contact_info --py &gt; extract_contact_info.py\n</code></pre>"},{"location":"hub/extract_contact_info/#motivation","title":"Motivation","text":"<p>You could potentially integrate this into a chatbot to extract relevant user information from user messages. With the use of machine learning driven validation it would reduce the need for a human to verify the information.</p>"},{"location":"hub/extract_contact_info/#defining-the-structure","title":"Defining the Structure","text":"<p>We'll model a customer lead as a Lead object, including attributes for the name and phone number. We'll use a Pydantic PhoneNumber type to validate the phone numbers entered and provide a Field to give the model more information on correctly populating the object.</p>"},{"location":"hub/extract_contact_info/#extracting-lead-information","title":"Extracting Lead Information","text":"<p>To extract lead information, we create the <code>parse_lead_from_message</code> function which integrates Instructor. It calls OpenAI's API, processes the text, and returns the extracted lead information as a Lead object.</p>"},{"location":"hub/extract_contact_info/#evaluating-lead-extraction","title":"Evaluating Lead Extraction","text":"<p>To showcase the <code>parse_lead_from_message</code> function we can provide sample user messages that may be obtained from a dialogue with a chatbot assistant. Also take note of the response model being set as <code>Iterable[Lead]</code> this allows for multiple leads being extracted from the same message.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom pydantic_extra_types.phone_numbers import PhoneNumber\nfrom typing import Iterable\n\n\nclass Lead(BaseModel):\n    name: str\n    phone_number: PhoneNumber = Field(\n        description=\"Needs to be a phone number with a country code. If none, assume +1\"\n    )\n\n    # Can define some function here to send Lead information to a database using an API\n\n\nclient = instructor.from_openai(OpenAI())\n\n\ndef parse_lead_from_message(user_message: str):\n    return client.chat.completions.create(\n        model=\"gpt-4-turbo-preview\",\n        response_model=Iterable[Lead],\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a data extraction system that extracts a user's name and phone number from a message.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Extract the user's lead information from this user's message: {user_message}\",\n            },\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    lead = parse_lead_from_message(\n        \"Yes, that would be great if someone can reach out my name is Patrick King 9175554587\"\n    )\n    assert all(isinstance(item, Lead) for item in lead)\n    for item in lead:\n        print(item.model_dump_json(indent=2))\n        \"\"\"\n        {\n          \"name\": \"Patrick King\",\n          \"phone_number\": \"tel:+1-917-555-4587\"\n        }\n        \"\"\"\n\n    # Invalid phone number example:\n    try:\n        lead2 = parse_lead_from_message(\n            \"Yes, that would be great if someone can reach out my name is Patrick King 9172234\"\n        )\n        assert all(isinstance(item, Lead) for item in lead2)\n        for item in lead2:\n            print(item.model_dump_json(indent=2))\n\n    except Exception as e:\n        print(\"ERROR:\", e)\n        \"\"\"\n        ERROR:\n        1 validation error for IterableLead\n        tasks.0.phone_number\n          value is not a valid phone number [type=value_error, input_value='+19172234', input_type=str]\n        \"\"\"\n</code></pre> <p>In this example, the <code>parse_lead_from_message</code> function successfully extracts lead information from a user message, demonstrating how automation can enhance the efficiency of collecting accurate customer details. It also shows how the function successfully catches that the phone number is invalid so functionality can be implemented for the user to get prompted again to give a correct phone number.</p>"},{"location":"hub/groq/","title":"Structured Outputs with Groq AI","text":"<p>If you want to try this example using <code>instructor hub</code>, you can pull it by running</p> <pre><code>instructor hub pull --slug groq --py &gt; groq_example.py\n</code></pre> <p>you'll need to sign up for an account and get an API key. You can do that here.</p> <pre><code>export GROQ_API_KEY=&lt;your-api-key-here&gt;\npip install groq\n</code></pre> <p>Other Languages</p> <p>This blog post is written in Python, but the concepts are applicable to other languages as well, as we currently have support for Javascript, Elixir and PHP.</p>"},{"location":"hub/groq/#patching","title":"Patching","text":"<p>Instructor's patch enhances the openai api it with the following features:</p> <ul> <li><code>response_model</code> in <code>create</code> calls that returns a pydantic model</li> <li><code>max_retries</code> in <code>create</code> calls that retries the call if it fails by using a backoff strategy</li> </ul> <p>Learn More</p> <p>To learn more, please refer to the docs. To understand the benefits of using Pydantic with Instructor, visit the tips and tricks section of the why use Pydantic page.</p>"},{"location":"hub/groq/#groq-ai","title":"Groq AI","text":"<p>While Groq AI does not support function calling directly, you can still leverage the TOOLS mode for structured outputs.</p> <p>Getting access</p> <p>If you want to try this out for yourself check out the docs</p> <pre><code>import os\nimport instructor\n\nfrom groq import Groq\nfrom pydantic import BaseModel\n\nclient = Groq(\n    api_key=os.environ.get(\"GROQ_API_KEY\"),\n)\n\n# By default, the patch function will patch the ChatCompletion.create and ChatCompletion.create methods to support the response_model parameter\nclient = instructor.from_groq(client, mode=instructor.Mode.TOOLS)\n\n\n# Now, we can use the response_model parameter using only a base model\n# rather than having to use the OpenAISchema class\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n\nuser: UserExtract = client.chat.completions.create(\n    model=\"mixtral-8x7b-32768\",\n    response_model=UserExtract,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nassert isinstance(user, UserExtract), \"Should be instance of UserExtract\"\nassert user.name.lower() == \"jason\"\nassert user.age == 25\n\nprint(user.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"jason\",\n  \"age\": 25\n}\n\"\"\"\n</code></pre>"},{"location":"hub/knowledge_graph/","title":"Building Knowledge Graphs from Textual Data","text":"<p>In this tutorial, we will explore the process of constructing knowledge graphs from textual data using OpenAI's API and Pydantic. This approach is crucial for efficiently automating the extraction of structured information from unstructured text.</p> <p>To experiment with this yourself through <code>instructor hub</code>, you can obtain the necessary code by executing:</p> <pre><code>instructor hub pull --slug knowledge_graph --py &gt; knowledge_graph.py\n</code></pre> <pre><code>from typing import List\nfrom pydantic import BaseModel, Field\nfrom openai import OpenAI\nimport instructor\n\n\nclass Node(BaseModel):\n    id: int\n    label: str\n    color: str = \"blue\"  # Default color set to blue\n\n\nclass Edge(BaseModel):\n    source: int\n    target: int\n    label: str\n    color: str = \"black\"  # Default color for edges\n\n\nclass KnowledgeGraph(BaseModel):\n    nodes: List[Node] = Field(default_factory=list)\n    edges: List[Edge] = Field(default_factory=list)\n\n\n# Patch the OpenAI client to add response_model support\nclient = instructor.from_openai(OpenAI())\n\n\ndef generate_graph(input_text: str) -&gt; KnowledgeGraph:\n    \"\"\"Generates a knowledge graph from the input text.\"\"\"\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Help me understand the following by describing it as a detailed knowledge graph: {input_text}\",\n            }\n        ],\n        response_model=KnowledgeGraph,\n    )\n\n\nif __name__ == \"__main__\":\n    input_text = \"Jason is Sarah's friend and he is a doctor\"\n    graph = generate_graph(input_text)\n    print(graph.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"nodes\": [\n        {\n          \"id\": 1,\n          \"label\": \"Jason\",\n          \"color\": \"blue\"\n        },\n        {\n          \"id\": 2,\n          \"label\": \"Sarah\",\n          \"color\": \"blue\"\n        },\n        {\n          \"id\": 3,\n          \"label\": \"Doctor\",\n          \"color\": \"blue\"\n        }\n      ],\n      \"edges\": [\n        {\n          \"source\": 1,\n          \"target\": 2,\n          \"label\": \"friend\",\n          \"color\": \"black\"\n        },\n        {\n          \"source\": 1,\n          \"target\": 3,\n          \"label\": \"is a\",\n          \"color\": \"black\"\n        }\n      ]\n    }\n    \"\"\"\n</code></pre>"},{"location":"hub/llama-cpp-python/","title":"Structured Outputs with llama-cpp-python","text":"<p>If you want to try this example using <code>instructor hub</code>, you can pull it by running</p> <pre><code>instructor hub pull --slug llama-cpp-python --py &gt; llama_cpp_python_example.py\n</code></pre> <p>Open-source LLMS are gaining popularity, and llama-cpp-python has made the <code>llama-cpp</code> model available to obtain structured outputs using JSON schema via a mixture of constrained sampling and speculative decoding. They also support a OpenAI compatible client, which can be used to obtain structured output as a in process mechanism to avoid any network dependency.</p>","tags":["patching"]},{"location":"hub/llama-cpp-python/#patching","title":"Patching","text":"<p>Instructor's patch enhances an create call it with the following features:</p> <ul> <li><code>response_model</code> in <code>create</code> calls that returns a pydantic model</li> <li><code>max_retries</code> in <code>create</code> calls that retries the call if it fails by using a backoff strategy</li> </ul> <p>Learn More</p> <p>To learn more, please refer to the docs. To understand the benefits of using Pydantic with Instructor, visit the tips and tricks section of the why use Pydantic page. If you want to check out examples of using Pydantic with Instructor, visit the examples page.</p>","tags":["patching"]},{"location":"hub/llama-cpp-python/#llama-cpp-python","title":"llama-cpp-python","text":"<p>Recently llama-cpp-python added support for structured outputs via JSON schema mode. This is a time-saving alternative to extensive prompt engineering and can be used to obtain structured outputs.</p> <p>In this example we'll cover a more advanced use case of JSON_SCHEMA mode to stream out partial models. To learn more partial streaming check out partial streaming.</p> <pre><code>import llama_cpp\nfrom llama_cpp.llama_speculative import LlamaPromptLookupDecoding\n\nimport instructor\n\nfrom pydantic import BaseModel\nfrom typing import List\nfrom rich.console import Console\n\n\nllama = llama_cpp.Llama(\n    model_path=\"../../models/OpenHermes-2.5-Mistral-7B-GGUF/openhermes-2.5-mistral-7b.Q4_K_M.gguf\",\n    n_gpu_layers=-1,\n    chat_format=\"chatml\",\n    n_ctx=2048,\n    draft_model=LlamaPromptLookupDecoding(num_pred_tokens=2),  # (1)!\n    logits_all=True,\n    verbose=False,\n)\n\n\ncreate = instructor.patch(\n    create=llama.create_chat_completion_openai_v1,\n    mode=instructor.Mode.JSON_SCHEMA,  # (2)!\n)\n\n\ntext_block = \"\"\"\nIn our recent online meeting, participants from various backgrounds joined to discuss\nthe upcoming tech conference. The names and contact details of the participants were as follows:\n\n- Name: John Doe, Email: johndoe@email.com, Twitter: @TechGuru44\n- Name: Jane Smith, Email: janesmith@email.com, Twitter: @DigitalDiva88\n- Name: Alex Johnson, Email: alexj@email.com, Twitter: @CodeMaster2023\n\nDuring the meeting, we agreed on several key points. The conference will be held on March 15th, 2024,\nat the Grand Tech Arena located at 4521 Innovation Drive. Dr. Emily Johnson, a renowned AI researcher,\nwill be our keynote speaker.\n\nThe budget for the event is set at $50,000, covering venue costs, speaker fees, and promotional activities.\nEach participant is expected to contribute an article to the conference blog by February 20th.\n\nA follow-up meetingis scheduled for January 25th at 3 PM GMT to finalize the agenda and confirm the list of speakers.\n\"\"\"\n\n\nclass User(BaseModel):\n    name: str\n    email: str\n    twitter: str\n\n\nclass MeetingInfo(BaseModel):\n    users: List[User]\n    date: str\n    location: str\n    budget: int\n    deadline: str\n\n\nextraction_stream = create(\n    response_model=instructor.Partial[MeetingInfo],  # (3)!\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"Get the information about the meeting and the users {text_block}\",\n        },\n    ],\n    stream=True,\n)\n\n\nconsole = Console()\n\nfor extraction in extraction_stream:\n    obj = extraction.model_dump()\n    console.clear()  # (4)!\n    console.print(obj)\n</code></pre> <p>We use LlamaPromptLookupDecoding to speed up structured output generation using speculative decoding. The draft model generates candidate tokens during generation 10 is good for GPU, 2 is good for CPU. 2. We use <code>instructor.Mode.JSON_SCHEMA</code> return a JSON schema response. 3. We use <code>instructor.Partial</code> to stream out partial models. 4. This is just a simple example of how to stream out partial models and clear the console.</p> <p></p>","tags":["patching"]},{"location":"hub/mistral/","title":"Structured Outputs with Mistral Large","text":"<p>If you want to try this example using <code>instructor hub</code>, you can pull it by running</p> <pre><code>instructor hub pull --slug mistral --py &gt; mistral_example.py\n</code></pre> <p>Mistral Large is the flagship model from Mistral AI, supporting 32k context windows and functional calling abilities. Mistral Large's addition of function calling makes it possible to obtain structured outputs using JSON schema.</p> <p>By the end of this blog post, you will learn how to effectively utilize Instructor with Mistral Large.</p>","tags":["patching"]},{"location":"hub/mistral/#patching","title":"Patching","text":"<p>Instructor's patch enhances the mistral api with the following features:</p> <ul> <li><code>response_model</code> in <code>create</code> calls that returns a pydantic model</li> <li><code>max_retries</code> in <code>create</code> calls that retries the call if it fails by using a backoff strategy</li> </ul> <p>Learn More</p> <p>To learn more, please refer to the docs. To understand the benefits of using Pydantic with Instructor, visit the tips and tricks section of the why use Pydantic page.</p>","tags":["patching"]},{"location":"hub/mistral/#mistral-client","title":"Mistral Client","text":"<p>The Mistral client employs a different client than OpenAI, making the patching process slightly different than other examples</p> <p>Getting access</p> <p>If you want to try this out for yourself check out the Mistral AI website. You can get started here.</p> <pre><code>import instructor\n\nfrom pydantic import BaseModel\nfrom mistralai.client import MistralClient\n\n# enables `response_model` in chat call\nclient = MistralClient()\n\npatched_chat = instructor.from_openai(create=client.chat, mode=instructor.Mode.MISTRAL_TOOLS)\n\nif __name__ == \"__main__\":\n\n    class UserDetails(BaseModel):\n        name: str\n        age: int\n\n    resp = patched_chat(\n        model=\"mistral-large-latest\",\n        response_model=UserDetails,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f'Extract the following entities: \"Jason is 20\"',\n            },\n        ],\n    )\n    print(resp)\n    #&gt; name='Jason' age=20\n</code></pre>","tags":["patching"]},{"location":"hub/multiple_classification/","title":"Multiple Classification Model","text":"<p>If you want to try outs via <code>instructor hub</code>, you can pull it by running</p> <pre><code>instructor hub pull --slug multiple_classification --py &gt; multiple_classification.py\n</code></pre> <p>For multi-label classification, we introduce a new enum class and a different Pydantic model to handle multiple labels.</p> <pre><code>import openai\nimport instructor\n\nfrom typing import List, Literal\nfrom pydantic import BaseModel, Field\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.from_openai(openai.OpenAI())\n\nLABELS = Literal[\"ACCOUNT\", \"BILLING\", \"GENERAL_QUERY\"]\n\n\nclass MultiClassPrediction(BaseModel):\n    labels: List[LABELS] = Field(\n        ...,\n        description=\"Only select the labels that apply to the support ticket.\",\n    )\n\n\ndef multi_classify(data: str) -&gt; MultiClassPrediction:\n    return client.chat.completions.create(\n        model=\"gpt-4-turbo-preview\",  # gpt-3.5-turbo fails\n        response_model=MultiClassPrediction,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"You are a support agent at a tech company. Only select the labels that apply to the support ticket.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Classify the following support ticket: {data}\",\n            },\n        ],\n    )  # type: ignore\n\n\nif __name__ == \"__main__\":\n    ticket = \"My account is locked and I can't access my billing info.\"\n    prediction = multi_classify(ticket)\n    assert {\"ACCOUNT\", \"BILLING\"} == {label for label in prediction.labels}\n    print(\"input:\", ticket)\n    #&gt; input: My account is locked and I can't access my billing info.\n    print(\"labels:\", LABELS)\n    #&gt; labels: typing.Literal['ACCOUNT', 'BILLING', 'GENERAL_QUERY']\n    print(\"prediction:\", prediction)\n    #&gt; prediction: labels=['ACCOUNT', 'BILLING']\n</code></pre>"},{"location":"hub/ollama/","title":"Structured Outputs with Ollama","text":"<p>If you want to try this example using <code>instructor hub</code>, you can pull it by running</p> <pre><code>instructor hub pull --slug ollama --py &gt; ollama_example.py\n</code></pre> <p>Open-source LLMS are gaining popularity, and the release of Ollama's OpenAI compatibility later it has made it possible to obtain structured outputs using JSON schema.</p> <p>By the end of this blog post, you will learn how to effectively utilize instructor with ollama. But before we proceed, let's first explore the concept of patching.</p>","tags":["patching","open source"]},{"location":"hub/ollama/#patching","title":"Patching","text":"<p>Instructor's patch enhances a openai api it with the following features:</p> <ul> <li><code>response_model</code> in <code>create</code> calls that returns a pydantic model</li> <li><code>max_retries</code> in <code>create</code> calls that retries the call if it fails by using a backoff strategy</li> </ul> <p>Learn More</p> <p>To learn more, please refer to the docs. To understand the benefits of using Pydantic with Instructor, visit the tips and tricks section of the why use Pydantic page.</p>","tags":["patching","open source"]},{"location":"hub/ollama/#ollama","title":"Ollama","text":"<p>Start by downloading Ollama, and then pull a model such as Llama 2 or Mistral.</p> <p>Make sure you update your <code>ollama</code> to the latest version!</p> <pre><code>ollama pull llama2\n</code></pre> <pre><code>from openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\nimport instructor\n\n\nclass Character(BaseModel):\n    name: str\n    age: int\n    fact: List[str] = Field(..., description=\"A list of facts about the character\")\n\n\n# enables `response_model` in create call\nclient = instructor.from_openai(\n    OpenAI(\n        base_url=\"http://localhost:11434/v1\",\n        api_key=\"ollama\",  # required, but unused\n    ),\n    mode=instructor.Mode.JSON,\n)\n\nresp = client.chat.completions.create(\n    model=\"llama2\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Tell me about the Harry Potter\",\n        }\n    ],\n    response_model=Character,\n)\nprint(resp.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"Harry James Potter\",\n  \"age\": 37,\n  \"fact\": [\n    \"He is the chosen one.\",\n    \"He has a lightning-shaped scar on his forehead.\",\n    \"He is the son of James and Lily Potter.\",\n    \"He attended Hogwarts School of Witchcraft and Wizardry.\",\n    \"He is a skilled wizard and sorcerer.\",\n    \"He fought against Lord Voldemort and his followers.\",\n    \"He has a pet owl named Snowy.\"\n  ]\n}\n\"\"\"\n</code></pre>","tags":["patching","open source"]},{"location":"hub/pandas_df/","title":"Extracting directly to a DataFrame","text":"<p>You can pull this example into your IDE by running the following command:</p> <pre><code>instructor hub pull --slug pandas_df --py &gt; pandas_df.py\n</code></pre> <p>In this example we'll show you how to extract directly to a <code>pandas.DataFrame</code></p> <pre><code>from io import StringIO\nfrom typing import Annotated, Any\nfrom pydantic import (\n    BaseModel,\n    BeforeValidator,\n    PlainSerializer,\n    InstanceOf,\n    WithJsonSchema,\n)\nimport pandas as pd\nimport instructor\nimport openai\n\n\ndef md_to_df(data: Any) -&gt; Any:\n    # Convert markdown to DataFrame\n    if isinstance(data, str):\n        return (\n            pd.read_csv(\n                StringIO(data),  # Process data\n                sep=\"|\",\n                index_col=1,\n            )\n            .dropna(axis=1, how=\"all\")\n            .iloc[1:]\n            .applymap(lambda x: x.strip())\n        )\n    return data\n\n\nMarkdownDataFrame = Annotated[\n    # Validates final type\n    InstanceOf[pd.DataFrame],\n    # Converts markdown to DataFrame\n    BeforeValidator(md_to_df),\n    # Converts DataFrame to markdown on model_dump_json\n    PlainSerializer(lambda df: df.to_markdown()),\n    # Adds a description to the type\n    WithJsonSchema(\n        {\n            \"type\": \"string\",\n            \"description\": \"\"\"\n            The markdown representation of the table,\n            each one should be tidy, do not try to join\n            tables that should be seperate\"\"\",\n        }\n    ),\n]\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\ndef extract_df(data: str) -&gt; pd.DataFrame:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=MarkdownDataFrame,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a data extraction system, table of writing perfectly formatted markdown tables.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Extract the data into a table: {data}\",\n            },\n        ],\n    )\n\n\nclass Table(BaseModel):\n    title: str\n    data: MarkdownDataFrame\n\n\ndef extract_table(data: str) -&gt; Table:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Table,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a data extraction system, table of writing perfectly formatted markdown tables.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Extract the data into a table: {data}\",\n            },\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    df = extract_df(\n        \"\"\"Create a table of the last 5 presidents of the United States,\n        including their party and the years they served.\"\"\"\n    )\n    assert isinstance(df, pd.DataFrame)\n    print(df)\n    \"\"\"\n                         Party    Years Served\n     President\n    Joe Biden        Democratic  2021 - Present\n    Donald Trump     Republican     2017 - 2021\n    Barack Obama     Democratic     2009 - 2017\n    George W. Bush   Republican     2001 - 2009\n    Bill Clinton     Democratic     1993 - 2001\n    \"\"\"\n\n    table = extract_table(\n        \"\"\"Create a table of the last 5 presidents of the United States,\n        including their party and the years they served.\"\"\"\n    )\n    assert isinstance(table, Table)\n    assert isinstance(table.data, pd.DataFrame)\n    print(table.title)\n    #&gt; Last 5 Presidents of the United States\n    print(table.data)\n    \"\"\"\n                         Party    Years Served\n     President\n    Joe Biden        Democratic  2021 - Present\n    Donald Trump     Republican     2017 - 2021\n    Barack Obama     Democratic     2009 - 2017\n    George W. Bush   Republican     2001 - 2009\n    Bill Clinton     Democratic     1993 - 2001\n    \"\"\"\n</code></pre> <p>Notice that you can extract both the raw <code>MarkdownDataFrame</code> or a more complex structure like <code>Table</code> which includes a title and the data as a DataFrame. You can even request <code>Iterable[Table]</code> to get multiple tables in a single response!</p>"},{"location":"hub/partial_streaming/","title":"Streaming Partial Responses","text":"<p>Field level streaming provides incremental snapshots of the current state of the response model that are immediately useable. This approach is particularly relevant in contexts like rendering UI components.</p> <p>Instructor supports this pattern by making use of <code>Partial[T]</code>. This lets us dynamically create a new class that treats all of the original model's fields as <code>Optional</code>.</p> <p>If you want to try outs via <code>instructor hub</code>, you can pull it by running</p> <pre><code>instructor hub pull --slug partial_streaming --py &gt; partial_streaming.py\n</code></pre> <pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nfrom typing import List\n\nclient = instructor.from_openai(OpenAI())\n\ntext_block = \"\"\"\nIn our recent online meeting, participants from various backgrounds joined to discuss the upcoming tech conference. The names and contact details of the participants were as follows:\n\n- Name: John Doe, Email: johndoe@email.com, Twitter: @TechGuru44\n- Name: Jane Smith, Email: janesmith@email.com, Twitter: @DigitalDiva88\n- Name: Alex Johnson, Email: alexj@email.com, Twitter: @CodeMaster2023\n\nDuring the meeting, we agreed on several key points. The conference will be held on March 15th, 2024, at the Grand Tech Arena located at 4521 Innovation Drive. Dr. Emily Johnson, a renowned AI researcher, will be our keynote speaker.\n\nThe budget for the event is set at $50,000, covering venue costs, speaker fees, and promotional activities. Each participant is expected to contribute an article to the conference blog by February 20th.\n\nA follow-up meetingis scheduled for January 25th at 3 PM GMT to finalize the agenda and confirm the list of speakers.\n\"\"\"\n\n\nclass User(BaseModel):\n    name: str\n    email: str\n    twitter: str\n\n\nclass MeetingInfo(BaseModel):\n    users: List[User]\n    date: str\n    location: str\n    budget: int\n    deadline: str\n\n\nPartialMeetingInfo = instructor.Partial[MeetingInfo]\n\n\nextraction_stream = client.chat.completions.create(\n    model=\"gpt-4\",\n    response_model=PartialMeetingInfo,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"Get the information about the meeting and the users {text_block}\",\n        },\n    ],\n    stream=True,\n)  # type: ignore\n\n\nfrom rich.console import Console\n\nconsole = Console()\n\nfor extraction in extraction_stream:\n    obj = extraction.model_dump()\n    console.clear()\n    console.print(obj)\n</code></pre>"},{"location":"hub/single_classification/","title":"Single-Label Classification","text":"<p>IF you want to try this code with <code>instructor hub</code> you can pull it by running</p> <pre><code>instructor hub pull --slug single_classification --py &gt; single_classification.py\n</code></pre> <p>This example demonstrates how to perform single-label classification using the OpenAI API. The example uses the <code>gpt-3.5-turbo</code> model to classify text as either <code>SPAM</code> or <code>NOT_SPAM</code>.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import Literal\nfrom openai import OpenAI\nimport instructor\n\n# Apply the patch to the OpenAI client\n# enables response_model keyword\nclient = instructor.from_openai(OpenAI())\n\n\nclass ClassificationResponse(BaseModel):\n    label: Literal[\"SPAM\", \"NOT_SPAM\"] = Field(\n        ...,\n        description=\"The predicted class label.\",\n    )\n\n\ndef classify(data: str) -&gt; ClassificationResponse:\n    \"\"\"Perform single-label classification on the input text.\"\"\"\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=ClassificationResponse,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Classify the following text: {data}\",\n            },\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    for text, label in [\n        (\"Hey Jason! You're awesome\", \"NOT_SPAM\"),\n        (\"I am a nigerian prince and I need your help.\", \"SPAM\"),\n    ]:\n        prediction = classify(text)\n        assert prediction.label == label\n        print(f\"Text: {text}, Predicted Label: {prediction.label}\")\n        #&gt; Text: Hey Jason! You're awesome, Predicted Label: NOT_SPAM\n        #&gt; Text: I am a nigerian prince and I need your help., Predicted Label: SPAM\n</code></pre>"},{"location":"hub/tables_from_vision/","title":"Extracting Tables from Images with OpenAI's GPT-4 Vision Model","text":"<p>First, we define a custom type, <code>MarkdownDataFrame</code>, to handle pandas DataFrames formatted in markdown. This type uses Python's <code>Annotated</code> and <code>InstanceOf</code> types, along with decorators <code>BeforeValidator</code> and <code>PlainSerializer</code>, to process and serialize the data.</p>"},{"location":"hub/tables_from_vision/#defining-the-table-class","title":"Defining the Table Class","text":"<p>The <code>Table</code> class is essential for organizing the extracted data. It includes a caption and a dataframe, processed as a markdown table. Since most of the complexity is handled by the <code>MarkdownDataFrame</code> type, the <code>Table</code> class is straightforward!</p> <p>This requires additional dependencies <code>pip install pandas tabulate</code>.</p> <pre><code>from openai import OpenAI\nfrom io import StringIO\nfrom typing import Annotated, Any, List\nfrom pydantic import (\n    BaseModel,\n    BeforeValidator,\n    PlainSerializer,\n    InstanceOf,\n    WithJsonSchema,\n)\nimport instructor\nimport pandas as pd\nfrom rich.console import Console\n\nconsole = Console()\nclient = instructor.from_openai(\n    client=OpenAI(),\n    mode=instructor.Mode.TOOLS,\n)\n\n\ndef md_to_df(data: Any) -&gt; Any:\n    if isinstance(data, str):\n        return (\n            pd.read_csv(\n                StringIO(data),  # Get rid of whitespaces\n                sep=\"|\",\n                index_col=1,\n            )\n            .dropna(axis=1, how=\"all\")\n            .iloc[1:]\n            .map(lambda x: x.strip())\n        )  # type: ignore\n    return data\n\n\nMarkdownDataFrame = Annotated[\n    InstanceOf[pd.DataFrame],\n    BeforeValidator(md_to_df),\n    PlainSerializer(lambda x: x.to_markdown()),\n    WithJsonSchema(\n        {\n            \"type\": \"string\",\n            \"description\": \"\"\"\n                The markdown representation of the table, \n                each one should be tidy, do not try to join tables\n                that should be seperate\"\"\",\n        }\n    ),\n]\n\n\nclass Table(BaseModel):\n    caption: str\n    dataframe: MarkdownDataFrame\n\n\nclass MultipleTables(BaseModel):\n    tables: List[Table]\n\n\nexample = MultipleTables(\n    tables=[\n        Table(\n            caption=\"This is a caption\",\n            dataframe=pd.DataFrame(\n                {\n                    \"Chart A\": [10, 40],\n                    \"Chart B\": [20, 50],\n                    \"Chart C\": [30, 60],\n                }\n            ),\n        )\n    ]\n)\n\n\ndef extract(url: str) -&gt; MultipleTables:\n    return client.chat.completions.create(\n        model=\"gpt-4-turbo\",\n        max_tokens=4000,\n        response_model=MultipleTables,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": url},\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": \"\"\"\n                            First, analyze the image to determine the most appropriate headers for the tables.\n                            Generate a descriptive h1 for the overall image, followed by a brief summary of the data it contains. \n                            For each identified table, create an informative h2 title and a concise description of its contents.\n                            Finally, output the markdown representation of each table.\n\n\n                            Make sure to escape the markdown table properly, and make sure to include the caption and the dataframe.\n                            including escaping all the newlines and quotes. Only return a markdown table in dataframe, nothing else.\n                        \"\"\",\n                    },\n                ],\n            }\n        ],\n    )\n\n\nurls = [\n    \"https://a.storyblok.com/f/47007/2400x1260/f816b031cb/uk-ireland-in-three-charts_chart_a.png/m/2880x0\",\n    \"https://a.storyblok.com/f/47007/2400x2000/bf383abc3c/231031_uk-ireland-in-three-charts_table_v01_b.png/m/2880x0\",\n]\n\nfor url in urls:\n    for table in extract(url).tables:\n        console.print(table.caption, \"\\n\", table.dataframe)\n</code></pre>"},{"location":"hub/together/","title":"Structured Outputs with Together AI","text":"<p>If you want to try this example using <code>instructor hub</code>, you can pull it by running</p> <pre><code>instructor hub pull --slug together --py &gt; together_example.py\n</code></pre> <p>Open-source LLMS are gaining popularity, and with the release of Together's Function calling models, its been easier than ever to get structured outputs.</p> <p>By the end of this blog post, you will learn how to effectively utilize instructor with Together AI. But before we proceed, let's first explore the concept of patching.</p> <p>Other Languages</p> <p>This blog post is written in Python, but the concepts are applicable to other languages as well, as we currently have support for Javascript, Elixir and PHP.</p>","tags":["patching","open source"]},{"location":"hub/together/#patching","title":"Patching","text":"<p>Instructor's patch enhances the openai api it with the following features:</p> <ul> <li><code>response_model</code> in <code>create</code> calls that returns a pydantic model</li> <li><code>max_retries</code> in <code>create</code> calls that retries the call if it fails by using a backoff strategy</li> </ul> <p>Learn More</p> <p>To learn more, please refer to the docs. To understand the benefits of using Pydantic with Instructor, visit the tips and tricks section of the why use Pydantic page.</p>","tags":["patching","open source"]},{"location":"hub/together/#together-ai","title":"Together AI","text":"<p>The good news is that Together employs the same OpenAI client, and its models support some of these output modes too!</p> <p>Getting access</p> <p>If you want to try this out for yourself check out the Together AI website. You can get started here.</p> <pre><code>import os\nimport openai\nfrom pydantic import BaseModel\nimport instructor\n\nclient = openai.OpenAI(\n    base_url=\"https://api.together.xyz/v1\",\n    api_key=os.environ[\"TOGETHER_API_KEY\"],\n)\n\n\n# By default, the patch function will patch the ChatCompletion.create and ChatCompletion.create methods to support the response_model parameter\nclient = instructor.from_openai(client, mode=instructor.Mode.TOOLS)\n\n\n# Now, we can use the response_model parameter using only a base model\n# rather than having to use the OpenAISchema class\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n\nuser: UserExtract = client.chat.completions.create(\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n    response_model=UserExtract,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nassert isinstance(user, UserExtract), \"Should be instance of UserExtract\"\nassert user.name.lower() == \"jason\"\nassert user.age == 25\n\nprint(user.model_dump_json(indent=2))\n\"\"\"\n{\n  \"name\": \"jason\",\n  \"age\": 25\n}\n\"\"\"\n{\n    \"name\": \"Jason\",\n    \"age\": 25,\n}\n</code></pre> <p>You can find more information about Together's function calling support here.</p>","tags":["patching","open source"]},{"location":"hub/vertexai/","title":"Structured Outputs with Vertex AI","text":"<p>Vertex AI is the recommended way to deploy the Gemini family of models in production. These models support up to 1 million tokens in their context window and boast native multimodality with files, video, and audio. The Vertex AI SDK offers a preview of tool calling that we can use to obtain structured outputs.</p> <p>By the end of this blog post, you will learn how to effectively utilize Instructor with the Gemini family of models.</p>","tags":["patching"]},{"location":"hub/vertexai/#patching","title":"Patching","text":"<p>Instructor's patch enhances the gemini api with the following features:</p> <ul> <li><code>response_model</code> in <code>create</code> calls that returns a pydantic model</li> <li><code>max_retries</code> in <code>create</code> calls that retries the call if it fails by using a backoff strategy</li> </ul> <p>Learn More</p> <p>To learn more, please refer to the docs. To understand the benefits of using Pydantic with Instructor, visit the tips and tricks section of the why use Pydantic page.</p>","tags":["patching"]},{"location":"hub/vertexai/#vertex-ai-client","title":"Vertex AI Client","text":"<p>The Vertex AI client employs a different client than OpenAI, making the patching process slightly different than other examples</p> <p>Getting access</p> <p>If you want to try this out for yourself check out the Vertex AI console. You can get started here.</p> <pre><code>import instructor\n\nfrom pydantic import BaseModel\nimport vertexai.generative_models as gm\nimport vertexai\n\nvertexai.init()\n\nclient = gm.GenerativeModel(\"gemini-1.5-pro-preview-0409\")\n\n# enables `response_model` in chat call\nclient = instructor.from_vertexai(client)\n\n\nif __name__ == \"__main__\":\n\n    class UserDetails(BaseModel):\n        name: str\n        age: int\n\n    resp = client.create(\n        response_model=UserDetails,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f'Extract the following entities: \"Jason is 20\"',\n            },\n        ],\n    )\n    print(resp)\n    #&gt; name='Jason' age=20\n</code></pre>","tags":["patching"]},{"location":"hub/vertexai/#json-mode","title":"JSON Mode","text":"<p>By default, <code>instructor.from_vertexai()</code> uses the mode <code>instructor.Mode.VERTEXAI_TOOLS</code>, which means it will use tool calling to create the model response. Alternatively, you can use <code>instructor.Mode.VERTEXAI_JSON</code> to use the response_schema parameter provided by the VertexAI SDK. This parameter will prompt Gemini to respond with JSON directly, which can then be parsed into a model response.</p> <p>If you are not getting good results with tool calling, or prefer this method for any reason, you can switch to this mode:</p> <pre><code>### rest of the code as above ...\n\nclient = gm.GenerativeModel(\"gemini-1.5-pro-preview-0409\", mode=instructor.Mode.VERTEXAI_JSON)\n\n## rest of the code as above ...\n</code></pre>","tags":["patching"]},{"location":"hub/vertexai/#limitations","title":"Limitations","text":"<p>Currently, Vertex AI offers does not support the following attributes from the OpenAPI schema: <code>optional</code>, <code>maximum</code>, <code>anyOf</code>. This means that not all pydantic models will be supported. Below, I'll share some models that could trigger this error and some work-arounds.</p>","tags":["patching"]},{"location":"hub/vertexai/#optional-anyof","title":"optional / anyOf","text":"<p>Using a pydantic model with an <code>Optional</code> field raise an exception, because the Optional type is translated to <code>\"anyOf\": [integer , null]</code> which is not yet supported.</p> <pre><code>from typing import Optional\n\nclass User(BaseModel):\n    name: str\n    age: Optional[int]\n\nresp = client.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Anibal is 23 years old.\",\n        }\n    ],\n    response_model=User,\n)\n\nprint(resp)\n# ValueError: Protocol message Schema has no \"anyOf\" field.\n</code></pre> <p>A workaround if to set a certain default value that Gemini can fall back on if the information is not present:</p> <pre><code>from pydantic import Field\n\nclass User(BaseModel):\n    name: str\n    age: int = Field(default=0) # or just age: int = 0\n\nresp = client.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Anibal is _ years old.\",\n        }\n    ],\n    response_model=User,\n)\n\nprint(resp)\n# name='Anibal' age=0\n</code></pre> <p>This workaround can also work with default_factories:</p> <pre><code>class User(BaseModel):\n    name: str\n    age: int\n    siblings: list[str] = Field(default_factory=lambda: [])\n\nresp = client.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Anibal is 23 years old.\",\n        }\n    ],\n    response_model=User,\n)\n\nprint(resp)\n# name='Anibal' age=23 siblings=[]\n</code></pre>","tags":["patching"]},{"location":"hub/vertexai/#maximum","title":"maximum","text":"<p>Using the <code>lt</code>(less than) or <code>gt</code>(greater than) paramateres in a pydantic field will raise exceptions:</p> <pre><code>class User(BaseModel):\n    name: str\n    age: int = Field(gt=0)\n\nresp = client.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Anibal is 23 years old.\",\n        }\n    ],\n    response_model=User,\n)\n\nprint(resp)\n# ValueError: Protocol message Schema has no \"exclusiveMinimum\" field.\n\nclass User(BaseModel):\n    name: str\n    age: int = Field(lt=100)\n\nresp = client.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Anibal is _ years old.\",\n        }\n    ],\n    response_model=User,\n)\n\nprint(resp)\n# ValueError: Protocol message Schema has no \"exclusiveMaximum\" field\n</code></pre> <p>A workaround for this is to use pydantic validadors to change these values post creation</p> <pre><code>from pydantic import field_validator\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n    @field_validator(\"age\")\n    def age_range_limit(cls, age: int) -&gt; int:\n        if age &gt; 100:\n            age = 100\n        elif age &lt; 0:\n            age = 0\n        return age\n\nresp = client.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Anibal is 1023 years old.\",\n        }\n    ],\n    response_model=User,\n)\n\nprint(resp)\n# name='Anibal' age=100\n\nresp = client.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Extract Anibal is -12 years old.\",\n        }\n    ],\n    response_model=User,\n)\n\nprint(resp)\n# name='Anibal' age=0\n</code></pre> <p>So by relying on pydantic, we can mitigate some of the current limitations with the Gemini models \ud83d\ude0a.</p>","tags":["patching"]},{"location":"hub/youtube_clips/","title":"Generating YouTube Clips from Transcripts","text":"<p>This guide demonstrates how to generate concise, informative clips from YouTube video transcripts using the <code>instructor</code> library. By leveraging the power of OpenAI's models, we can extract meaningful segments from a video's transcript, which can then be recut into smaller, standalone videos. This process involves identifying key moments within a transcript and summarizing them into clips with specific titles and descriptions.</p> <p>If you're interested in trying this example using <code>instructor hub</code>, you can pull it by running:</p> <pre><code>pip install youtube_transcript_api instructor rich \ninstructor hub pull --slug youtube-clips --py &gt; youtube_clips.py\n</code></pre> <p></p> <pre><code>from youtube_transcript_api import YouTubeTranscriptApi\nfrom pydantic import BaseModel, Field\nfrom typing import List, Generator, Iterable\nimport instructor\nimport openai\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\ndef extract_video_id(url: str) -&gt; str | None:\n    import re\n\n    match = re.search(r\"v=([a-zA-Z0-9_-]+)\", url)\n    if match:\n        return match.group(1)\n\n\nclass TranscriptSegment(BaseModel):\n    source_id: int\n    start: float\n    text: str\n\n\ndef get_transcript_with_timing(\n    video_id: str,\n) -&gt; Generator[TranscriptSegment, None, None]:\n    \"\"\"\n    Fetches the transcript of a YouTube video along with the start and end times\n    for each text segment, and returns them as a list of Pydantic models.\n    \"\"\"\n    transcript = YouTubeTranscriptApi.get_transcript(video_id)\n    for ii, segment in enumerate(transcript):\n        yield TranscriptSegment(\n            source_id=ii, start=segment[\"start\"], text=segment[\"text\"]\n        )\n\n\nclass YoutubeClip(BaseModel):\n    title: str = Field(description=\"Specific and informative title for the clip.\")\n    description: str = Field(\n        description=\"A detailed description of the clip, including notable quotes or phrases.\"\n    )\n    start: float\n    end: float\n\n\nclass YoutubeClips(BaseModel):\n    clips: List[YoutubeClip]\n\n\ndef yield_clips(segments: Iterable[TranscriptSegment]) -&gt; Iterable[YoutubeClips]:\n    return client.chat.completions.create(\n        model=\"gpt-4-turbo-preview\",\n        stream=True,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are given a sequence of YouTube transcripts and your job \n                is to return notable clips that can be recut as smaller videos. Give very \n                specific titles and descriptions. Make sure the length of clips is proportional \n                to the length of the video. Note that this is a transcript and so there might \n                be spelling errors. Note that and correct any spellings. Use the context to \n                make sure you're spelling things correctly.\"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Let's use the following transcript segments.\\n{segments}\",\n            },\n        ],\n        response_model=instructor.Partial[YoutubeClips],\n        validation_context={\"segments\": segments},\n    )  # type: ignore\n\n\n# Example usage\nif __name__ == \"__main__\":\n    from rich.table import Table\n    from rich.console import Console\n    from rich.prompt import Prompt\n\n    console = Console()\n    url = Prompt.ask(\"Enter a YouTube URL\")\n\n    with console.status(\"[bold green]Processing YouTube URL...\") as status:\n        video_id = extract_video_id(url)\n\n        if video_id is None:\n            raise ValueError(\"Invalid YouTube video URL\")\n\n        transcript = list(get_transcript_with_timing(video_id))\n        status.update(\"[bold green]Generating clips...\")\n\n        for clip in yield_clips(transcript):\n            console.clear()\n\n            table = Table(title=\"Extracted YouTube Clips\", padding=(0, 1))\n\n            table.add_column(\"Title\", style=\"cyan\")\n            table.add_column(\"Description\", style=\"magenta\")\n            table.add_column(\"Start\", justify=\"right\", style=\"green\")\n            table.add_column(\"End\", justify=\"right\", style=\"green\")\n            for youtube_clip in clip.clips or []:\n                table.add_row(\n                    youtube_clip.title,\n                    youtube_clip.description,\n                    str(youtube_clip.start),\n                    str(youtube_clip.end),\n                )\n            console.print(table)\n</code></pre>"},{"location":"prompting/","title":"Prompting","text":"<p>Effective prompting is an art which requires a nuanced understanding of different techniques. When executed well, prompting can significantly enhance model performance.</p> <p>To help, we've created examples of 58 different prompting techniques<sup>*</sup> using <code>instructor</code> that you can take advantage of today.</p> <p>The prompting techniques are separated into the following categories - Zero Shot, Few Shot, Thought Generation, Ensembling, Self-Criticism, and Decomposition.</p> <p>Click on the links to learn more about each method and how to apply them effectively in your prompts.</p>"},{"location":"prompting/#zero-shot","title":"Zero-Shot","text":"<p>How do we increase the performance of our model without any examples?</p> <ol> <li>Use Emotional Language</li> <li>Assign a Role</li> <li>Define a Style</li> <li>Auto-Refine The Prompt</li> <li>Simulate a Perspective</li> <li>Clarify Ambiguous Information</li> <li>Ask Model To Repeat Query</li> <li>Generate Follow-Up Questions</li> </ol>"},{"location":"prompting/#few-shot","title":"Few-Shot","text":"<p>How do we choose effective examples to include in our prompt?</p> <ol> <li>Auto-Generate Examples</li> <li>Re-Order Examples</li> <li>Choose Examples Similar to the Query (KNN)</li> <li>Choose Examples Similar to the Query (Vote-K)</li> </ol>"},{"location":"prompting/#thought-generation","title":"Thought Generation","text":"<p>How do we encourage our model to mimic human-like reasoning?</p>"},{"location":"prompting/#zero-shot_1","title":"Zero Shot","text":"<ol> <li>Auto-Generate Chain-Of-Thought Examples</li> <li>First Ask a Higher-Level Question</li> <li>Encourage Analysis</li> <li>Encourage Structural Reasoning</li> </ol>"},{"location":"prompting/#few-shot_1","title":"Few Shot","text":"<ol> <li>Annotate Only Uncertain Examples</li> <li>Choose Diverse Examples</li> <li>Choose Complex Examples</li> <li>Include Incorrect Demonstrations</li> <li>Choose Similar, Auto-Generated, High-Certainty Chain-Of-Thought Reasonings</li> <li>Choose the Most Certain Reasoning</li> <li>Generate Template-Based Prompts</li> </ol>"},{"location":"prompting/#ensembling","title":"Ensembling","text":"<p>How can we use multiple prompts and aggregate their responses?</p> <ol> <li>Build a Set of Consistent, Diverse Examples</li> <li>Batch In-Context Examples</li> <li>Verify Individual Reasoning Steps</li> <li>Maximize Information Between Input and Output</li> <li>Merge Multiple Chains-Of-Thought</li> <li>Use Specialized Experts</li> <li>Choose The Most Consistent Reasoning</li> <li>Choose The Most Consistent Reasioning (Universal)</li> <li>Use Task-Specific Example Selection</li> <li>Paraphrase The Prompt</li> </ol>"},{"location":"prompting/#self-criticism","title":"Self-Criticism","text":"<p>How can a model verify or critique its own response?</p> <ol> <li>Generate Verification Questions</li> <li>Ask If the Answer is Correct</li> <li>Generate Feedback and Auto-Improve</li> <li>Score Multiple Candidate Solutions</li> <li>Reconstruct The Problem</li> <li>Generate Possible Steps</li> </ol>"},{"location":"prompting/#decomposition","title":"Decomposition","text":"<p>How can we break down complex problems? How do we solve subproblems?</p> <ol> <li>Implement Subproblems As Functions</li> <li>Use Natural and Symbolic Language</li> <li>Solve Increasingly Complex Subproblems</li> <li>Generate a Plan</li> <li>Use Code As Reasoning</li> <li>Recursively Solve Subproblems</li> <li>Generate a Skeleton</li> <li>Search Through Subproblems</li> </ol> <p><sup>*</sup>: The Prompt Report: A Systematic Survey of Prompting Techniques</p>"},{"location":"prompting/decomposition/decomp/","title":"Break Down Complex Tasks","text":"<p>Decomposed Prompting<sup>1</sup> leverages a Language Model (LLM) to deconstruct a complex task into a series of manageable sub-tasks. Each sub-task is then processed by specific functions, enabling the LLM to handle intricate problems more effectively and systematically.</p> <p>In the code snippet below, we define a series of data models and functions to implement this approach.</p> <p>The <code>derive_action_plan</code> function generates an action plan using the LLM, which is then executed step-by-step. Each action can be</p> <ol> <li>InitialInput: Which represents the chunk of the original prompt we need to process</li> <li>Split : An operation to split strings using a given separator</li> <li>StrPos: An operation to help extract a string given an index</li> <li>Merge: An operation to join a list of strings together using a given character</li> </ol> <p>We can implement this using <code>instructor</code> as seen below.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom typing import Union\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass Split(BaseModel):\n    split_char: str = Field(\n        description=\"\"\"This is the character to split\n        the string with\"\"\"\n    )\n\n    def split_chars(self, s: str, c: str):\n        return s.split(c)\n\n\nclass StrPos(BaseModel):\n    index: int = Field(\n        description=\"\"\"This is the index of the character\n        we wish to return\"\"\"\n    )\n\n    def get_char(self, s: list[str], i: int):\n        return [c[i] for c in s]\n\n\nclass Merge(BaseModel):\n    merge_char: str = Field(\n        description=\"\"\"This is the character to merge the\n        inputs we plan to pass to this function with\"\"\"\n    )\n\n    def merge_string(self, s: list[str]):\n        return self.merge_char.join(s)\n\n\nclass Action(BaseModel):\n    id: int = Field(\n        description=\"\"\"Unique Incremental id to identify\n        this action with\"\"\"\n    )\n    action: Union[Split, StrPos, Merge]\n\n\nclass ActionPlan(BaseModel):\n    initial_data: str\n    plan: list[Action]\n\n\ndef derive_action_plan(task_description: str) -&gt; ActionPlan:\n    return client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"Generate an action plan to help you complete\n                the task outlined by the user\"\"\",\n            },\n            {\"role\": \"user\", \"content\": task_description},\n        ],\n        response_model=ActionPlan,\n        max_retries=3,\n        model=\"gpt-4o\",\n    )\n\n\nif __name__ == \"__main__\":\n    task = \"\"\"Concatenate the second letter of every word in Jack\n    Ryan together\"\"\"\n    plan = derive_action_plan(task)\n    print(plan.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"initial_data\": \"Jack Ryan\",\n      \"plan\": [\n        {\n          \"id\": 1,\n          \"action\": {\n            \"split_char\": \" \"\n          }\n        },\n        {\n          \"id\": 2,\n          \"action\": {\n            \"index\": 1\n          }\n        },\n        {\n          \"id\": 3,\n          \"action\": {\n            \"merge_char\": \"\"\n          }\n        }\n      ]\n    }\n    \"\"\"\n\n    curr = plan.initial_data\n    cache = {}\n\n    for action in plan.plan:\n        if isinstance(action.action, Split) and isinstance(curr, str):\n            curr = action.action.split_chars(curr, action.action.split_char)\n        elif isinstance(action.action, StrPos) and isinstance(curr, list):\n            curr = action.action.get_char(curr, action.action.index)\n        elif isinstance(action.action, Merge) and isinstance(curr, list):\n            curr = action.action.merge_string(curr)\n        else:\n            raise ValueError(\"Unsupported Operation\")\n\n        print(action, curr)\n        #&gt; id=1 action=Split(split_char=' ') ['Jack', 'Ryan']\n        #&gt; id=2 action=StrPos(index=1) ['a', 'y']\n        #&gt; id=3 action=Merge(merge_char='') ay\n\n    print(curr)\n    #&gt; ay\n</code></pre>"},{"location":"prompting/decomposition/decomp/#references","title":"References","text":"<p><sup>1</sup>: Decomposed Prompting: A Modular Approach for Solving Complex Tasks</p>"},{"location":"prompting/decomposition/faithful_cot/","title":"Leverage Task Specific Systems","text":"<p>Faithful Chain of Thought<sup>1</sup> improves the faithfulness of reasoning chains generated by Language Models by breaking it up into two stages</p> <ol> <li>Translation : We first translate a user query into a series of reasoning steps. These are a task specific set of steps that we can execute deterministically.</li> <li>Problem Solving: We execute our steps and arrive at a final answer that we can derive. This ensures that our Chain Of Thought is able to derive a answer that is consistent with the reasoning steps.</li> </ol> <p>They list a few examples in the paper of what these task-specific steps could be</p> <ol> <li>Math Word Problems : Python Code that can be executed by an interpreter to derive a final answer</li> <li>Multi-Hop QA : This is a multi-step reasoning process. To solve this, they use a mix of python and Datalog ( which is a relation and log programming language ) to arrive at a final answer</li> <li>Planning : When trying to generate a plan to solve a user query, they generate a list of symbolic goals in a Programming Language and then call a PDDL Planner to obtain a plan to solve the user's query</li> </ol> <p></p> <p>In the example below, we show how you can use a LLM to generate python code that can be executed by an Interpreter to arrive at a final answer.</p> <p>We can implement it in <code>instructor</code> as seen below</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass ReasoningStep(BaseModel):\n    id: int = Field(description=\"Unique ID\")\n    rationale: list[str] = Field(\n        description=\"\"\"Specific sections from prior reasoning\n        steps or the context that ground this reasoning step\"\"\"\n    )\n    dependencies: list[int] = Field(\n        description=\"\"\"IDs of prior reasoning steps that this\n        reasoning step depends on\"\"\"\n    )\n    eval_string: str = Field(\n        description=\"\"\"Python Code to execute to generate the\n        final evaluation\"\"\"\n    )\n\n\ndef generate_reasoning_steps(query: str) -&gt; list[ReasoningStep]:\n    return client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"\n                You are a world class AI who excels at\n                generating reasoning steps to answer a\n                question. You will be given a question\n                and you will generate a list of reasoning\n                steps that are needed to answer the\n                question.\n\n                At each point you should either\n                - declare a variable to be referenced\n                later on\n                - combine multiple variables together to\n                generate a new result that you should\n                store in another variable\n\n                The final answer should be stored in a\n                variable called `answer`.\n                \"\"\",\n            },\n            {\"role\": \"user\", \"content\": query},\n        ],\n        model=\"gpt-4o\",\n        response_model=list[ReasoningStep],\n    )\n\n\nif __name__ == \"__main__\":\n    steps = generate_reasoning_steps(\n        \"\"\"If there are 3 cars in the parking lot and 2 more\n        cars arrive, how many cars are in the parking lot\n        after another 2 more arrive?\"\"\"\n    )\n\n    code = \"\\n\".join([step.eval_string for step in steps])\n    print(code)\n    \"\"\"\n    initial_cars = 3\n    arriving_cars = 2\n    cars_after_first_arrival = initial_cars + arriving_cars\n    final_car_count = cars_after_first_arrival + 2\n    answer = final_car_count\n    \"\"\"\n    exec(code)\n\n    local_vars = {}\n    exec(code, {}, local_vars)\n    print(local_vars.get(\"answer\"))\n    #&gt; 7\n</code></pre>"},{"location":"prompting/decomposition/faithful_cot/#references","title":"References","text":"<p><sup>1</sup>: Faithful Chain-of-Thought Reasoning</p>"},{"location":"prompting/decomposition/least_to_most/","title":"Solve simpler subproblems","text":"<p>Given a complex problem, how can we encourage an LLM to solve simpler subproblems?</p> <p>Least-to-Most is a prompting technique that breaks a complex problem down into a series of increasingly complex subproblems.</p> <p>Subproblems Example</p> <p>original problem: Adam is twice as old as Mary. Adam will be 11 in 1 year. How old is Mary?</p> <p>subproblems: (1) How old is Adam now? (2) What is half of Adam's current age?</p> <p>These subproblems are solved sequentially, allowing the answers from earlier (simpler) subproblems to inform the LLM while solving later (more complex) subproblems.</p> <pre><code>import instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\nfrom typing import Iterable\n\n\nclass Subquestion(BaseModel):\n    question: str\n\n\nclass Answer(BaseModel):\n    answer: int\n\n\nclass SubquestionWithAnswers(BaseModel):\n    question: str\n    answer: int\n\n\nclient = instructor.from_openai(OpenAI())\n\n\ndef decompose(question):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Iterable[Subquestion],\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Break this question down into subquestions to solve sequentially: {question}\",\n            }\n        ],\n    )\n\n\ndef solve(question, solved_questions, original_question):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Answer,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                    &lt;original_question&gt;\n                    {original_question}\n                    &lt;/original_question&gt;\n\n                    &lt;solved_subquestions&gt;\n                    {solved_questions}\n                    &lt;/solved_subquestions&gt;\n\n                    Solve this next subquestion: {question}\n                    \"\"\",\n            }\n        ],\n    ).answer\n\n\nif __name__ == \"__main__\":\n    question = \"Four years ago, Kody was only half as old as Mohamed. If Mohamed is currently twice 30 years old, how old is Kody?\"\n\n    # Stage 1: Decompose Question into Subquestions\n    subquestions = decompose(question)\n\n    # Stage 2: Sequentially Solve Subquestions\n    solved_questions = []\n    for subquestion in subquestions:\n        solved_questions.append(\n            SubquestionWithAnswers(\n                question=subquestion.question,\n                answer=solve(subquestion, solved_questions, question),\n            )\n        )\n\n    # Print\n    for item in solved_questions:\n        print(f\"{item.question} {item.answer}\")\n        #&gt; How old is Mohamed currently? 60\n        #&gt; How old was Mohamed four years ago? 56\n        #&gt; How old was Kody four years ago if he was half as old as Mohamed? 28\n        #&gt; How old is Kody currently? 32\n</code></pre>"},{"location":"prompting/decomposition/least_to_most/#references","title":"References","text":"<p><sup>1</sup>: Least-to-Most Prompting Enables Complex Reasoning in Large Language Models</p> <p><sup>*</sup>: The Prompt Report: A Systematic Survey of Prompting Techniques</p>"},{"location":"prompting/decomposition/plan_and_solve/","title":"Ditch Vanilla Chain Of Thought","text":"<p>Plan and Solve<sup>1</sup> improves the use of an improved Zero-Shot Chain Of Thought (CoT) prompt which adds more detailed instructions to the prompt given to these large language models.</p> <p>Plan and Solve Prompt</p> <p>[User Prompt]</p> <p>Let\u2019s first understand the problem, extract relevant variables and their corresponding numerals, and make a complete plan.Then, let\u2019s carry out the plan, calculate intermediate variables (pay attention to correct numerical calculation and commonsense), solve the problem step by step, and show the answer.</p> <p>[Model Response]</p> <p>Therefore the answer(arabic numerals) is</p> <p>This is a two step process which guides the LLM to pay more attention to calculation and intermediate results to ensure that they are correctly performed as much as possible.</p> <ol> <li>Generate Reasoning: In the first step we prompt the model with the user's query and prime the model using plan and solve prompting to explicitly devise a plan for solving a problem before generating an intermediate reasoning process</li> <li>Extract Answer : Once we've obtained the model's reasoning, we then extract the answer from a new prompt which includes the model's chain of thought.</li> </ol> <p></p> <p>We can implement this using <code>instructor</code> as seen below.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass Reasoning(BaseModel):\n    chain_of_thought: str\n\n\nclass Response(BaseModel):\n    correct_answer: str\n\n\ndef generate_reasoning(query: str):\n    return client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                &lt;user query&gt;\n                {query}\n                &lt;/user query&gt;\n\n                Let's first understand the problem,\n                extract relevant variables and their\n                corresponding numerals, and make a\n                complete plan. Then, let's carry out\n                the plan, calculate intermediate\n                variables (pay attention to correct\n                numerical calculation and commonsense),\n                solve the problem step by step, and\n                show the answer.\n                \"\"\",\n            },\n        ],\n        response_model=Reasoning,\n        model=\"gpt-4o\",\n    )\n\n\ndef extract_answer(query: str, reasoning: Reasoning):\n    return client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                &lt;user query&gt;\n                    {query}\n                &lt;/user query&gt;\n\n                Let's first understand the problem,\n                extract relevant variables and their\n                corresponding numerals, and make a\n                complete plan. Then, let's carry out\n                the plan, calculate intermediate\n                variables (pay attention to correct\n                numerical calculation and commonsense),\n                solve the problem step by step, and\n                show the answer.\n\n                &lt;reasoning&gt;\n                {reasoning.chain_of_thought}\n                &lt;/reasoning&gt;\n\n                Therefore the answer (arabic numerals) is\n                \"\"\",\n            }\n        ],\n        model=\"gpt-4o\",\n        response_model=Response,\n    )\n\n\nif __name__ == \"__main__\":\n    query = (\n        \"In a dance class of 20 students, 20% enrolled \"\n        \"in contemporary dance, 25% of the remaining \"\n        \"enrolled in jazz dance and the rest enrolled \"\n        \"in hip-hop dance. What percentage of the entire \"\n        \"students enrolled in hip-hop dance?\"\n    )\n\n    reasoning = generate_reasoning(query)\n    print(reasoning.model_dump_json(indent=2))\n    \"\"\"\n    {\n    \"chain_of_thought\": \"Let's first break down the\n    problem:\\n\\n1. Total number of students = 20\\n2.\n    Percentage enrolled in contemporary dance = 20%\\n\\n\n    Step-by-Step Plan:\\n1. Calculate the number of\n    students enrolled in contemporary dance.\\n2.\n    Calculate the remaining students after contemporary\n    dance enrollment.\\n3. Calculate the percentage and\n    number of students from the remaining who enrolled in\n    jazz dance.\\n4. Determine the remaining students who\n    enrolled in hip-hop dance.\\n5. Finally, calculate the\n    percentage of the entire students who enrolled in\n    hip-hop dance.\\n\\nLet's carry out the plan:\\n\\n1.\n    Number of students enrolled in contemporary dance =\n    20% of 20 = (20/100) * 20 = 4\\n2. Remaining students\n    after contemporary = 20 - 4 = 16\\n3. Percentage of\n    remaining students enrolled in jazz dance = 25%\\n\n    Number of students enrolled in jazz dance = 25% of 16\n    = (25/100) * 16 = 4\\n4. Remaining students after\n    contemporary and jazz = 16 - 4 = 12\\n5. The number of\n    students enrolled in hip-hop dance = 12\\n6.\n    Percentage of entire students enrolled in hip-hop =\n    (Number of hip-hop students / Total students) *\n    100\\n   Percentage = (12 / 20) * 100 = 60%\\n\\nThus,\n    60% of the entire students enrolled in hip-hop dance.\"\n    }\n    \"\"\"\n\n    response = extract_answer(query, reasoning)\n    print(response.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"correct_answer\": \"60\"\n    }\n    \"\"\"\n</code></pre>"},{"location":"prompting/decomposition/plan_and_solve/#references","title":"References","text":"<p><sup>1</sup>: Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models</p>"},{"location":"prompting/decomposition/program_of_thought/","title":"Generate Python for Intermediate Steps","text":"<p>Program of Thought aims to leverage an external python interpreter in order to generate intermediate reasoning steps. This helps us to achieve a greater degree of performance in mathematical and programming-related tasks by grounding our final response in deterministic code.</p> <p></p> <p>We can implement it in <code>instructor</code> as seen below</p> <pre><code>from openai import OpenAI\nfrom pydantic import BaseModel, Field, field_validator\nimport instructor\nfrom textwrap import dedent\nfrom typing import Literal\n\nclient = instructor.from_openai(OpenAI())\n\nprefix = \"\"\"\n# Answer this question by implementing a solver()\n# function, use for loop if necessary.\ndef solver():\n    # Let's write a Python program step by step,\n    # and then return the answer\n    # Firstly, we need to define the following\n    # variable:\n\"\"\".strip()\n\n\ndef execute_program(code: str):\n    code = code.strip() + \"\\nans = solver()\"\n    print(code)\n    \"\"\"\n    # Answer this question by implementing a\n    # solver() function, use for loop if necessary.\n    def solver():\n        # Let's write a Python program step by step,\n        # and then return the answer\n        # Firstly, we need to define the following\n        # variable:\n        selling_price = 360\n        profit_percentage = 20\n\n        # To find the cost price, use the formula:\n        # cost_price = selling_price / (1 + profit_percentage / 100)\n        cost_price = selling_price / (1 + profit_percentage / 100)\n\n        return cost_price\n\n    # Running the solver function to get the cost price\n    result = solver()\n    print(result)\n    ans = solver()\n    \"\"\"\n    exec(code)\n    locals_ = locals()\n    return locals_.get(\"ans\")\n\n\nclass Prediction(BaseModel):\n    choice: Literal[\"A\", \"B\", \"C\", \"D\", \"E\"]\n\n\nclass ProgramExecution(BaseModel):\n    program_code: str = Field(description=\"\"\"Program Code that\n    once executed contains the final answer\"\"\")\n\n    @field_validator(\"program_code\")\n    @classmethod\n    def ensure_valid_code(cls, v: str) -&gt; str:\n        if not v.startswith(prefix):\n            raise ValueError(\n                f\"\"\"Program Code must begin with the desired\n                prefix of {prefix}\"\"\"\n            )\n\n        answer = execute_program(v)\n        if not answer:\n            raise ValueError(\n                f\"\"\"Make sure to return the answer to the\n                question within the solver function\"\"\"\n            )\n\n        return str(answer)\n\n\ndef generate_intermediate_reasoning(query: str):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": dedent(\n                    f\"\"\"\n                You are a world class AI system that excels\n                at answering user queries in a systematic\n                and detailed manner. You are about to be\n                passed a user query to respond to. Make sure\n                to generate a valid program that can be\n                executed to answer the user query.\n\n                Make sure to begin your generated program\n                with the following prefix\n\n                {prefix}\n                \"\"\"\n                ),\n            },\n            {\n                \"role\": \"user\",\n                \"content\": query,\n            },\n        ],\n        response_model=ProgramExecution,\n    )\n\n\ndef generate_prediction(\n    predicted_answer: str, options: list[str], query: str\n) -&gt; Prediction:\n    formatted_options = \",\".join(options)\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Prediction,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": dedent(\n                    f\"\"\"\n                Find the closest options based on the\n                question and prediction.\n\n                Question: {query}\n                Prediction: {predicted_answer}\n                Options: [{formatted_options}]\n                \"\"\"\n                ),\n            }\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    query = \"\"\"A trader sold an article at a profit of 20%\n    for Rs.360. What is the cost price of the article?\"\"\"\n    reasoning = generate_intermediate_reasoning(query)\n    options = [\"A)270\", \"B)300\", \"C)280\", \"D)320\", \"E)315\"]\n    print(reasoning.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"program_code\": \"300.0\"\n    }\n    \"\"\"\n\n    prediction = generate_prediction(reasoning.program_code,\n    options, query)\n    print(prediction.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"choice\": \"B\"\n    }\n    \"\"\"\n</code></pre>"},{"location":"prompting/decomposition/recurs_of_thought/","title":"","text":"<p>[wip]</p>"},{"location":"prompting/decomposition/skeleton_of_thought/","title":"Generate in Parallel","text":"<p>How do we decrease the latency of an LLM pipeline?</p> <p>Skelelton-of-Thought is a technique which prompts an LLM to generate a skeleton outline of the response, then completes each point in the skeleton in parallel. The parallelism can be achieved by parallel API calls or batched decoding.</p> <p>Below is an example of an implementation using parallel API calls with <code>instructor</code>:</p> <pre><code>import instructor\nfrom pydantic import BaseModel\nfrom openai import AsyncOpenAI\nimport asyncio\n\nclient = instructor.from_openai(AsyncOpenAI())\n\n\nclass Point(BaseModel):\n    index: int\n    description: str\n\n\nclass Skeleton(BaseModel):\n    points: list[Point]\n\n\nclass Response(BaseModel):\n    response: str\n\n\nasync def get_skeleton(question):\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Skeleton,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                You\u2019re an organizer responsible for only giving the skeleton (not the full content) for answering the question.\n                Provide the skeleton in a list of points (numbered 1., 2., 3., etc.) to answer the question.\n                Instead of writing a full sentence, each skeleton point should be very short with only 3\u223c5 words.\n                Generally, the skeleton should have 3\u223c10 points.\n\n                Now, please provide the skeleton for the following question.\n\n                &lt;question&gt;\n                {question}\n                &lt;/question&gt;\n\n                Skeleton:\n                \"\"\",\n            }\n        ],\n    )\n\n\nasync def expand_point(question, skeleton, point_index):\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                You\u2019re responsible for continuing the writing of one and only one point in the overall answer to the following question.\n\n                &lt;question&gt;\n                {question}\n                &lt;/question&gt;\n\n                The skeleton of the answer is:\n\n                &lt;skeleton&gt;\n                {skeleton}\n                &lt;/skeleton&gt;\n\n                Continue and only continue the writing of point {point_index}.\n                Write it **very shortly** in 1\u223c2 sentence and do not continue with other points!\n                \"\"\",\n            }\n        ],\n    )\n\n\nasync def main():\n    query = \"Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.\"\n\n    # Step 1: Get the skeleton\n    skeleton = await get_skeleton(query)\n\n    for point in skeleton.points:\n        print(point)\n        #&gt; index=1 description='Introduction to Hawaii trip'\n        #&gt; index=2 description='Arrival and first impressions'\n        #&gt; index=3 description='Traditional Hawaiian cuisine'\n        #&gt; index=4 description='Exploring local markets'\n        #&gt; index=5 description='Visit to historic sites'\n        #&gt; index=6 description='Experience a Hawaiian luau'\n        #&gt; index=7 description='Day at the beach'\n        #&gt; index=8 description='Hiking adventures'\n        #&gt; index=9 description='Scenic viewpoints'\n        #&gt; index=10 description='Closing remarks and tips'\n\n    # Step 2: Expand on each point in parallel\n    tasks = [expand_point(query, skeleton, point.index) for point in skeleton.points]\n    responses = await asyncio.gather(*tasks)\n\n    for response in responses:\n        print(response.response)\n        \"\"\"\n        Hawaii\u2014a paradise of golden beaches, lush landscapes, and vibrant culture\u2014beckoned us with the promise of adventure and unforgettable experiences. Our journey began the moment we landed on this magical archipelago, ready to explore its unique blend of natural beauty and rich traditions.\n        \"\"\"\n        \"\"\"\n        The moment we landed in Hawaii, we were greeted with warm aloha spirit, lush tropical landscapes, and the gentle aroma of hibiscus flowers in the air.\n        \"\"\"\n        \"\"\"\n        The traditional Hawaiian cuisine was an exotic delight; from savoring the rich flavors of poke bowls to indulging in the sweet taste of haupia, every bite was a unique cultural experience.\n        \"\"\"\n        \"\"\"\n        Exploring local markets was a vibrant and delightful experience, where the air was filled with the scent of exotic fruits, freshly-made poke, and sounds of local musicians. We discovered unique handicrafts and interacted with friendly vendors eager to share their stories and traditions.\n        \"\"\"\n        \"\"\"\n        A visit to Pearl Harbor is a poignant reminder of the past, offering a chance to pay respects and learn about the events that shaped history. Walking through the USS Arizona Memorial and exploring the interactive exhibits was both humbling and enlightening.\n        \"\"\"\n        \"\"\"\n        Point 6: Experience a Hawaiian luau - Attending a traditional Hawaiian luau was unforgettable, filled with vibrant dances, soulful music, and a feast of mouthwatering dishes cooked in an imu (underground oven). It was a magical evening that immersed us in the heart of Hawaiian culture.\n        \"\"\"\n        \"\"\"\n        A day at the beach in Hawaii was pure bliss. The crystal-clear waters and soft sands were the perfect backdrop for both relaxation and adventure, from sunbathing to snorkeling.\n        \"\"\"\n        \"\"\"\n        Hiking adventures in Hawaii offer a unique chance to connect with nature, with trails leading to stunning waterfalls and lush rainforests. Don\u2019t miss out on the Na Pali Coast's breathtaking hikes!\n        \"\"\"\n        \"\"\"\n        One of the highlights of my trip was visiting the scenic viewpoints such as the Na Pali Coast and Haleakal\u0101 National Park, offering breathtaking panoramic views that are perfect for photography aficionados and nature lovers alike.\n        \"\"\"\n        \"\"\"\n        As you plan your trip, don't forget to pack plenty of sunscreen and a camera to capture every magical moment. Hawaii offers a unique blend of relaxation and adventure that's sure to leave you with unforgettable memories.\n        \"\"\"\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"prompting/decomposition/skeleton_of_thought/#references","title":"References","text":"<p><sup>1</sup>: Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation</p> <p><sup>*</sup>: The Prompt Report: A Systematic Survey of Prompting Techniques</p>"},{"location":"prompting/decomposition/tree-of-thought/","title":"","text":"<p>[wip]</p>"},{"location":"prompting/ensembling/cosp/","title":"Prioritize Consistent Examples","text":"<p>Consistency Based Self Adaptive Prompting (COSP)<sup>1</sup> aims to improve LLM output quality by generating high quality few shot examples to be included in the final prompt. These are examples without labelled ground truth so they use self-consistency and a metric known as normalized entropy to select the best examples.</p> <p>Once they've selected the examples, they then append them to the prompt and generate multiple reasoning chains before selecting the final result using Self-Consistency.</p>"},{"location":"prompting/ensembling/cosp/#cosp-process","title":"COSP process","text":"<p>How does this look in practice? Let's dive into greater detail.</p>"},{"location":"prompting/ensembling/cosp/#step-1-selecting-examples","title":"Step 1 - Selecting Examples","text":"<p>In the first step, we try to generate high quality examples from questions that don't have ground truth labels. This is challenging because we want to find a way to automatically determine answer quality when sampling our model multiple times.</p> <p>In this case, we have <code>n</code> questions which we want to generate <code>m</code> possible reasoning chains for each question. This gives a total of <code>nm</code> examples. We then want to filter out <code>k</code> final few shot examples from these <code>nm</code> examples to be included inside our final prompt.</p> <ol> <li>Using chain of thought, we first generate <code>m</code> responses for each question. These responses contain a final answer and a rationale behind that answer.</li> <li>We compute a score for each response using a weighted sum of two values - normalized entropy and repetitiveness ( How many times this rationale appears for this amswer )</li> <li>We rank all of our <code>nm</code> responses using this score and choose the <code>k</code> examples with the lowest scores as our final few shot examples.</li> </ol>"},{"location":"prompting/ensembling/cosp/#normalized-entropy","title":"Normalized Entropy","text":"<p>In the paper, the authors write that normalized entropy is a good proxy over a number of different tasks where low entropy is positively correlated with correctness. Entropy is also supposed to range from 0 to 1.</p> <p>Therefore in order to do so, we introduce a <code>-</code> term in our implementation so that the calculated values range from 0 to 1.</p> <p></p> <p>Assuming that for a specific question \\(x^{(i)}\\), we have generated \\(m\\) final answers of which \\(u\\) are unique. ( Note that this only cares about the answer itself and not the rationale )</p> \\[ \\mathcal{H}\\left(x^{(i)} \\mid \\left\\{\\hat{y}_j^{(i)}\\right\\}_{j=1}^m\\right) = \\frac{\\sum_{\\alpha=1}^u \\hat{p}\\left(\\hat{y}_{\\alpha}^{(i)}\\right) \\log \\hat{p}\\left(\\hat{y}_{\\alpha}^{(i)}\\right)}{\\log m}, \\] <p>We can measure the entropy of the generated responses using the formula above where</p> <ul> <li>\\(x_i\\) is the original question that we prompted the model with</li> <li>\\(y_j^{i}\\) represents the \\(i\\)-th sampled response from the \\(m\\) that we generated</li> <li>\\(\\hat{p}\\left(\\hat{y}_{\\alpha}^{(i)}\\right)\\) is the frequency of the unique answer in all the \\(m\\) generated answers. (Eg. if we generate 8 responses and 4 of them return the value 10, then \\(\\hat{p}\\left(\\hat{y}_{\\alpha}^{(i)}\\right)\\) is just going to be 0.5)</li> </ul>"},{"location":"prompting/ensembling/cosp/#repetitiveness","title":"Repetitiveness","text":"\\[ R_r(r_j^{(i)}) = \\frac{2}{Q(Q-1)} \\sum_{a=1}^{Q} \\sum_{b=a+1}^{Q} W_{ab} \\] <p>In the formula above, \\(Q\\) refers to the number of phrases in the sentence and \\(W_{ab}\\) refers to the cosine similarity of two phrases \\(a\\) and \\(b\\).</p> <p>Repetitiveness aims to measure how often the language model repeats itself. To do so, the paper sums up the cosine similarity between each sentence inside the generated chain of thought rationale before normalizing it.</p> <p>The intuition behind this is that high repetitiveness indicates redundancy, which can lead to poorer performance. Therefore responses with a high number of similar sentences will have a larger score for repetitiveness ( since cosine similarity will be larger for each sentence ).</p>"},{"location":"prompting/ensembling/cosp/#step-2-self-consistency","title":"Step 2 - Self Consistency","text":"<p>We now take our <code>k</code> responses and append them to our prompt. We then sample our model multiple times using this new prompt and take the majority vote as the answer.</p>"},{"location":"prompting/ensembling/cosp/#implementation","title":"Implementation","text":"<p>Now that we understand what COSP is, let's see how we can implement it in instructor. Note that here we'll measure repetitiveness using cosine similarity between sentence embeddings.</p> <pre><code>import instructor\nfrom pydantic import BaseModel\nfrom openai import AsyncOpenAI, OpenAI\nfrom collections import defaultdict, Counter\nimport asyncio\nfrom textwrap import dedent\nimport math\n\nclient = instructor.from_openai(AsyncOpenAI())\n\n\nclass Response(BaseModel):\n    chain_of_thought: list[str]\n    answer: int\n\n\nclass ResponseScore(BaseModel):\n    query: str\n    response: Response\n    score: float\n\n    def format_response(self):\n        return dedent(\n            f\"\"\"\n            Q: {self.query}\n            A: {''.join(self.response.chain_of_thought)}. Therefore the answer is {self.response.answer}.\n            \"\"\"\n        )\n\n\ndef cosine_similarity(vec1: list[float], vec2: list[float]):\n    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n    magnitude1 = math.sqrt(sum(a * a for a in vec1))\n    magnitude2 = math.sqrt(sum(b * b for b in vec2))\n\n    if magnitude1 * magnitude2 == 0:\n        return 0  # Handle the case of zero vectors\n\n    return dot_product / (magnitude1 * magnitude2)\n\n\ndef score_repetitiveness(prediction: Response):\n    if len(prediction.chain_of_thought) == 1:\n        return 0\n\n    embedding = OpenAI().embeddings.create(\n        input=prediction.chain_of_thought, model=\"text-embedding-3-small\"\n    )\n    embedding = [item.embedding for item in embedding.data]\n\n    ttl = 0\n    num_comparisons = 0\n    for idx in range(len(embedding)):\n        for idx2 in range(idx + 1, len(embedding)):\n            ttl += cosine_similarity(embedding[idx], embedding[idx2])\n            num_comparisons += 1\n\n    return ttl / num_comparisons if num_comparisons &gt; 0 else 0\n\n\nasync def generate_cot_response(query: str) -&gt; tuple[Response, str]:\n    return (\n        await client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=[{\"role\": \"user\", \"content\": query}],\n            response_model=Response,\n            temperature=0.4,\n        ),\n        query,\n    )\n\n\nasync def generate_batch_cot_responses(\n    queries: list[str], m: int\n) -&gt; list[tuple[Response, str]]:\n    coros = [generate_cot_response(query) for query in queries for _ in range(m)]\n    return await asyncio.gather(*coros)\n\n\ndef score_entropy(predictions: list[Response]):\n    counter = Counter([prediction.answer for prediction in predictions])\n\n    prob = [counter[i] / len(predictions) for i in counter]\n\n    numer = -sum([p * math.log(p) for p in prob])\n    denom = math.log(len(predictions))\n\n    return numer / denom\n\n\ndef score_responses(\n    predictions: list[tuple[Response, str]], trade_off_param: float\n) -&gt; list[ResponseScore]:\n    query_to_responses: dict[str, list[Response]] = defaultdict(list)\n    for prediction, query in predictions:\n        query_to_responses[query].append(prediction)\n\n    query_to_entropy = {\n        query: score_entropy(predictions)\n        for query, predictions in query_to_responses.items()\n    }\n\n    return [\n        ResponseScore(\n            query=query,\n            response=prediction,\n            score=query_to_entropy[query]\n            + trade_off_param * score_repetitiveness(prediction),\n        )\n        for prediction, query in predictions\n    ]\n\n\ndef get_top_k_examples(queries: list[ResponseScore], k: int):\n    \"\"\"\n    This gets the top k responses that have the minimum possible score\n    \"\"\"\n    sorted_responses = sorted(queries, key=lambda x: x.score)\n    return sorted_responses[:k]\n\n\nasync def generate_answer_with_examples(query: str, examples: list[ResponseScore]):\n    formatted_examples = \"\\n\".join([example.format_response() for example in examples])\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": dedent(\n                    f\"\"\"\n                You are a world class AI system that excels at answering user queries\n\n                &lt;query&gt;\n                {query}\n                &lt;/query&gt;\n\n                &lt;examples&gt;\n                {formatted_examples}\n                &lt;/examples&gt;\n                \"\"\"\n                ),\n            }\n        ],\n        response_model=Response,\n    )\n\n\nasync def generate_final_answers(\n    query: str, examples: list[ResponseScore], number_samples: int\n):\n    coros = [\n        generate_answer_with_examples(query, examples) for _ in range(number_samples)\n    ]\n\n    return await asyncio.gather(*coros)\n\n\nif __name__ == \"__main__\":\n    query = (\n        \"The schools debate team had 5 boys and 40 girls on it. \"\n        \"If they were split into groups of 9 how many groups \"\n        \"could they make?\"\n    )\n\n    example_questions = [\n        (\n            \"Debby's class is going on a field trip to the zoo. \"\n            \"If each van can hold 4 people and there are 2 students \"\n            \"and 6 adults going, how many vans will they need?\"\n        ),\n        (\n            \"Nancy had 80 files on her computer. She deleted 31 of \"\n            \"them and put the rest into folders with 7 files in each \"\n            \"one. How many folders did Nancy end up with?\"\n        ),\n        (\n            \"At the arcade, Tom won 32 tickets playing 'whack a mole' \"\n            \"and 25 tickets playing 'skee ball'. If he spent 7 of his \"\n            \"tickets on a hat, how many tickets does Tom have left?\"\n        ),\n    ]\n\n    m = 2  # Number of Reasoning Chains per example ( Step 1 )\n    k = 3  # Number of Examples to include in final prompt (Step 2)\n    n = 2  # Number of Reasoning Chains For Self-Consistency ( Step 2 )\n\n    # Step 1 : Generate the examples\n    responses = asyncio.run(generate_batch_cot_responses(example_questions, m))\n    scored_responses = score_responses(responses, 0.2)\n\n    chosen_examples = get_top_k_examples(scored_responses, k)\n\n    # Step 2 : Run Self-Consistency\n    final_responses = asyncio.run(generate_final_answers(query, chosen_examples, n))\n\n    c = Counter([response.answer for response in final_responses])\n    answer = c.most_common(1)[0][0]\n\n    print(answer)\n    #&gt; 5\n</code></pre>"},{"location":"prompting/ensembling/cosp/#references","title":"References","text":"<p><sup>1</sup>: Better Zero-Shot Reasoning with Self-Adaptive Prompting</p>"},{"location":"prompting/ensembling/dense/","title":"Use Distinct Example Subsets","text":"<p>We can maximise the use of our examples by prompting our model multiple times, each time using a different subset of examples. We can then take these multiple outputs and aggregate over them to generate a final response. This is known as Demonstration Ensembling ( DENSE ) <sup>1</sup>.</p> <p>For simplicity in this example, we simply iterate over the examples and partition them equally to get equally sized clusters. However, depending on your use-case you might also want to consider sampling these using some form of embedding clusering.</p> <p>We can implement this using <code>instructor</code> as seen below.</p> <pre><code>import instructor\nfrom pydantic import BaseModel\nfrom openai import AsyncOpenAI\nimport asyncio\nfrom collections import Counter\nfrom typing import Literal\nfrom textwrap import dedent\n\n\nclass DemonstrationResponse(BaseModel):\n    correct_answer: Literal[\"Positive\", \"Negative\", \"Neutral\"]\n\n\nclient = instructor.from_openai(AsyncOpenAI())\n\n\nasync def generate_self_consistent_response(prompt: str, examples: list[str]):\n    concetenated_examples = \"\\n\".join(examples)\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": dedent(\n                    f\"\"\"\n                You are an intelligent AI System that excels\n                at classifying user queries into three\n                possible labels:\n                - Positive\n                - Negative\n                - Neutral\n\n                You are about to be given a user query and\n                asked to classify it into one of the three\n                categories. Make sure to refer closely to\n                the examples provided to you, examining each\n                individual example before coming up with the\n                final answer.\n\n                Here are the examples:\n                {concetenated_examples}\n                \"\"\"\n                ),\n            },\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n        response_model=DemonstrationResponse,\n        temperature=0,\n    )\n\n\nasync def generate_self_consistent_responses(\n    prompt: str, num_responses: int, examples: list[str]\n):\n    assert (\n        len(examples) % num_responses == 0\n    ), \"The number of examples must be evenly divisible by num_responses\"\n\n    # Batch the examples into num_responses batches\n    batch_size = len(examples) // num_responses\n\n    coros = [\n        generate_self_consistent_response(prompt, examples[i : i + batch_size])\n        for i in range(0, len(examples), batch_size)\n    ]\n\n    responses = await asyncio.gather(*coros)\n    return responses\n\n\nif __name__ == \"__main__\":\n    user_query = \"What is the weather like today?\"\n    examples = [\n        \"I love this product! [Positive]\",\n        \"This is the worst service ever. [Negative]\",\n        \"The movie was okay, not great but not terrible. [Neutral]\",\n        \"I'm so happy with my new phone! [Positive]\",\n        \"The food was terrible and the service was slow. [Negative]\",\n        \"It's an average day, nothing special. [Neutral]\",\n        \"Fantastic experience, will come again! [Positive]\",\n        \"I wouldn't recommend this to anyone. [Negative]\",\n        \"The book was neither good nor bad. [Neutral]\",\n        \"Absolutely thrilled with the results! [Positive]\",\n    ]\n    responses = asyncio.run(generate_self_consistent_responses(user_query, 5, examples))\n    answer_counts = Counter([response.correct_answer for response in responses])\n    most_common_answer, _ = answer_counts.most_common(1)[0]\n    print(most_common_answer)\n    #&gt; Neutral\n</code></pre>"},{"location":"prompting/ensembling/dense/#references","title":"References","text":"<p><sup>1</sup>: Exploring Demonstration Ensembling for In Context Learning</p>"},{"location":"prompting/ensembling/diverse/","title":"Verify Responses over Majority Voting","text":"<p>Diverse Verifier On Reasoning Step (DiVeRSe)<sup>1</sup> is a prompting technique which provides two main improvements</p> <ol> <li>Diverse Prompts : They generate multiple variations of the same prompt by varying the examples used in each prompt</li> <li>Verification : They use a finetuned <code>Deberta-V3-Large</code> to determine the quality of a generated response. Instead of using majority voting, they use their model to score each generated response from 0 to 1. They then aggregate these scores for each unique answer to determine the best generated solution.</li> </ol> <p>In the paper itself, they also train a step-wise verifier that is able to score each individual reasoning step. This enables much more fine-grained predictions but is challenging to obtain training data for.</p> <p>We can implement this in <code>instructor</code>. However, instead of using a <code>deberta-v3-large</code> model, we'll be using gpt-4o to score its own outputs and generate a quality score.</p> <pre><code>import instructor\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel\nfrom typing import Literal\nfrom textwrap import dedent\nimport asyncio\nfrom collections import defaultdict\n\nclient = instructor.from_openai(AsyncOpenAI())\n\n\nclass Response(BaseModel):\n    chain_of_thought: str\n    answer: int\n\n\nclass Grading(BaseModel):\n    grade: Literal[\"Poor\", \"Average\", \"Good\", \"Excellent\"]\n\n    def get_score(self):\n        mapping = {\n            \"Poor\": 0.25,\n            \"Average\": 0.5,\n            \"Good\": 0.75,\n            \"Excellent\": 1,\n        }\n        return mapping[self.grade]\n\n\nasync def generate_response(query: str, examples: list[str]):\n    formatted_examples = \"\\n\".join(examples)\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": dedent(\n                    f\"\"\"\n                You are a world class AI that excels at answering\n                user queries in a succint and accurate manner.\n\n                &lt;query&gt;\n                {query}\n                &lt;/query&gt;\n\n                &lt;examples&gt;\n                {formatted_examples}\n                &lt;/examples&gt;\n                \"\"\"\n                ),\n            }\n        ],\n        response_model=Response,\n    )\n\n\nasync def score_response(query: str, response: Response) -&gt; tuple[Response, Grading]:\n    return (\n        response,\n        await client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": dedent(\n                        f\"\"\"\n                You are a world class AI that excels at grading\n                responses to a user query in a succint and clear\n                manner.\n\n                &lt;query&gt;\n                {query}\n                &lt;/query&gt;\n\n                &lt;response&gt;\n                {response}\n                &lt;/response&gt;\n                \"\"\"\n                    ),\n                }\n            ],\n            response_model=Grading,\n        ),\n    )\n\n\nasync def generate_response_batch(\n    query: str, examples: list[str], n_examples_per_batch: int\n):\n    batches: list[list[str]] = []\n    for i in range(0, len(examples), n_examples_per_batch):\n        batches.append(examples[i : i + n_examples_per_batch])\n\n    coros = [generate_response(query, example_batch) for example_batch in batches]\n    return await asyncio.gather(*coros)\n\n\nasync def score_responses(\n    query: str, responses: list[Response]\n) -&gt; list[tuple[Response, Grading]]:\n    coros = [score_response(query, response) for response in responses]\n    return await asyncio.gather(*coros)\n\n\nif __name__ == \"__main__\":\n    examples = [\n        \"\"\"\n        Q: James decides to run 3 sprints 3 times a week.\n        He runs 60 meters each sprint. How many total\n        meters does he run a week?\n        A: James decides to run 3 sprints 3 times a week.\n        He runs 60 meters each sprint. So he runs 60 meters\n        x 3 sprints x 3 times a week. That is 60 meters x 9.\n        The answer is 540.\n        \"\"\",\n        \"\"\"\n        Q: Brandon's iPhone is four times as old as Ben's\n        iPhone. Ben's iPhone is two times older than Suzy's\n        iPhone. If Suzy's iPhone is 1 year old, how old is\n        Brandon's iPhone?\n        A: Brandon's iPhone is 4 times as old as Ben's\n        iPhone. Ben's iPhone is 2 times older than Suzy's\n        iPhone. So Brandon's iPhone is 4 x 2 = 8 times older\n        than Suzy's iPhone. Suzy's iPhone is 1 year old. So\n        Brandon's iPhone is 8 x 1 = 8 years old. The answer\n        is 8.\n        \"\"\",\n        \"\"\"\n        Q: Jean has 30 lollipops. Jean eats 2 of the\n        lollipops. With the remaining lollipops, Jean wants\n        to package 2 lollipops in one bag. How many bags can\n        Jean fill?\n        A: Jean started with 30 lollipops. She ate 2 of\n        them. So she has 28 lollipops left. She wants to\n        package 2 lollipops in one bag. So she can package\n        28 / 2 = 14 bags. The answer is 14.\n        \"\"\",\n        \"\"\"\n        Q: Weng earns $12 an hour for babysitting.\n        Yesterday, she just did 50 minutes of babysitting.\n        How much did she earn?\n        A: Weng earns 12/60 = $&lt;&lt;12/60=0.2&gt;&gt;0.2 per minute.\n        Working 50 minutes, she earned 0.2 x 50 =\n        $&lt;&lt;0.2*50=10&gt;&gt;10. The answer is 10\n        \"\"\",\n    ]\n\n    query = \"\"\"Betty is saving money for a new wallet which\n    costs $100. Betty has only half of the money she needs.\n    Her parents decided to give her $15 for that purpose,\n    and her grandparents twice as much as her parents. How\n    much more money does Betty need to buy the wallet?\"\"\"\n\n    generated_responses = asyncio.run(generate_response_batch(query, examples, 1))\n\n    scored_responses = asyncio.run(score_responses(query, generated_responses))\n\n    scores: dict[int, float] = defaultdict(int)\n\n    for response, grade in scored_responses:\n        scores[response.answer] += grade.get_score()\n\n    print(scores)\n    #&gt; defaultdict(&lt;class 'int'&gt;, {5: 3.5})\n\n    answer = max(scores, key=scores.get)\n    print(answer)\n    #&gt; 5\n</code></pre>"},{"location":"prompting/ensembling/diverse/#references","title":"References","text":"<p><sup>1</sup>: Making Language Models Better Reasoners with Step-Aware Verifier</p>"},{"location":"prompting/ensembling/max_mutual_information/","title":"Use Ensembles To Test Prompts","text":""},{"location":"prompting/ensembling/max_mutual_information/#whats-max-mutual-information","title":"What's Max Mutual Information?","text":"<p>Max Mutual Information Method is a method of prompting that aims to find the best prompt to elicit the desired response from a LLM. We do so by maximising a metric called Mutual Information - which indicates the reduction in a model's uncertainty as a result of the prompt.</p>"},{"location":"prompting/ensembling/max_mutual_information/#entropy","title":"Entropy","text":"<p>When a language model recieves a prompt as input, it outputs a series of token probabilities sequentially until it reaches the <code>&lt;EOS&gt;</code> token. In the paper, they take the final probability distribution as \\(P(Y|X)\\) where \\(Y\\) is the final prediction of the model and \\(X\\) the prompt.</p> <p>When we have a probability distribution, we can calculate a probability known as entropy. The lower this value is, the better. This is because a lower entropy value means that the model is more confident in its prediction.</p> <p>We can calculate entropy with the following formula where \\(P(T_i)\\) represents the probability of the \\(i\\)-th token in the final output distribution.</p> \\[ H(P(Y|X)) = \\sum_{i=0}^n P(T_i) log (P(T_i)) \\]"},{"location":"prompting/ensembling/max_mutual_information/#mutual-information","title":"Mutual Information","text":"<p>We can apply this to the calculation of Mutual Information as seen above.</p> <p>We'll indicate the calculate of entropy of a probability distribution as \\(H(X)\\) where \\(X\\) here represents a final probability distribution. We also assume you have a train dataset of \\(n\\) examples to use.</p> <ol> <li> <p>First, we choose a set of tokens that are likely to be part of the final answer. This could be words that appear inside the choices we have provided.</p> </li> <li> <p>Once we've chosen these tokens, we extract out the log probs for each token from our final distribution. We then normalise it so that these new log probs now sum up to 1.</p> </li> <li> <p>We do this for the \\(n\\) example inside our train set, this gives us a new distribution \\(P(Y_i|X_i)\\) for each \\(i\\)-th example.</p> </li> <li> <p>We then take the average of these \\(n\\) distributions to get \\(H_{marginal}\\)</p> </li> <li> <p>We then calculate the average of the entropy of each distribution to get \\(H_{conditional}\\)</p> </li> <li> <p>We then derive the Mutual Information by taking \\(H_{marginal} - H_{conditional}\\), the higher this metric the better.</p> </li> </ol> Unsure how to calculate \\(H_{marginal}\\) and \\(H\\_{conditional}\\) \\[     H_{marginal} = H(\\frac{1}{n} \\sum_{i=0}^n P(Y_i | X_i) ) \\] \\[     H_{conditional} = \\frac{1}{n} \\sum_{i=0}^n H(P(Y_i|X_i)) \\] <p>We can then use this new mutual information metric to compare the effectiveness of different prompts at eliciting a desired response from our train dataset.</p>"},{"location":"prompting/ensembling/max_mutual_information/#implementation","title":"Implementation","text":"<p>Since we don't have access to the raw log probabilites of specific tokens we want in the OpenAI API, we'll instead get the language model to generate a final score from 1 - 10 of its confidence in it's prediction.</p> <p>We'll then convert this to a probability distribution with two outcomes and calculate a value for the entropy off of that.</p> <p>Next we'll compare the Mutual Information value for different prompts before choosing what the best prompt is. For this example, we'll be using values from the Story Cloze set.</p> <pre><code>from openai import AsyncOpenAI\nfrom instructor import from_openai\nfrom pydantic import BaseModel\nfrom typing import Callable, Literal\nfrom textwrap import dedent\nimport math\nimport asyncio\n\n\nclass Response(BaseModel):\n    chain_of_thought: str\n    response: Literal[\"A\", \"B\"]\n    confidence: Literal[\n        \"Very High Confidence\",\n        \"High Confidence\",\n        \"Moderate Confidence\",\n        \"Low Confidence\",\n        \"Very Low Confidence\",\n    ]\n\n    def generate_score(self) -&gt; float:\n        confidence_scores = {\n            \"Very High Confidence\": 1,\n            \"High Confidence\": 0.8,\n            \"Moderate Confidence\": 0.6,\n            \"Low Confidence\": 0.4,\n            \"Very Low Confidence\": 0.2,\n        }\n        return confidence_scores[self.confidence]\n\n\nclient = from_openai(AsyncOpenAI())\n\n\ndef prompt_template_1(question: str, options: list[str]):\n    assert len(options) == 2\n    a, b = options\n\n    return dedent(\n        f\"\"\"\n    You are a world class AI System which excels at understanding complex user stories and generating responses. Output your prediction and also quantify your confidence in your prediction with the following scale.\n\n    - Very High Confidence: The model is highly confident in its prediction, displaying deep understanding, flawless execution, and no noticeable errors.\n    - High Confidence: The model is confident in its prediction, with strong relevance and minor errors that do not detract from overall quality.\n    - Moderate Confidence: The model has moderate confidence in its prediction, which is generally relevant with some inaccuracies, and meets minimum requirements.\n    - Low Confidence: The model has low confidence in its prediction, with limited relevance and several inaccuracies.\n    - Very Low Confidence: The model has very low confidence in its prediction, which is largely irrelevant, inaccurate, or incomplete, needing significant improvement\n\n\n    Context\n    {question}\n\n    Options\n    A. {a}\n    B. {b}\n    \"\"\"\n    )\n\n\ndef prompt_template_2(question: str, options: list[str]):\n    assert len(options) == 2\n    a, b = options\n\n    return dedent(\n        f\"\"\"\n    &lt;prompt&gt;\n        &lt;Task&gt;\n        You are about to be passed a story. You are to select the correct response from the options provided.\n\n         &lt;confidence-levels&gt;\n             &lt;level&gt;\n                 &lt;name&gt;Very High Confidence&lt;/name&gt;\n                 &lt;description&gt;The model is highly confident in its prediction, displaying deep understanding, flawless execution, and no noticeable errors.&lt;/description&gt;\n             &lt;/level&gt;\n             &lt;level&gt;\n                 &lt;name&gt;High Confidence&lt;/name&gt;\n                 &lt;description&gt;The model is confident in its prediction, with strong relevance and minor errors that do not detract from overall quality.&lt;/description&gt;\n             &lt;/level&gt;\n             &lt;level&gt;\n                 &lt;name&gt;Moderate Confidence&lt;/name&gt;\n                 &lt;description&gt;The model has moderate confidence in its prediction, which is generally relevant with some inaccuracies, and meets minimum requirements.&lt;/description&gt;\n             &lt;/level&gt;\n             &lt;level&gt;\n                 &lt;name&gt;Low Confidence&lt;/name&gt;\n                 &lt;description&gt;The model has low confidence in its prediction, with limited relevance and several inaccuracies.&lt;/description&gt;\n             &lt;/level&gt;\n             &lt;level&gt;\n                 &lt;name&gt;Very Low Confidence&lt;/name&gt;\n                 &lt;description&gt;The model has very low confidence in its prediction, which is largely irrelevant, inaccurate, or incomplete, needing significant improvement&lt;/description&gt;\n             &lt;/level&gt;\n         &lt;/confidence-levels&gt;\n        &lt;/Task&gt;\n\n        &lt;Question&gt;\n        {question}\n        &lt;/Question&gt;\n\n        &lt;Options&gt;\n        &lt;option&gt;A: {a}&lt;/option&gt;\n        &lt;option&gt;B: {b}&lt;/option&gt;\n        &lt;/Options&gt;\n    &lt;/prompt&gt;\n    \"\"\"\n    )\n\n\nasync def generate_response(\n    question: str, options: list[str], prompt_template: Callable[[str, list[str]], str]\n):\n    return await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": prompt_template(question, options),\n            }\n        ],\n        response_model=Response,\n    )\n\n\nasync def generate_responses(\n    questions: list[str], prompt_template: Callable[[str, list[str]], str]\n):\n    return await asyncio.gather(\n        *[\n            generate_response(\n                question=question[\"question\"],\n                options=question[\"options\"],\n                prompt_template=prompt_template,\n            )\n            for question in questions\n        ]\n    )\n\n\ndef calculate_entropy(probs: list[float]) -&gt; float:\n    return sum([p * math.log(p) if p != 0 else 0 for p in probs])\n\n\ndef calculate_mutual_information(predictions: list[Response]) -&gt; float:\n    probs = [\n        [prediction.generate_score(), 1 - prediction.generate_score()]\n        for prediction in predictions\n    ]\n\n    avg_probs = [0, 0]\n\n    for p1, p2 in probs:\n        avg_probs[0] += p1\n        avg_probs[1] += p2\n\n    h_marginal = calculate_entropy([i / len(probs) for i in avg_probs])\n    h_conditional = sum([calculate_entropy(prob) for prob in probs]) / len(probs)\n\n    return h_marginal - h_conditional\n\n\nif __name__ == \"__main__\":\n    queries = [\n        {\n            \"question\": \"Karen was assigned a roommate her first year of college. Her roommate asked her to go to a nearby city for a concert. Karen agreed happily. The show was absolutely exhilarating.\",\n            \"options\": [\n                \"Karen became good friends with her roommate.\",\n                \"Karen hated her roommate.\",\n            ],\n        },\n        {\n            \"question\": \"Jim got his first credit card in college. He didn\u2019t have a job so he bought everything on his card. After he graduated he amounted a $10,000 debt. Jim realized that he was foolish to spend so much money.    \",\n            \"options\": [\n                \"Jim decided to devise a plan for repayment.\",\n                \"Jim decided to open another credit card.\",\n            ],\n        },\n        {\n            \"question\": \"Gina misplaced her phone at her grandparents. It wasn\u2019t anywhere in the living room. She realized she was in the car before. She grabbed her dad\u2019s keys and ran outside.\",\n            \"options\": [\n                \"She found her phone in the car.\",\n                \"She didn\u2019t want her phone anymore.\",\n            ],\n        },\n    ]\n\n    best_mi_score = float(\"-inf\")\n    best_template = None\n\n    for prompt_template in [prompt_template_1, prompt_template_2]:\n        responses = asyncio.run(generate_responses(queries, prompt_template))\n        mi_score = calculate_mutual_information(responses)\n        print(f\"{prompt_template.__name__}: {mi_score}\")\n        #&gt; prompt_template_1: -0.0781292189485728\n        #&gt; prompt_template_2: -0.05907285153542691\n        if mi_score &gt; best_mi_score:\n            best_mi_score = mi_score\n            best_template = prompt_template.__name__\n\n    print(best_template, best_mi_score)\n    #&gt; prompt_template_2 -0.05907285153542691\n</code></pre>"},{"location":"prompting/ensembling/meta_cot/","title":"Combine Multiple Reasoning Chains","text":"<p>Meta Chain Of Thought (Meta COT) <sup>1</sup>. involves the use of multiple reasoning chains to generate a response to a given query. This helps our model evaluate multiple potential reasoning paths and from there, determine a more accurate answer.</p> <p>We can implement this using <code>instructor</code> as seen below.</p> <pre><code>import instructor\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel, Field\nimport asyncio\nfrom typing import Optional\n\nclient = instructor.from_openai(AsyncOpenAI())\n\n\nclass ReasoningAndResponse(BaseModel):\n    intermediate_reasoning: str = Field(\n        description=\"\"\"\n    Intermediate reasoning steps\"\"\"\n    )\n    correct_answer: str\n\n\nclass MaybeResponse(BaseModel):\n    result: Optional[ReasoningAndResponse]\n    error: Optional[bool]\n    error_message: Optional[str] = Field(\n        description=\"\"\"Informative explanation of why\n        the reasoning chain was unable to generate\n        a result\"\"\"\n    )\n\n\nclass QueryDecomposition(BaseModel):\n    queries: list[str] = Field(\n        description=\"\"\"A list of queries that need to be\n        answered in order to derive the final answer\"\"\"\n    )\n\n\nasync def generate_queries(query: str):\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are a helpful assistant that\n                decomposes a query into multiple sub-queries.\"\"\",\n            },\n            {\"role\": \"user\", \"content\": query},\n        ],\n        response_model=QueryDecomposition,\n    )\n\n\nasync def generate_reasoning_chain(query: str) -&gt; MaybeResponse:\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"\n                Given a question and a context,\n                answer the question step-by-step.\n\n                Indicate the intermediate reasoning\n                steps.\n                \"\"\",\n            },\n            {\"role\": \"user\", \"content\": query},\n        ],\n        response_model=MaybeResponse,\n    )\n\n\nasync def batch_reasoning_chains(\n    queries: list[str],\n) -&gt; list[MaybeResponse]:\n    coros = [generate_reasoning_chain(query) for query in queries]\n    results = await asyncio.gather(*coros)\n    return results\n\n\nasync def generate_response(query: str, context: list[MaybeResponse]):\n    formatted_context = \"\\n\".join(\n        [\n            f\"\"\"\n            {item.result.intermediate_reasoning}\n            {item.result.correct_answer}\n            \"\"\"\n            for item in context\n            if not item.error and item.result\n        ]\n    )\n\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"\n                Given a question and a context,\n                answer the question step-by-step.\n\n                If you are unsure, answer Unknown.\n                \"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                    &lt;question&gt;\n                    {query}\n                    &lt;/question&gt;\n                    &lt;context&gt;\n                    {formatted_context}\n                    &lt;/context&gt;\n                    \"\"\",\n            },\n        ],\n        response_model=ReasoningAndResponse,\n    )\n\n\nif __name__ == \"__main__\":\n    query = \"\"\"Would Arnold Schwarzenegger have been\n    able to deadlift an adult Black rhinoceros at his\n    peak strength?\"\"\"\n    decomposed_queries = asyncio.run(generate_queries(\n        query))\n\n    for generated_query in decomposed_queries.queries:\n        print(generated_query)\n        #&gt; How much weight could Arnold Schwarzenegger\n        #&gt; deadlift at his peak strength?\n        #&gt; What is the average weight of an adult Black\n        #&gt; rhinoceros?\n\n    chains = asyncio.run(batch_reasoning_chains(\n        decomposed_queries.queries))\n\n    for chain in chains:\n        print(chain.model_dump_json(indent=2))\n        \"\"\"\n        {\n          \"result\": {\n            \"intermediate_reasoning\": \"Determining Arnold\n            Schwarzenegger's peak deadlift involves\n            researching historical records, interviews,\n            and Arnold\u2019s competitive powerlifting\n            results.\",\n            \"correct_answer\": \"Arnold Schwarzenegger's\n            peak deadlift was reportedly 710 lbs (322\n            kg).\"\n          },\n          \"error\": false,\n          \"error_message\": null\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"result\": {\n            \"intermediate_reasoning\": \"To determine the\n            average weight of an adult Black rhinoceros,\n            I need to consult reliable sources such as\n            wildlife encyclopedias, zoological databases,\n            or scientific articles. Commonly, the average\n            weight of adult Black rhinoceros ranges\n            between 800 to 1,400 kg.\",\n            \"correct_answer\": \"The average weight of an\n            adult Black rhinoceros ranges between 800 to\n            1,400 kg.\"\n          },\n          \"error\": false,\n          \"error_message\": null\n        }\n        \"\"\"\n\n    response = asyncio.run(generate_response(query,\n        chains))\n\n    print(response.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"intermediate_reasoning\": \"Arnold Schwarzenegger's\n      peak deadlift was 710 lbs (322 kg). The average\n      weight of an adult Black rhinoceros ranges between\n      800 to 1,400 kg (1764 to 3086 lbs). Even at the\n      lower end of the rhinoceros weight range (800 kg\n      or 1764 lbs), it exceeds Arnold Schwarzenegger's\n      peak deadlift capacity of 710 lbs (322 kg).\n      Therefore, Arnold Schwarzenegger would not have\n      been able to deadlift an adult Black rhinoceros at\n      his peak strength.\",\n      \"correct_answer\": \"No\"\n    }\n    \"\"\"\n</code></pre>"},{"location":"prompting/ensembling/meta_cot/#references","title":"References","text":"<p><sup>1</sup>: Answering Questions by Meta-Reasoning over Multiple Chains of Thought</p>"},{"location":"prompting/ensembling/more/","title":"Combine Different Specialized LLMs","text":"<p>Language Models struggle to generalize across question types that require distinct reasoning abilities. By combining a variety of different specialized language models, we can improve the quality of our responses. This is done through a technique called Mixture Of Reasoning Experts (MoRE).</p> <p>In the original paper, they utilise four different experts</p> <ol> <li> <p>Factual Expert : This is a model that is augmented by a RAG prompting pipeline. WHen it recieves a query, it retrieves the top 10 most relevant passages from Wikipedia and appends them to the prompt right before the question.</p> </li> <li> <p>Multihop Expert : This is an expert that has manually written rationales after each demo to elicit multi-step reasoning processes for the questions</p> </li> <li> <p>Math Expert : This is an expert that has manually written explanations for the GSM8k Dataset to bias the model towards different reasoning steps</p> </li> <li> <p>Commonsense expert: This is an expert that is provided with 10 different facts that are generated by a Codex model which are appended to the prompt right before the question</p> </li> </ol> <p></p> <p>Once each expert has genearted a response, they then use a random forest classifier to score it from 0 to 1. This is then used for selecting the final answer and determining if we've generated a sufficiently good answer ( Since we have the option to abstain at each point )</p> <p>We can implement a simplified version of MoRE with <code>instructor</code> with a few modifications.</p> <pre><code>from openai import OpenAI\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom textwrap import dedent\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass MultihopExpert(BaseModel):\n    chain_of_thought: str\n    answer: str\n\n\nclass FactualExpert(BaseModel):\n    answer: str\n\n\nclass ModelScore(BaseModel):\n    score: float = Field(ge=0, lt=1)\n\n\ndef query_factual_expert(query: str, evidence: list[str]):\n    formatted_evidence = \"\\n-\".join(evidence)\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=FactualExpert,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": dedent(\n                    f\"\"\"\n                &lt;query&gt;\n                {query}\n                &lt;/query&gt;\n\n                &lt;evidences&gt;\n                {formatted_evidence}\n                &lt;/evidences&gt;\n                \"\"\"\n                ),\n            }\n        ],\n    )\n\n\ndef query_multihop_expert(query: str):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=MultihopExpert,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": dedent(\n                    f\"\"\"\n                &lt;query&gt;\n                {query}\n                &lt;/query&gt;\n                \"\"\"\n                ),\n            }\n        ],\n    )\n\n\ndef score_answer(query: str, answer: str):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=ModelScore,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are a helpful assistant that scores\n                answers based on well they are able to answer a\n                specific user query\"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                &lt;user query&gt;\n                {query}\n                &lt;/user query&gt;\n\n                &lt;response&gt;\n                {answer}\n                &lt;/response&gt;\n                \"\"\",\n            },\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    query = \"\"\"Who's the original singer of Help Me Make It\n    Through The Night?\"\"\"\n    evidences = [\n        \"\"\"Help Me Make It Through The Night is a country\n        music ballad written and composed by Kris Kristofferson\n        and released on his 1970 album 'Kristofferson'\"\"\"\n    ]\n\n    threshold = 0.8\n\n    factual_expert_output = query_factual_expert(query, evidences)\n    print(factual_expert_output.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"answer\": \"The original singer of 'Help Me Make It Through the\n      Night' is Kris Kristofferson, who released it on his 1970 album\n      'Kristofferson'.\"\n    }\n    \"\"\"\n\n    multihop_expert_output = query_multihop_expert(query)\n    print(multihop_expert_output.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"chain_of_thought\": \"To identify the original singer of 'Help Me\n      Make It Through The Night,' I need to look for the person who\n      first recorded and released the song.\",\n      \"answer\": \"The original singer of 'Help Me Make It Through\n      The Night' is Kris Kristofferson.\"\n    }\n    \"\"\"\n\n    factual_expert_score = score_answer(query, factual_expert_output.answer)\n    multihop_expert_score = score_answer(query, multihop_expert_output.answer)\n\n    if max(factual_expert_score.score, multihop_expert_score.score) &lt; threshold:\n        answer = \"Abstaining from responding\"\n    elif factual_expert_score.score &gt; multihop_expert_score.score:\n        answer = factual_expert_output.answer\n    else:\n        answer = multihop_expert_output.answer\n\n    print(answer)\n    \"\"\"\n    The original singer of 'Help Me Make It Through the Night' is Kris\n    Kristofferson, who released it on his 1970 album 'Kristofferson'.\n    \"\"\"\n</code></pre>"},{"location":"prompting/ensembling/prompt_paraphrasing/","title":"Use Translation for Paraphrasing","text":"<p>Large Language Models are sensitive to the way that they are prompted. When prompted incorrectly, they might perform much worse despite having the information or capability to respond to the prompt. We can help find semantically similar prompts by performing back translation - where we translate our prompts to another language and back to encourage more diversity in the rephrased prompts.</p> <p>Prompt paraphrasing <sup>1</sup>. provides some ways for us to improve on the phrasing of our prompts to do so.</p> <p>We can implement this using <code>instructor</code> as seen below.</p> <pre><code>import instructor\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel\nimport random\n\nclient = instructor.from_openai(AsyncOpenAI())\n\n\nclass TranslatedPrompt(BaseModel):\n    translation: str\n\n\nasync def translate_prompt(prompt: str, from_language: str, to_language: str):\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"\"\"\n                You are an expert translation assistant.\n                You are going to be given a prompt and\n                asked to translate it from {from_language}\n                to {to_language}. Paraphrase and use\n                synonyms where possible, especially for\n                the examples.\n                \"\"\",\n            },\n            {\"role\": \"user\", \"content\": f\"Prompt: {prompt}\"},\n        ],\n        response_model=TranslatedPrompt,\n    )\n\n\nasync def generate_permutation(prompt: str, language: str) -&gt; str:\n    tranlated_prompt = await translate_prompt(prompt, \"english\", language)\n    backtranslated_prompt = await translate_prompt(\n        tranlated_prompt.translation, language, \"english\"\n    )\n    return backtranslated_prompt.translation\n\n\nasync def generate_prompts(\n    prompt: str, languages: list[str], permutations: int\n) -&gt; list[str]:\n    coros = [\n        generate_permutation(prompt, random.choice(languages))\n        for _ in range(permutations)\n    ]\n    return await asyncio.gather(*coros)\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    prompt = \"\"\"\n    You are an expert system that excels at Sentiment\n    Analysis of User Reviews.\n\n    Here are a few examples to refer to:\n\n    1. That was a fantastic experience I had! I'm\n    definitely recommending this to all my friends\n    // Positive\n    2. I think it was a passable evening. I don't think\n    there was anything remarkable or off-putting for me.\n    // Negative\n    3. I'm horrified at the state of affairs in this new\n    restaurant // Negative\n\n    Sentence: This was a fantastic experience!\n    \"\"\"\n    languages = [\"french\", \"spanish\", \"chinese\"]\n    permutations = 2\n\n    generated_prompts = asyncio.run(generate_prompts(prompt, languages, permutations))\n    for prompt in generated_prompts:\n        print(prompt)\n        \"\"\"\n        You are an expert system specializing in user review sentiment analysis. Here are a few examples to guide you: 1. It was an exceptional experience! I will definitely recommend it to all my friends // Positive 2. I think it was a mediocre evening. There wasn't anything outstanding or particularly bad for me // Negative 3. I am horrified by the condition of things in this new restaurant // Negative Sentence: It was an amazing experience!\n        \"\"\"\n        \"\"\"\n        You are an expert system that excels in User Review Sentiment Analysis.\n\n        Here are some reference examples:\n\n        1. I had an amazing experience! I will definitely recommend it to all my friends.\n        // Positive\n        2. I think it was an average evening. I don\u2019t believe there was anything remarkable or unpleasant about it for me.\n        // Negative\n        3. I am horrified by the situation at this new restaurant.\n        // Negative\n\n        Sentence: This was a fantastic experience!\n        \"\"\"\n        \"\"\"\n        You are an expert system skilled in conducting user\n        review sentiment analysis.\n\n        Here are some examples for reference:\n\n        1. That was an awesome experience! I'll definitely\n        recommend it to all my friends // Positive\n        2. I think it was an okay evening. I don't find\n        anything particularly outstanding or unpleasant.\n        // Neutral\n        3. I am very shocked by the condition of this new\n        restaurant // Negative\n\n        Sentence: This was a wonderful experience!\n        \"\"\"\n</code></pre>"},{"location":"prompting/ensembling/prompt_paraphrasing/#references","title":"References","text":"<p><sup>1</sup>: How Can We Know What Language Models Know? </p>"},{"location":"prompting/ensembling/self_consistency/","title":"Generate Multiple Candidate Responses","text":"<p>By generating multiple candidate responses in parallel and choosing the most common answer among them, we can get a more accurate answer. This is known as Self-Consistency <sup>1</sup></p> <p>We can implement this using <code>instructor</code> as seen below.</p> <pre><code>import instructor\nfrom pydantic import BaseModel, Field\nfrom openai import AsyncOpenAI\nimport asyncio\nfrom collections import Counter\nfrom textwrap import dedent\n\n\nclass SelfConsistencyResponse(BaseModel):\n    chain_of_thought: str = Field(\n        description=\"reasoning behind the final correct answer\"\n    )\n    correct_answer: int\n\n\nclient = instructor.from_openai(AsyncOpenAI())\n\n\nasync def generate_self_consistent_response(prompt: str):\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are an intelligent question\n                answering AI system that excels at answering\n                user queries. Make sure to generate a\n                comprehensive explanation of your thought\n                process before providing the final answer\"\"\",\n            },\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n        response_model=SelfConsistencyResponse,\n        temperature=0.5,\n    )\n\n\nasync def generate_self_consistent_responses(prompt: str, num_responses: int):\n    coros = [\n        generate_self_consistent_response(prompt)\n        for _ in range(num_responses)\n    ]\n    responses = await asyncio.gather(*coros)\n    return responses\n\n\nif __name__ == \"__main__\":\n    prompt = dedent(\n        \"\"\"\n        Janet's ducks lay 16 eggs per day.\n        She eats three for breakfast every\n        morning and bakes muffins for her\n        friends every day with four. She sells\n        the remainder for $2 per egg. How\n        much does she make every day?\n        \"\"\"\n    )\n    responses = asyncio.run(generate_self_consistent_responses(prompt, 5))\n    answer_counts = Counter([\n        response.correct_answer for response in responses\n    ])\n    most_common_answer, _ = answer_counts.most_common(1)[0]\n\n    print(most_common_answer)\n    #&gt; 18\n</code></pre>"},{"location":"prompting/ensembling/self_consistency/#references","title":"References","text":"<p><sup>1</sup>: Self-Consistency Improves Chain Of Thought Reasoning In Language Models</p>"},{"location":"prompting/ensembling/universal_self_consistency/","title":"Use LLMs to Combine Different Responses","text":"<p>Universal Self Consistency<sup>1</sup> aims to extend self-consistency by using a second LLM model to judge the quality of individual responses. Therefore instead of choosing the final answer based on the most frequently occuring value among each reasoning chain, we instead prompt the model to choose the most consistent answer for us relative to the prompt.</p> <p></p> <p>This enables us to support a greater variety of different response formats and answer, leading to greater diversity of outputs and hence higher accuracy.</p> <p>We can implement this in <code>instructor</code> as seen below.</p> <pre><code>from openai import AsyncOpenAI\nfrom pydantic import BaseModel, Field, ValidationInfo, field_validator\nimport instructor\nfrom textwrap import dedent\nimport asyncio\n\nclient = instructor.from_openai(AsyncOpenAI())\n\n\nclass Response(BaseModel):\n    chain_of_thought: str\n    answer: str\n\n\nclass SelectedResponse(BaseModel):\n    most_consistent_response_id: int = Field(\n        description=\"\"\"The ID of the most consistent response that\n        was provided\"\"\"\n    )\n\n    @field_validator(\"most_consistent_response_id\")\n    @classmethod\n    def validate_id(cls, v: int, info: ValidationInfo):\n        context = info.context\n        number_responses = context.get(\"number_responses\", float(\"inf\"))\n\n        if v &gt; number_responses:\n            raise ValueError(\n                f\"\"\"Most consistent response ID {v} is greater than the\n                number of responses {number_responses}. Please return a\n                valid id between 0 and {number_responses-1}\"\"\"\n            )\n        return v\n\n\nasync def generate_response(query: str) -&gt; Response:\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[{\"role\": \"user\", \"content\": query}],\n    )\n\n\nasync def generate_batch_responses(query: str, no_responses: int):\n    coros = [generate_response(query) for _ in range(no_responses)]\n    return await asyncio.gather(*coros)\n\n\nasync def select_consistent_response(responses: list[Response], query: str):\n    formatted_responses = \"\\n\".join(\n        [\n        f\"Response {idx}: {response.chain_of_thought}. {response.answer}\"\n        for idx, response in enumerate(responses)\n        ]\n    )\n\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=SelectedResponse,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": dedent(\n                    f\"\"\"\n                &lt;user query&gt;\n                {query}\n                &lt;/user query&gt;\n\n                {formatted_responses}\n\n                Evaluate these responses.\n                Select the most consistent response based on majority\n                consensus\n                \"\"\"\n                ),\n            }\n        ],\n        validation_context={\"number_responses\": len(responses)},\n    )\n\n\nif __name__ == \"__main__\":\n    query = \"\"\"The three-digit number 'ab5' is divisible by 3. How many different\n     three-digit numbers can 'ab5' represent?\"\"\"\n    responses = asyncio.run(generate_batch_responses(query, 3))\n\n    for response in responses:\n        print(response.model_dump_json(indent=2))\n        \"\"\"\n        {\n          \"chain_of_thought\": \"A number is divisible by 3 if\n          the sum of its digits is divisible by 3. Given the\n          number 'ab5', we need to check how many different\n          values of 'a' and 'b', where both are digits (0-9)\n          can make the sum divisible by 3.\\n\\nThe sum of the\n          digits is a + b + 5.\\n\\nWe need to find pairs (a, b)\n          such that (a + b + 5) % 3 == 0.\",\n          \"answer\": \"30\"\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"chain_of_thought\": \"A number is divisible by 3 if\n          the sum of its digits is divisible by 3. Let's\n          denote the digits a and b. The number 'ab5' has\n          digits a, b, and 5. Therefore, the sum of the\n          digits is a + b + 5. Since the number is divisible\n          by 3, a + b + 5 must be divisible by 3.\\n\\nNow,\n          since a and b are single digits (0-9), we need to\n          find pairs (a, b) such that a + b + 5 is divisible\n          by 3. We will evaluate all possible combinations of\n          values for a and b to count how many valid pairs\n          (a, b) exist.\\n\\nLet's start by considering b's\n          values:\\n1. If b = 0, then a + 5 must be divisible\n          by 3.\\n2. If b = 1, then a + 6 must be divisible by\n          3.\\n3. If b = 2, then a + 7 must be divisible by\n          3.\\n4. If b = 3, then a + 8 must be divisible by\n          3.\\n5. If b = 4, then a + 9 must be divisible by\n          3.\\n6. If b = 5, then a + 10 must be divisible by\n          3.\\n7. If b = 6, then a + 11 must be divisible by\n          3.\\n8. If b = 7, then a + 12 must be divisible by\n          3.\\n9. If b = 8, then a + 13 must be divisible by\n          3.\\n10. If b = 9, then a + 14 must be divisible by\n          3.\\n\\nWe will find all corresponding a values for\n          each b and count the valid combinations.\\n\",\n          \"answer\": \"There are 30 different three-digit\n          numbers that 'ab5' can represent.\"\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"chain_of_thought\": \"A number is divisible by 3 if\n          the sum of its digits is divisible by 3. The given\n          number is in the form 'ab5', where 'a' and 'b' are\n          digits from 0 to 9. To find the total number of\n          different three-digit numbers that 'ab5' can\n          represent, we need to determine all possible digit\n          combinations for 'a' and 'b' such that 'a + b + 5'\n          is divisible by 3.\",\n          \"answer\": \"30\"\n        }\n        \"\"\"\n\n    selected_response = asyncio.run(select_consistent_response(responses, query))\n    print(selected_response.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"most_consistent_response_id\": 0\n    }\n    \"\"\"\n\n    print(\n        responses[selected_response.most_consistent_response_id].model_dump_json(\n            indent=2\n        )\n    )\n    \"\"\"\n    {\n      \"chain_of_thought\": \"A number is divisible by 3 if the sum of its digits is divisible by 3. Given the number 'ab5', we need to\n      check how many different values of 'a' and 'b', where both are digits (0-9) can make the sum divisible by 3.\\n\\nThe sum of the\n      digits is a + b + 5.\\n\\nWe need to find pairs (a, b) such that (a + b + 5) % 3 == 0.\",\n      \"answer\": \"30\"\n    }\n    \"\"\"\n</code></pre>"},{"location":"prompting/ensembling/universal_self_consistency/#references","title":"References","text":"<p><sup>1</sup>: Universal Self-Consistency For Large Language Model Generation</p>"},{"location":"prompting/ensembling/usp/","title":"Use Task Specific Evaluation Metrics","text":"<p>Universal Self Prompting is a two stage process similar to Consistency Based Self Adaptive Prompting (COSP). Here is a breakdown of the two stages.</p> <ol> <li>Generate Examples : LLMs are prompted to generate a collection of candidate responses using a test dataset</li> <li>Answer Query : We then select a few of these model-generated responses as examples to prompt the LLM to obtain a final prediction.</li> </ol> <p>Note here that the final answer is obtained using a single forward pass with greedy decoding.</p>"},{"location":"prompting/ensembling/usp/#usp-process","title":"USP Process","text":"<p>Let's see how this works in greater detail.</p>"},{"location":"prompting/ensembling/usp/#generate-few-shot-examples","title":"Generate Few Shot Examples","text":"<p>We first prompt our model to generate responses for a given set of prompts. Instead of measuring the entropy and repetitiveness as in COSP, we use one of three possible methods to measure the quality of the generated responses. These methods are decided based on the three categories supported.</p> <p>This category has to be specified by a user ahead of time.</p> <p>Note that for Short Form and Long Form generation, we generate \\(m\\) different samples. This is not the case for classification tasks.</p> <ul> <li>Classification : Classification Tasks are evaluated using the normalized probability of each label using the raw logits from the LLM.</li> </ul> \\[ F_{CLS}(p^{(j)}|d^{(j)}) := -\\sum_{c \\in C} P(c|d^{(j)}) \\log P(c|d^{(j)}) \\] <p>In short, we take the raw logit for each token corresponding to the label, use a softmax to normalize each of them and then sum across the individual probabilities and their log probs. We also try to sample enough queries such that we have a balanced number of predictions across each class ( so that our model doesn't have a bias towards specific classes )</p> <ul> <li>Short Form Generation: This is done by using a similar formula to COSP but without the normalizing term</li> </ul> \\[ \\mathcal{H}\\left(x^{(i)} \\mid \\left\\{\\hat{y}_j^{(i)}\\right\\}_{j=1}^m\\right) = \\frac{\\sum_{\\alpha=1}^u \\hat{p}\\left(\\hat{y}_{\\alpha}^{(i)}\\right) \\log \\hat{p}\\left(\\hat{y}_{\\alpha}^{(i)}\\right)}{\\log m}, \\] <ul> <li>Long Form Generation: This is done by using the average pairwise ROUGE score between all pairs of the \\(m\\) responses.</li> </ul> <p>What is key here is that depending on the task specified by the user, we have a task-specific form of evaluation. This eventually allows us to better evaluate our individual generated examples. Samples of tasks for each category include</p> <ol> <li>Classification: Natural Language Inference, Topic Classification and Sentiment Analysis</li> <li>Short Form Generation : Question Answering and Sentence Completion</li> <li>Long Form Generation : Text Summarization and Machine Translation</li> </ol> <p>This helps to ultimately improve the performance of these large language models across different types of tasks.</p>"},{"location":"prompting/ensembling/usp/#generate-single-response","title":"Generate Single Response","text":"<p>Once we've selected our examples, the second step is relatively simple. We just need to append a few of our chosen examples that score best on our chosen metric to append to our solution.</p>"},{"location":"prompting/ensembling/usp/#implementation","title":"Implementation","text":"<p>We've implemented a classification example below that tries to sample across different classes in a balanced manner before generating a response using a single inference call.</p> <p>We bias this sampling towards samples that the model is more confident towards by using a confidence label.</p> <pre><code>from pydantic import BaseModel\nfrom typing import Literal\nfrom instructor import from_openai\nfrom openai import AsyncOpenAI\nimport asyncio\nfrom collections import defaultdict\n\n\nclass Classification(BaseModel):\n    chain_of_thought: str\n    label: Literal[\"Happy\", \"Angry\", \"Sadness\"]\n    confidence: Literal[\n        \"Uncertain\", \"Somewhat Confident\", \"Confident\", \"Highly Confident\"\n    ]\n\n    def confidence_score(self) -&gt; int:\n        confidence_order = {\n            \"Highly Confident\": 4,\n            \"Confident\": 3,\n            \"Somewhat Confident\": 2,\n            \"Uncertain\": 1,\n        }\n        return confidence_order[self.confidence]\n\n\nclient = from_openai(AsyncOpenAI())\n\n\nasync def generate_prediction(query: str):\n    return (\n        await client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"Classify the following query {query} into\n                    one of the following categories: Happy, Angry, Sadness\"\"\",\n                }\n            ],\n            response_model=Classification,\n        ),\n        query,\n    )\n\n\nasync def generate_predictions(queries: list[str]) -&gt; list[tuple[Classification, str]]:\n    return await asyncio.gather(*[generate_prediction(query) for query in queries])\n\n\ndef get_balanced_sample(predictions: list[tuple[Classification, str]], k: int):\n    label_to_queries: dict[str, list[tuple[Classification, str]]] = defaultdict(list)\n\n    for prediction in predictions:\n        label_to_queries[prediction[0].label].append(prediction)\n\n    num_classes = len(label_to_queries)\n    num_samples_per_class = k // num_classes\n\n    res: list[str] = []\n    for label, label_queries in label_to_queries.items():\n        label_queries = sorted(\n            label_queries, key=lambda x: x[0].confidence_score(), reverse=True\n        )\n        label_queries = [\n            label_queries[1] for label_queries in label_queries[:num_samples_per_class]\n        ]\n        res.extend([f\"{query} ({label})\" for query in label_queries])\n\n    return res\n\n\nasync def generate_response_with_examples(query: str, examples: list[str]):\n    formatted_examples = \"\\n\".join(examples)\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Classification,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"\"\"\n                You are a helpful assistant that classifies queries into one of the following categories: Happy, Angry, Sadness.\n\n                Here are some samples of queries and their categories:\n\n                &lt;examples&gt;\n                {formatted_examples}\n                &lt;/examples&gt;\n\n                Here is a user query to classify\n\n                &lt;query&gt;\n                {query}\n                &lt;/query&gt;\n                \"\"\",\n            },\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    examples = [\n        \"\"\"\n        i do feel that running is a divine experience and\n        that i can expect to have some type of spiritual\n        encounter\n        \"\"\",\n        \"\"\"\n        i get giddy over feeling elegant in a perfectly\n        fitted pencil skirt\n        \"\"\",\n        \"\"\"\n        i plan to share my everyday life stories traveling\n        adventures inspirations and handmade creations with\n        you and hope you will also feel inspired\n        \"\"\",\n        \"\"\"\n        i need to feel the dough to make sure its just\n        perfect\n        \"\"\",\n        \"\"\"\n        i found myself feeling a little discouraged that\n        morning\n        \"\"\",\n        \"i didnt really feel that embarrassed\",\n        \"i feel like a miserable piece of garbage\",\n        \"\"\"\n        i feel like throwing away the shitty piece of shit\n        paper\n        \"\"\",\n        \"\"\"\n        i feel irritated and rejected without anyone doing\n        anything or saying anything\n        \"\"\",\n        \"i feel angered and firey\",\n        \"\"\"\n        im feeling bitter today my mood has been strange the\n        entire day so i guess its that\n        \"\"\",\n        \"i just feel really violent right now\",\n        \"i know there are days in which you feel distracted\",\n    ]\n\n    labels = asyncio.run(generate_predictions(examples))\n    balanced_sample = get_balanced_sample(labels, 3)\n    for sample in balanced_sample:\n        print(sample)\n        \"\"\"\n        i do feel that running is a divine experience and that i can\n        expect to have some type of spiritual encounter (Happy)\n        \"\"\"\n        #&gt; i feel like a miserable piece of garbage (Sadness)\n        #&gt; i feel like throwing away the shitty piece of shit paper (Angry)\n\n    response = asyncio.run(\n        generate_response_with_examples(\n            \"\"\"\n            i feel furious that right to life advocates can\n            and do tell me how to live and die through\n            lobbying and supporting those politicians\n            sympathic to their views\n            \"\"\",\n            balanced_sample,\n        )\n    )\n    print(response.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"chain_of_thought\": \"The user expresses feelings of\n      anger and frustration specifically directed at right\n      to life advocates. The language used, such as\n      'furious,' indicates a high level of emotion\n      associated with anger.\",\n      \"label\": \"Angry\",\n      \"confidence\": \"Highly Confident\"\n    }\n    \"\"\"\n</code></pre>"},{"location":"prompting/few_shot/example_ordering/","title":"Example Ordering","text":"<p>The order of few-shot examples in the prompt can affect LLM outputs <sup>1234</sup><sup>*</sup>. Consider permutating the order of these examples in your prompt to achieve better results.</p>"},{"location":"prompting/few_shot/example_ordering/#choosing-your-examples","title":"Choosing Your Examples","text":"<p>Depending on your use-case, here are a few different methods that you can consider using to improve the quality of your examples.</p>"},{"location":"prompting/few_shot/example_ordering/#combinatorics","title":"Combinatorics","text":"<p>One of the easiest methods is for us to manually iterate over each of the examples that we have and try all possible combinations we could create. This will in turn allow us to find the best combination that we can find.</p>"},{"location":"prompting/few_shot/example_ordering/#kate","title":"KATE","text":"<p>KATE (k-Nearest Example Tuning) is a method designed to enhance GPT-3's performance by selecting the most relevant in-context examples. The method involves:</p> <p>For each example in the test set, K nearest neighbors (examples) are retrieved based on semantic similarity. Among these K examples, those that appear most frequently across different queries are selected as the best in-context examples.</p>"},{"location":"prompting/few_shot/example_ordering/#using-a-unsupervised-retriever","title":"Using a Unsupervised Retriever","text":"<p>We can use a large LLM to compute a single score for each example with respect to a given prompt. This allows us to create a training set that scores an example's relevance when compared against a prompt. Using this training set, we can train a model that mimics this functionality. This allows us to determine the top <code>k</code> most relevant and most irrelevant examples when a user makes a query so that we can include this in our final prompt.</p>"},{"location":"prompting/few_shot/example_ordering/#references","title":"References","text":"<p><sup>1</sup>: Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity</p> <p><sup>2</sup>: Reordering Examples Helps during Priming-based Few-Shot Learning</p> <p><sup>3</sup>: What Makes Good In-Context Examples for GPT-3?</p> <p><sup>4</sup>: Learning To Retrieve Prompts for In-Context Learning</p> <p><sup>*</sup>: The Prompt Report: A Systematic Survey of Prompting Techniques</p>"},{"location":"prompting/few_shot/example_generation/sg_icl/","title":"Generate In-Context Examples","text":"<p>How can we generate examples for our prompt?</p> <p>Self-Generated In-Context Learning (SG-ICL) is a technique which uses an LLM to generate examples to be used during the task. This allows for in-context learning, where examples of the task are provided in the prompt.</p> <p>We can implement SG-ICL using <code>instructor</code> as seen below.</p> <pre><code>import instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\nfrom typing import Literal\n\nn = 4  # num examples to generate per class\n\n\nclass GeneratedReview(BaseModel):\n    review: str\n    sentiment: Literal[\"positive\", \"negative\"]\n\n\nclass SentimentPrediction(BaseModel):\n    sentiment: Literal[\"positive\", \"negative\"]\n\n\nclient = instructor.from_openai(OpenAI())\n\n\ndef generate_sample(input_review, sentiment):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=GeneratedReview,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                           Generate a '{sentiment}' review similar to: {input_review}\n                           Generated review:\n                           \"\"\",\n            }\n        ],\n    )\n\n\ndef predict_sentiment(input_review, in_context_samples):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=SentimentPrediction,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"\".join(\n                    [\n                        f\"Review: {sample.review}\\nSentiment: {sample.sentiment}\\n\\n\"\n                        for sample in in_context_samples\n                    ]\n                )\n                + f\"Review: {input_review}\\nSentiment:\",\n            }\n        ],\n    ).sentiment\n\n\nif __name__ == \"__main__\":\n    input_review = (\n        \"This movie was a rollercoaster of emotions, keeping me engaged throughout.\"\n    )\n\n    # Generate in-context samples\n    samples = [\n        generate_sample(input_review, sentiment)\n        for sentiment in ('positive', 'negative')\n        for _ in range(n)\n    ]\n    for sample in samples:\n        print(sample)\n        \"\"\"\n        review='This film was an emotional journey, keeping me captivated from start to finish.' sentiment='positive'\n        \"\"\"\n        \"\"\"\n        review='This film was an emotional journey, captivating me from start to finish.' sentiment='positive'\n        \"\"\"\n        \"\"\"\n        review='This film captivated me from start to finish with its thrilling plot and emotional depth.' sentiment='positive'\n        \"\"\"\n        \"\"\"\n        review='This movie was a breathtaking journey, capturing my attention from start to finish.' sentiment='positive'\n        \"\"\"\n        \"\"\"\n        review='This movie was a chaotic mess of emotions, losing me at every turn.' sentiment='negative'\n        \"\"\"\n        \"\"\"\n        review='This movie was a confusing mess, leaving me disengaged throughout.' sentiment='negative'\n        \"\"\"\n        \"\"\"\n        review='This movie was a chore to sit through, leaving me bored most of the time.' sentiment='negative'\n        \"\"\"\n        \"\"\"\n        review='This movie was a mishmash of confusing scenes, leaving me frustrated throughout.' sentiment='negative'\n        \"\"\"\n\n    # Predict sentiment\n    print(predict_sentiment(input_review, samples))\n    #&gt; positive\n</code></pre>"},{"location":"prompting/few_shot/example_generation/sg_icl/#references","title":"References","text":"<p><sup>1</sup>: Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator</p> <p><sup>*</sup>: The Prompt Report: A Systematic Survey of Prompting Techniques</p>"},{"location":"prompting/few_shot/exemplar_selection/knn/","title":"Select Effective Examples","text":"<p>We can select effective in-context examples by choosing those that are semantically closer to the query using <code>KNN</code>.</p> <p>In the below implementation using <code>instructor</code>, we follow these steps:</p> <ol> <li>Embed the query examples</li> <li>Embed the query that we want to answer</li> <li>Find the k query examples closest to the query</li> <li>Use the chosen examples and their as the context for the LLM</li> </ol> <pre><code>import instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\nimport math\nfrom textwrap import dedent\n\n\nclass Example(BaseModel):\n    question: str\n    answer: str\n\n\nclass Response(BaseModel):\n    answer: str\n\n\noai = OpenAI()\nclient = instructor.from_openai(oai)\n\n\ndef distance(a: list[float], b: list[float]):\n    return 1 - sum(ai * bi for ai, bi in zip(a, b)) / (\n        math.sqrt(sum(ai**2 for ai in a)) * math.sqrt(sum(bi**2 for bi in b))\n    )\n\n\ndef embed_queries(queries: list[str]) -&gt; list[tuple[list[float], str]]:\n    return [\n        (embedding_item.embedding, query)\n        for embedding_item, query in zip(\n            oai.embeddings.create(input=queries, model=\"text-embedding-3-large\").data,\n            queries,\n        )\n    ]\n\n\ndef knn(\n    embedded_examples: list[tuple[list[float], str]],\n    query_embedding: list[float],\n    k: int,\n):\n    distances = [\n        (distance(embedding, query_embedding), example)\n        for embedding, example in embedded_examples\n    ]\n    distances.sort(key=lambda x: x[0])\n    return distances[:k]\n\n\ndef generate_response(examples: list[str], query: str):\n    formatted_examples = \"\\n\".join(examples)\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": dedent(\n                    f\"\"\"\n                    Respond to the following query with the most accurate\n                    and concise answer possible.\n                    &lt;examples&gt;\n                    {formatted_examples}\n                    &lt;/examples&gt;\n                    &lt;query&gt;\n                    {query}\n                    &lt;/query&gt;\n                \"\"\"\n                ),\n            }\n        ],\n    )\n\n\ndef generate_question_and_answer_pair(\n    questions: list[str], question_and_answers: list[dict[str, str]]\n) -&gt; list[str]:\n    question_to_answer = {}\n\n    for question in question_and_answers:\n        question_to_answer[question[\"question\"]] = question[\"answer\"]\n\n    return [\n        dedent(\n            f\"\"\"\n        &lt;example&gt;\n        &lt;question&gt;{question}&lt;/question&gt;\n        &lt;answer&gt;{question_to_answer[question]}&lt;/answer&gt;\n        &lt;/example&gt;\n        \"\"\"\n        )\n        for question in questions\n    ]\n\n\nif __name__ == \"__main__\":\n    examples = [\n        {\"question\": \"What is the capital of France?\", \"answer\": \"Paris\"},\n        {\"question\": \"Who wrote Romeo and Juliet\", \"answer\": \"Shakespeare\"},\n        {\"question\": \"What is the capital of Germany?\", \"answer\": \"Berlin\"},\n    ]\n\n    query = \"What is the capital of Italy?\"\n\n    # Step 1 : Embed the Examples\n    embeddings = embed_queries([example[\"question\"] for example in examples] + [query])\n\n    embedded_examples = embeddings[:-1]\n    embedded_query = embeddings[-1]\n\n    # # Step 3: Find the k closest examples to the query\n    k_closest_examples = knn(embedded_examples, embedded_query[0], 2)\n\n    for example in k_closest_examples:\n        print(example)\n        #&gt; (0.4015450506411443, 'What is the capital of France?')\n        #&gt; (0.4472610680568724, 'What is the capital of Germany?')\n\n    # Step 4: Use these examples as in-context examples\n    formatted_examples = generate_question_and_answer_pair(\n        [example[1] for example in k_closest_examples], examples\n    )\n    response = generate_response(formatted_examples, query)\n    print(response.answer)\n    #&gt; Rome\n</code></pre>"},{"location":"prompting/few_shot/exemplar_selection/knn/#references","title":"References","text":"<p><sup>1</sup>: What Makes Good In-Context Examples for GPT-3?</p> <p><sup>*</sup>: The Prompt Report: A Systematic Survey of Prompting Techniques</p>"},{"location":"prompting/few_shot/exemplar_selection/vote_k/","title":"","text":"<p>[wip]</p>"},{"location":"prompting/self_criticism/chain_of_verification/","title":"Independently Verify Responses","text":"<p>Chain Of Verification ( CoVe )<sup>1</sup> is a method that allows us to be able to verify our LLM's generated responses. We can do so using the following steps</p> <ol> <li>First we get our LLM to generate a response to a query</li> <li>Then we generate a set of follow up questions that need to be answered to validate the response</li> <li>We then independently generate a set of responses to these questions</li> <li>Lastly, we use a final LLM call to verify the response in light of these new question and answer pairs that we've generated</li> </ol> <pre><code>import instructor\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel, Field\nimport asyncio\n\nclient = instructor.from_openai(AsyncOpenAI())\n\n\nclass QueryResponse(BaseModel):\n    correct_answer: str\n\n\nclass ValidationQuestions(BaseModel):\n    question: list[str] = Field(\n        description=\"\"\"A list of questions that need to be\n        answered to validate the response\"\"\"\n    )\n\n\nclass ValidationAnswer(BaseModel):\n    answer: str\n\n\nclass FinalResponse(BaseModel):\n    correct_answer: str\n\n\nasync def generate_initial_response(query: str):\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=QueryResponse,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are an expert question answering system\",\n            },\n            {\"role\": \"user\", \"content\": query},\n        ],\n    )\n\n\nasync def generate_verification_questions(llm_response: str):\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=ValidationQuestions,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are an expert AI system that excels at\n                generating follow up questions to validate a response.\n                These questions should validate key assumptions, facts\n                and other important portions of the generated response\"\"\",\n            },\n            {\"role\": \"user\", \"content\": llm_response},\n        ],\n    )\n\n\nasync def generate_verification_response(questions: list[str]):\n    async def verify_question(question: str) -&gt; tuple[ValidationAnswer, str]:\n        return (\n            await client.chat.completions.create(\n                model=\"gpt-4o\",\n                response_model=ValidationAnswer,\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"\"\"You are an expert AI system that\n                        excels at answering validation questions.\"\"\",\n                    },\n                    {\"role\": \"user\", \"content\": question},\n                ],\n            ),\n            question,\n        )\n\n    coros = [verify_question(question) for question in questions]\n    return await asyncio.gather(*coros)\n\n\nasync def generate_final_response(\n    answers: list[tuple[ValidationAnswer, str]],\n    initial_response: QueryResponse,\n    original_query: str,\n):\n    formatted_answers = \"\\n\".join(\n        [f\"Q: {question}\\nA: {answer.answer}\" for answer, question in answers]\n    )\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=FinalResponse,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are an expert AI system that excels at\n                validating and verifying if an initial answer answers an\n                initial query based off some Verification Questions and\n                Answers provided. Return the original answer if it is\n                valid else generate a new response off the verification\n                questions and answers provided.\"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                Initial query: {original_query}\n                Initial Answer : {initial_response.correct_answer}\n                Verification Questions and Answers:\n                {formatted_answers}\n            \"\"\",\n            },\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    query = \"What was the primary cause of the Mexican-American war and how long did it last?\"\n    initial_response = asyncio.run(generate_initial_response(query))\n    print(initial_response.model_dump_json())\n    \"\"\"\n    {\"correct_answer\":\"The primary cause of the Mexican-American War was\n    the annexation of Texas by the United States and the dispute over\n    whether Texas ended at the Nueces River (as the Mexicans claimed) or\n    the Rio Grande (as the U.S. claimed). The war lasted from April 25,\n    1846, to February 2, 1848, totaling nearly two years.\"}\n    \"\"\"\n\n    verification_questions = asyncio.run(\n        generate_verification_questions(initial_response.correct_answer)\n    )\n    print(verification_questions.model_dump_json())\n    \"\"\"\n    {\"question\":[\"Is it accurate that the primary cause of the\n    Mexican-American War was the annexation of Texas by the United\n    States?\",\"Was there a dispute over whether Texas ended at the Nueces\n    River or the Rio Grande?\",\"Did the Mexican-American War last from\n    April 25, 1846, to February 2, 1848?\",\"Is it correct to state that\n    the disagreement over the Texas border was between the Nueces River\n    and the Rio Grande?\",\"Was the Mexican claim that Texas ended at the\n    Nueces River while the U.S. claimed it was at the Rio Grande?\"]}\n    \"\"\"\n\n    responses = asyncio.run(\n        generate_verification_response(verification_questions.question)\n    )\n\n    final_answer = asyncio.run(\n        generate_final_response(responses, initial_response, query)\n    )\n    print(final_answer.model_dump_json())\n    \"\"\"\n    {\"correct_answer\":\"The primary cause of the Mexican-American War was\n    the annexation of Texas by the United States and the dispute over\n    whether Texas ended at the Nueces River (as the Mexicans claimed) or\n    the Rio Grande (as the U.S. claimed). The war lasted from April 25,\n    1846, to February 2, 1848, totaling nearly two years.\"}\n    \"\"\"\n</code></pre>"},{"location":"prompting/self_criticism/chain_of_verification/#references","title":"References","text":"<p><sup>1</sup>: Chain-Of-Verification Reduces Hallucination In Large Language Models</p>"},{"location":"prompting/self_criticism/cumulative_reason/","title":"Break Down Reasoning Into Multiple Steps","text":"<p>Cumulative Reasoning<sup>1</sup> aims to generate better outputs by dividing the reasoning process into three separate steps</p> <ol> <li>Propose : A LLM first suggests potential steps based o the current context, initiating the reasoning cycle</li> <li>Verify : We then assess the proposer's suggestions for accuracy, incorporating valid steps into the ongoing context</li> <li>Report : We then determine the appropriate moment to conclude the reasoning process</li> </ol> <p>By first generating potential steps and separating out each portions of the reasoning process, we are able to obtain significant improvements in logical inference tasks and mathematical problems.</p> <p>We can implement this using <code>instructor</code> as seen below</p> <pre><code>import instructor\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel, Field\nfrom textwrap import dedent\nfrom typing import Literal\nimport asyncio\n\nclient = instructor.from_openai(AsyncOpenAI())\n\n\nclass Proposition(BaseModel):\n    premise1: str\n    premise2: str\n    reasoning: str\n    proposition: str\n\n\nclass ProposerOutput(BaseModel):\n    reasoning: str\n    valid_propositions: list[Proposition] = Field(\n        description=\"Concise list of Propositions that are derived from the premises that are relevant to the hypothesis. Note that each Proposition is derived from two given premises at most\",\n        min_length=4,\n    )\n    prediction: Literal[\"False\", \"True\", \"Unknown\"]\n\n\nclass VerifiedProposition(BaseModel):\n    proposition: str\n    reasoning: str\n    is_valid: bool\n\n\nclass ReporterOutput(BaseModel):\n    reasoning: str\n    is_valid_hypothesis: bool\n\n\nasync def generate_propositions(premises: list[str], hypothesis: str) -&gt; ProposerOutput:\n    formatted_premises = \"\\n- \".join(premises)\n    return await client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": dedent(\n                    \"\"\"\n                Suppose you are one of the greatest AI\n                scientists, logicians, and mathematicians.\n\n                Let us think step by step. Please use\n                First-Order Logic (FOL) to deduce a list\n                of Propositions. Each Proposition is\n                derived from two given Premises and\n                should be logically correct. Most\n                importantly, each Proposition should\n                not duplicate the two premises that it\n                is derived from. Please make sure your\n                reasoning is directly deduced from the\n                Premises and Propositions rather than\n                introducing unsourced common knowledge\n                and unsourced information by common\n                sense reasoning.\n                \"\"\"\n                ),\n            },\n            {\n                \"role\": \"user\",\n                \"content\": dedent(\n                    f\"\"\"\n                Premises:\n                {formatted_premises}\n\n                We want to deduce more Propositions to\n                determine the correctness of the following\n                Hypothesis:\n                Hypothesis: {hypothesis}\n                \"\"\"\n                ),\n            },\n        ],\n        response_model=ProposerOutput,\n        model=\"gpt-4o\",\n    )\n\n\nasync def verify_propositions(\n    premise_evaluation: ProposerOutput,\n) -&gt; list[VerifiedProposition]:\n    async def create_verification_task(proposition: Proposition) -&gt; VerifiedProposition:\n        return await client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"\n                    Suppose you are one of the greatest AI\n                    scientists, logicians, and mathematicians.\n                    Let us think step by step. Please use\n                    First-Order Logic (FOL) to determine\n                    whether the deduction of two given\n                    Premises to a Proposition is valid or not,\n                    and reply with True or False.\n                    \"\"\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"\n                    Premises:\n                    {proposition.premise1}\n                    {proposition.premise2}\n\n                    Proposition:\n                    {proposition.proposition}\n                    \"\"\",\n                },\n            ],\n            response_model=VerifiedProposition,\n            model=\"gpt-4o\",\n        )\n\n    tasks = [\n        create_verification_task(proposition)\n        for proposition in premise_evaluation.valid_propositions\n    ]\n\n    return await asyncio.gather(*tasks)\n\n\nasync def final_evaluation(\n    verification_result: list[str], hypothesis: str, premises: list[str]\n) -&gt; ReporterOutput:\n    formatted_premises = \"\\n- \".join(premises)\n    formatted_propositions = \"\\n- \".join(verification_result)\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"\n                Suppose you are one of the greatest AI\n                scientists, logicians, and mathematicians.\n                Let us think step by step. Read and analyze\n                the \u201cPremises\u201d first, then use First-Order\n                Logic (FOL) to judge whether the \u201cHypothesis\u201d\n                is True, False, or Unknown. Please make sure\n                your reasoning is directly deduced from the\n                \"Premises\" and \"Propositions\" rather than\n                introducing unsourced common knowledge and\n                unsourced information by common sense\n                reasoning.\n                \"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                Premises:\n                {formatted_premises}\n\n                Hypothesis: {hypothesis}\n                \"\"\",\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": f\"\"\"\n                Let's think step by step. From the premises,\n                we can deduce the following propositions:\n                {formatted_propositions}\n\n                Recall the Hypothesis: {hypothesis}\n                \"\"\",\n            },\n        ],\n        response_model=ReporterOutput,\n    )\n\n\nif __name__ == \"__main__\":\n    hypothesis = \"Hyraxes lay eggs\"\n    premises = [\n        \"The only types of mammals that lay eggs are platypuses and echidnas\",\n        \"Platypuses are not hyrax\",\n        \"Echidnas are not hyrax\",\n        \"No mammals are invertebrates\",\n        \"All animals are either vertebrates or invertebrates\",\n        \"Mammals are animals\",\n        \"Hyraxes are mammals\",\n        \"Grebes lay eggs\",\n        \"Grebes are not platypuses and also not echidnas\",\n    ]\n    premise_evaluation = asyncio.run(generate_propositions(premises, hypothesis))\n\n    verification_result = asyncio.run(verify_propositions(premise_evaluation))\n\n    filtered_propositions = [\n        proposition.proposition\n        for proposition in verification_result\n        if proposition.is_valid\n    ]\n\n    reporter_output = asyncio.run(\n        final_evaluation(filtered_propositions, hypothesis, premises)\n    )\n    print(reporter_output.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"reasoning\": \"Based on the premises provided, the\n      only mammals that lay eggs are platypuses and\n      echidnas. Hyraxes are mammals but are explicitly\n      stated as not being platypuses or echidnas. Hence,\n      there is no basis in the premises to conclude that\n      hyraxes lay eggs. \\n\\nTherefore, the hypothesis that\n      hyraxes lay eggs is False.\",\n      \"is_valid_hypothesis\": false\n    }\n    \"\"\"\n</code></pre>"},{"location":"prompting/self_criticism/cumulative_reason/#references","title":"References","text":"<p><sup>1</sup>: Cumulative Reasoning with Large Language Models</p>"},{"location":"prompting/self_criticism/reversecot/","title":"Reconstruct Prompt from Reasoning Steps","text":"<p>We can use a method called Reverse Chain Of Thought<sup>1</sup> to reverse engineer a problem given a solution. This helps us to find specific inconsistencies in the reasoning steps taken by our model and to give targetted feedback which can improve the quality of the solution.</p> <p>This is done through a 3 step process</p> <ol> <li>Reconstruct The Question : We first attempt to reconstruct the original problem given the solution and reasoning steps generated</li> <li>Identify Inconsistencies : Identify the inconsistencies between the original problem and the reconstructed problem</li> <li>Generate Feedback : Give fine-grained fedback to guide the LLM in revising its solution</li> </ol> <p>We can implement this using <code>instructor</code> as seen below.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass ReconstructedPrompt(BaseModel):\n    chain_of_thought: str\n    reconstructed_prompt: str = Field(\n        description=\"\"\"Reconstruction of a potential prompt\n        that could have been used to generate the reasoning\n        and final solution provided by the user\"\"\"\n    )\n\n\nclass ConditionList(BaseModel):\n    conditions: list[str] = Field(\n        description=\"\"\"Key information and conditions present\n        in the reasoning steps which are relevant to answering\n        the question\"\"\"\n    )\n\n\nclass ModelFeedback(BaseModel):\n    detected_inconsistencies: list[str] = Field(\n        description=\"\"\"Inconsistencies that were detected between\n        the original condition list and the reconstructed condition\n        list\"\"\"\n    )\n    feedback: str = Field(\n        description=\"\"\"Feedback on how to fix the inconsistencies\n        detected in the original condition list and the reconstructed\n        condition list\"\"\"\n    )\n    is_equal: bool\n\n\nclass ModelResponse(BaseModel):\n    chain_of_thought: str = Field(\n        description=\"\"\"Logical Steps that were taken to derive\n        the final concluding statement\"\"\"\n    )\n    correct_answer: str\n\n\ndef generate_response(query: str):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"\n                You are a helpful AI Question Answerer. You are\n                about to be passed a query by a User.\n\n                Make sure to generate a series of logical steps\n                and reason about the problem before generating\n                a solution.\n                \"\"\",\n            },\n            {\"role\": \"user\", \"content\": query},\n        ],\n        response_model=ModelResponse,\n    )\n\n\ndef reconstruct_prompt(model_response: ModelResponse):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=ReconstructedPrompt,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"\"\"\n                    Give the concrete prompt (problem) that can\n                    generate this answer. The problem should\n                    contain all basic and necessary information\n                    and correspond to the answer. The problem\n                    can only ask for one result\n\n                    Reasoning: {model_response.chain_of_thought}\n                    Response: {model_response.correct_answer}\n                    \"\"\",\n            }\n        ],\n    )\n\n\ndef deconstruct_prompt_into_condition_list(prompt: str):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=ConditionList,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"\n                You are an expert AI system that excels at\n                analyzing and decomposing questions into their\n                constituent parts.\n\n                Please list the conditions of the problem given\n                below. There might be multiple conditions in the\n                problem so make sure to navigate through the\n                prompt incrementally, indentifying and extracting\n                the conditions necessary to answer the question\n                in your final response.\n                \"\"\",\n            },\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n    )\n\n\ndef generate_feedback(\n    original_condition_list: list[str], final_condition_list: list[str]\n):\n    formatted_original_conditions = \"\\n- \".join(original_condition_list)\n    formatted_final_conditions = \"\\n- \".join(final_condition_list)\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=ModelFeedback,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"\"\"\n                You are an expert AI system that excels at\n                analyzing and comparing two lists of conditions.\n\n                Original Condition List:\n                {formatted_original_conditions}\n\n                Reconstructed Condition List:\n                {formatted_final_conditions}\n\n                Determine if the two condition lists are roughly\n                equivalent. If they are not, give targetted\n                feedback on what is missing from the reconstructed\n                condition list as compared to the original condition\n                list and how it can be fixed.\n                \"\"\",\n            }\n        ],\n    )\n\n\ndef revise_response(response: ModelResponse, feedback: ModelFeedback):\n    formatted_inconsistencies = \"\\n- \".join(feedback.detected_inconsistencies)\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"\"\"\n                Here are the mistakes and reasons in your answer\n                to the prompt\n\n                Original Response: {response.correct_answer}\n                You have overlooked some real conditions:\n                {formatted_inconsistencies}\n\n                Here are detailed reasons:\n                {feedback.feedback}\n\n                Generate a revised response that takes into account\n                the detailed feedback and includes the ignored\n                conditions\n                \"\"\",\n            }\n        ],\n        response_model=ModelResponse,\n    )\n\n\nif __name__ == \"__main__\":\n    query = \"\"\"\n    Mary is an avid gardener. Yesterday, she received 18 new\n    potted plants from her favorite plant nursery. She already\n    has 2 potted plants on each of the 40 window ledges of her\n    large backyard. How many potted plants will Mary remain\n    with?\n    \"\"\"\n    response = generate_response(query)\n    reconstructed_prompt = reconstruct_prompt(response)\n    print(reconstructed_prompt.reconstructed_prompt)\n    \"\"\"\n    Mary received 18 new potted plants. She already has 2 potted plants on each\n    of the 40 window ledges in her backyard. How many potted plants does she have now?\n    \"\"\"\n\n    original_condition_list = deconstruct_prompt_into_condition_list(query)\n    new_condition_list = deconstruct_prompt_into_condition_list(\n        reconstructed_prompt.reconstructed_prompt\n    )\n    print(original_condition_list.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"conditions\": [\n        \"Mary received 18 new potted plants.\",\n        \"Mary has 2 potted plants on each of the 40 window ledges in her backyard.\",\n        \"We are required to find the total number of potted plants Mary will have.\"\n      ]\n    }\n    \"\"\"\n    print(new_condition_list.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"conditions\": [\n        \"Mary received 18 new potted plants.\",\n        \"She already has 2 potted plants on each of the 40 window ledges in her backyard.\"\n      ]\n    }\n    \"\"\"\n\n    feedback = generate_feedback(\n        original_condition_list.conditions,\n        new_condition_list.conditions\n    )\n    print(feedback.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"detected_inconsistencies\": [\n        \"The reconstructed list is missing the requirement\n        to find the total number of potted plants Mary will\n        have.\"\n      ],\n      \"feedback\": \"Add the requirement of finding the total\n      number of potted plants Mary will have to the\n      reconstructed condition list to match the original\n      condition list.\",\n      \"is_equal\": false\n    }\n    \"\"\"\n\n    if not feedback.is_equal:\n        response = revise_response(response, feedback)\n\n    print(response.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"chain_of_thought\": \"First, we note that Mary starts\n      with 18 potted plants. According to the problem, she\n      bought 2 packs of 40 new potted plants. So, to find\n      the total number of plants she will have, we add the\n      number of plants she initially has to the number she\n      bought. This gives us 18 (initial) + 2 * 40 (new) =\n      18 + 80 = 98 potted plants.\",\n      \"correct_answer\": \"98 potted plants\"\n    }\n    \"\"\"\n</code></pre>"},{"location":"prompting/self_criticism/reversecot/#references","title":"References","text":"<p><sup>1</sup>: RCoT: Detecting And Rectifying Factual Inconsistency In Reasoning By Reversing Chain-Ofthought</p>"},{"location":"prompting/self_criticism/self_calibration/","title":"Determine Uncertainty of Reasoning Chain","text":"<p>We want our language models to be able to output the extent of their confidence in predictions. To do so, we can get language models to evaluate their responses to a given prompt using a technique called Self Calibration <sup>1</sup></p> <p>The original paper used a fine-tuned regression head over the language model's final output. However, since we don't have access to the model's final hidden states, we can substitute it for a function call instead to achieve a similar result.</p> <p>We can ask language models to evaluate their outputs by using the following template</p> <p>We can implement this using <code>instructor</code> as seen below</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass SelfCalibration(BaseModel):\n    chain_of_thought: str\n    is_valid_answer: bool = Field(description=\"Whether the answer is correct or not\")\n\n\ndef evaluate_model_output(original_prompt: str, model_response: str):\n    return client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                Question: {original_prompt}\n\n                {model_response}\n\n                Is this a valid answer to the question?\n                Make sure to examine the question\n                thoroughly and generate a complete\n                reasoning for why the answer is correct\n                or not before responding.\n                \"\"\",\n            }\n        ],\n        response_model=SelfCalibration,\n        model=\"gpt-4o\",\n    )\n\n\nif __name__ == \"__main__\":\n    original_prompt = \"\"\"\n    Question: Who was the third president of the\n    United States?\n    \"\"\"\n    model_response = \"\"\"\n    Here are some brainstormed ideas: James Monroe\n    Thomas Jefferson\n    Jefferson\n    Thomas Jefferson\n    George Washington\n    \"\"\"\n    response = evaluate_model_output(original_prompt, model_response)\n    print(response.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"chain_of_thought\": \"Let's examine the question\n      carefully: 'Who was the third president of the\n      United States?'\\n\\nThe brainstormed ideas are:\n      \\n1. James Monroe\\n2. Thomas Jefferson\\n3.\n      Jefferson\\n4. Thomas Jefferson\\n5. George\n      Washington.\\n\\nTo determine the validity of these\n      answers, I'll cross-check with historical\n      records.\\n\\n1. James Monroe was not the third\n      president; he was the fifth president.\\n2. Thomas\n      Jefferson was indeed the third president of the\n      United States.\\n3. 'Jefferson' is a correct but\n      incomplete answer; it lacks the first name, though\n      it is commonly understood.\\n4. 'Thomas Jefferson'\n      is the full name and correct answer.\\n5. George\n      Washington was the first president, not the\n      third.\\n\\nTherefore, the correct, valid answer to\n      the question 'Who was the third president of the\n      United States?' is 'Thomas Jefferson,' and this\n      answer is correct.\",\n      \"is_valid_answer\": true\n    }\n    \"\"\"\n</code></pre>"},{"location":"prompting/self_criticism/self_calibration/#references","title":"References","text":"<p><sup>1</sup>: Language Models (Mostly) Know What They Know</p>"},{"location":"prompting/self_criticism/self_refine/","title":"Improve With Feedback","text":"<p>How can we provide feedback for an LLM to improve its responses?</p> <p>Self-refine is an approach that uses an LLM to generate an output, provide feedback on the output, and improve the output based on the provided feedback. This processes repeats until a stopping condition is achieved. The same LLM is used for all three steps.</p> <pre><code>graph TD\n    A[Generate initial response]:::blue --&gt; B[Generate feedback]:::orange\n    B --&gt; C{Stopping&lt;br&gt;condition&lt;br&gt;met?}:::orange\n    C --&gt;|No| D[Refine response]:::orange\n    C --&gt;|Yes| E[Final output]:::green\n    D --&gt; B\n\n    classDef blue fill:#E3F2FD,stroke:#90CAF9,color:#1565C0\n    classDef orange fill:#FFF3E0,stroke:#FFE0B2,color:#E65100\n    classDef green fill:#E8F5E9,stroke:#A5D6A7,color:#2E7D32\n    linkStyle default stroke:#90A4AE,stroke-width:2px;\n    linkStyle 1,2,4 stroke:#FFB74D,stroke-width:2px;</code></pre> <pre><code>import instructor\nfrom pydantic import BaseModel, Field\nfrom openai import OpenAI\nfrom typing import Optional\n\n\nclass Response(BaseModel):\n    code: str\n\n\nclass Feedback(BaseModel):\n    feedback: list[str] = Field(\n        description=\"A list of actions to take to improve the code.\"\n    )\n    done: bool\n\n\nclass Timestep(BaseModel):\n    response: str\n    feedback: Optional[list[str]] = Field(default_factory=list)\n    refined_response: Optional[str] = Field(default=\"\")\n\n\nclass History(BaseModel):\n    history: list[Timestep] = Field(default_factory=list)\n\n    def add(self, code, feedback, refined_code):\n        self.history.append(\n            Timestep(response=code, feedback=feedback, refined_response=refined_code)\n        )\n\n\nclient = instructor.from_openai(OpenAI())\n\n\ndef generate_feedback(response):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Feedback,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                        You are an expert Python coder.\n                        Provide feedback on this code.\n                        How can we make it (1) faster and (2) more readable?\n\n                        &lt;code&gt;\n                        {response.code}\n                        &lt;/code&gt;\n\n                        If the code does not need to be improved, then indicate by setting \"done\" to True.\n                        \"\"\",\n            }\n        ],\n    )\n\n\ndef refine(response, feedback):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                        You are an expert Python coder.\n\n                        &lt;response&gt;\n                        {response.code}\n                        &lt;/response&gt;\n\n                        &lt;feedback&gt;\n                        {feedback.feedback}\n                        &lt;/feedback&gt;\n\n                        Refine your response.\n                        \"\"\",\n            }\n        ],\n    )\n\n\ndef stop_condition(feedback, history):\n    return feedback.done or len(history.history) &gt;= 3\n\n\nif __name__ == \"__main__\":\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"Write Python code to calculate the fibonacci sequence.\",\n            }\n        ],\n    )\n\n    history = History()\n\n    while True:\n        feedback = generate_feedback(response)\n        if stop_condition(feedback, history):\n            break\n        refined_response = refine(response, feedback)\n\n        # Save to history\n        history.add(response.code, feedback.feedback, refined_response.code)\n        response = refined_response\n\n    print(history.history[0].response)\n    \"\"\"\n    def fibonacci(n):\n        sequence = [0, 1]\n        while len(sequence) &lt; n:\n            sequence.append(sequence[-1] + sequence[-2])\n        return sequence[:n]\n\n    # Example usage:\n    n = 10\n    print(fibonacci(n))\n    \"\"\"\n    print(history.history[0].feedback)\n    \"\"\"\n    [\n        'Use a generator to reduce memory consumption for large `n` values and improve speed.',\n        'Enhance readability by adding type hints for input and output.',\n        \"Add docstrings to explain the function's purpose and parameters.\",\n        \"Avoid slicing the list at the end if it's not necessary; instead, ensure the loop condition is precise.\",\n    ]\n    \"\"\"\n    print(history.history[0].refined_response)\n    \"\"\"\n    def fibonacci(n: int) -&gt; list[int]:\n        \"\"\"Generate a Fibonacci sequence of length n.\n\n        Args:\n            n (int): The length of the Fibonacci sequence to generate.\n\n        Returns:\n            list[int]: A list containing the Fibonacci sequence of length n.\n        \"\"\"\n        def fibonacci_generator():\n            a, b = 0, 1\n            for _ in range(n):\n                yield a\n                a, b = b, a + b\n        return list(fibonacci_generator())\n\n    # Example usage:\n    n = 10\n    print(fibonacci(n))\n    \"\"\"\n    print(f\"...process repeated {len(history.history)} times...\")\n    #&gt; ...process repeated 3 times...\n    print(response.code)\n    \"\"\"\n    def fibonacci(n: int) -&gt; list[int]:\n        \"\"\"Generate a Fibonacci sequence of length n.\n\n        Args:\n            n (int): The length of the Fibonacci sequence to generate.\n\n        Returns:\n            list[int]: A list containing the Fibonacci sequence of length n.\n        \"\"\"\n        if n &lt;= 0:\n            return []\n        sequence = [0] * n\n        if n &gt; 1:\n            sequence[1] = 1\n        for i in range(2, n):\n            sequence[i] = sequence[i-1] + sequence[i-2]\n        return sequence\n\n    # Example usage:\n    n = 10\n    print(fibonacci(n))\n    \"\"\"\n</code></pre>"},{"location":"prompting/self_criticism/self_refine/#references","title":"References","text":"<p><sup>1</sup>: Self-Refine: Iterative Refinement with Self-Feedback</p> <p><sup>*</sup>: The Prompt Report: A Systematic Survey of Prompting Techniques</p>"},{"location":"prompting/self_criticism/self_verification/","title":"Self-Verify LLM Responses","text":"<p>We want to verify that an LLM response is correct. How can we automate this?</p> <p>The self-verification framework generates multiple response candidates, then uses an LLM to verify these candidates. The process follows two stages:</p> <ol> <li>Forward Reasoning</li> <li>Backward Verification</li> </ol>"},{"location":"prompting/self_criticism/self_verification/#forward-reasoning","title":"Forward Reasoning","text":"<p>In forward reasoning, we leaverage CoT to generate multiple candidate solutions.</p>"},{"location":"prompting/self_criticism/self_verification/#backward-verification","title":"Backward Verification","text":"<p>Backward verification involves three steps.</p>"},{"location":"prompting/self_criticism/self_verification/#rewrite-as-declarative","title":"Rewrite As Declarative","text":"<p>Rewrite the original question and its solution as a declarative.</p> <p>Rewritten Declaritive Example</p> <p>original question: Jackie has 10 apples. Adam has 8 apples. How many more apples does Jackie have than Adam? response candidate: Jackie has 10 apples. so Jackie has 10-8=2 more apples than Adam, and the answer is 2. rewritten declarative: Jackie has 10 apples. Adam has 8 apples. Jackie has 2 more apples than Adam.</p>"},{"location":"prompting/self_criticism/self_verification/#construct-new-question","title":"Construct New Question","text":"<p>Construct a new question and prompt the LLM to verify it. Two possible methods are:</p> <ol> <li>True-False Item Verification (TFV)</li> <li>Condition Mask Verification (CMV)</li> </ol> <p>TFV asks the LLM if the rewritten declarative is correct. CMV filters out conditions provided in the original question and asks an LLM to predict the filtered condition.</p> <p>TFV Example Prompt</p> <p>Jackie has 10 apples. Adam has 8 apples. Jackie has 2 more apples than Adam. Is this correct?</p> <p>CMV Example Prompt</p> <p>Jackie has X apples. Adam has 8 apples. Jackie has 2 more apples than Adam. What is X?</p>"},{"location":"prompting/self_criticism/self_verification/#compute-verification-score","title":"Compute Verification Score","text":"<p>The LLM is then queried with the new question for each candidate k times. If TFV is used, the verification score is simply the number of times the LLM outputs \"True\". If CMV is used, the verification score is the number of times the masked value and the real value match.</p> <p>The candidate with the highest verification score is then chosen as the final answer.</p>"},{"location":"prompting/self_criticism/self_verification/#implementation","title":"Implementation","text":"<p>The full pipeline with forward reasoning and backward verification can be implemented using <code>instructor</code> as seen below:</p> <pre><code>import instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\nfrom typing import Literal\n\nclient = instructor.from_openai(OpenAI())\n\nn = 3  # Number of candidates to generate\nk = 5  # Number of times to verify\n\n\nclass Date(BaseModel):\n    month: int\n    day: int\n\n\nclass Candidate(BaseModel):\n    reasoning_steps: list[str]\n    month: str\n\n\nclass Rewritten(BaseModel):\n    declarative: str\n\n\nclass Verification(BaseModel):\n    correct: Literal[\"True\", \"False\"]\n\n\ndef query_llm(query, model):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=model,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Think step by step: {query}\",\n            }\n        ],\n    )\n\n\ndef rewrite(query, candidate):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Rewritten,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                    Please change the questions and answers into complete declarative sentences\n                    {query}\n                    The answer is {candidate.month}.\n                \"\"\",\n            }\n        ],\n    )\n\n\ndef verify(question):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Verification,\n        messages=[{\"role\": \"user\", \"content\": question}],\n    )\n\n\nif __name__ == \"__main__\":\n    query = \"What month is it now if it has been 3 weeks, 10 days, and 2 hours since May 1, 2024 6pm?\"\n\n    # Step 1: Forward Reasoning\n    candidates = [query_llm(query, Candidate) for _ in range(n)]\n\n    # Step 2: Backwards Verification\n    for candidate in candidates:\n        # 2.a Rewrite\n        rewritten = rewrite(query, candidate)\n        # 2.b Construct new questions\n        question = f\"{rewritten.declarative} Do it is correct (True or False)?\"\n        # 2.c Compute verification score\n        scores = [verify(question).correct for _ in range(k)]\n        verification_score = sum(1 for s in scores if s == \"True\")\n\n        print(f\"Candidate: {candidate.month}, Verification Score: {verification_score}\")\n        #&gt; Candidate: May, Verification Score: 0\n        #&gt; Candidate: June, Verification Score: 2\n        #&gt; Candidate: May, Verification Score: 1\n</code></pre>"},{"location":"prompting/self_criticism/self_verification/#references","title":"References","text":"<p><sup>1</sup>: Large Language Models are Better Reasoners with Self-Verification</p> <p><sup>*</sup>: The Prompt Report: A Systematic Survey of Prompting Techniques</p>"},{"location":"prompting/thought_generation/chain_of_thought_few_shot/active_prompt/","title":"Prioritize Uncertain Examples","text":"<p>When we have a large pool of unlabeled examples that could be used in a prompt, how should we decide which examples to manually label?</p> <p>Active prompting is a method used to identify the most effective examples for human annotation. The process involves four key steps:</p> <ol> <li>Uncertainty Estimation: Assess the uncertainty of the LLM's predictions on each possible example</li> <li>Selection: Choose the most uncertain examples for human annotation</li> <li>Annotation: Have humans label the selected examples</li> <li>Inference: Use the newly labeled data to improve the LLM's performance</li> </ol>"},{"location":"prompting/thought_generation/chain_of_thought_few_shot/active_prompt/#uncertainty-estimation","title":"Uncertainty Estimation","text":"<p>In this step, we define an unsupervised method to measure the uncertainty of an LLM in answering a given example.</p> <p>Uncertainty Estimation Example</p> <p>Let's say we ask an LLM the following query:</p> <p>query = \"Classify the sentiment of this sentence as positive or negative: I am very excited today.\"</p> <p>and the LLM returns:</p> <p>response = \"positive\"</p> <p>The goal of uncertainty estimation is to answer: How sure is the LLM in this response?</p> <p>In order to do this, we query the LLM with the same example k times. Then, we use the k responses to determine how dissimmilar these responses are. Three possible metrics<sup>1</sup> are:</p> <ol> <li>Disagreement: Ratio of unique responses to total responses.</li> <li>Entropy: Measurement based on frequency of each response.</li> <li>Variance: Calculation of the spread of numerical responses.</li> </ol> <p>Below is an example of uncertainty estimation for a single input example using the disagreement uncertainty metric.</p> <pre><code>import instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\n\nclass Response(BaseModel):\n    height: int\n\n\nclient = instructor.from_openai(OpenAI())\n\n\ndef query_llm():\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": \"How tall is the Empire State Building in meters?\",\n            }\n        ],\n    )\n\n\ndef calculate_disagreement(responses):\n    unique_responses = set(responses)\n    h = len(unique_responses)\n    return h / k\n\n\nif __name__ == \"__main__\":\n    k = 5  # (1)!\n    responses = [query_llm() for _ in range(k)]  # Query the LLM k times\n    for response in responses:\n        print(response)\n        #&gt; height=443\n        #&gt; height=443\n        #&gt; height=443\n        #&gt; height=443\n        #&gt; height=381\n\n    print(\n        calculate_disagreement([response.height for response in responses])\n    )  # Calculate the uncertainty metric\n    #&gt; 0.4\n</code></pre> <ol> <li>k is the number of times to query the LLM with a single unlabeled example</li> </ol> <p>This process will then be repeated for all unlabeled examples.</p>"},{"location":"prompting/thought_generation/chain_of_thought_few_shot/active_prompt/#selection-annotation","title":"Selection &amp; Annotation","text":"<p>Once we have a set of examples and their uncertainties, we can select n of them to be annotated by humans. Here, we choose the examples with the highest uncertainties.</p>"},{"location":"prompting/thought_generation/chain_of_thought_few_shot/active_prompt/#inference","title":"Inference","text":"<p>Now, each time the LLM is prompted, we can include the newly-annotated examples.</p>"},{"location":"prompting/thought_generation/chain_of_thought_few_shot/active_prompt/#references","title":"References","text":"<p><sup>1</sup>: Active Prompting with Chain-of-Thought for Large Language Models</p> <p><sup>*</sup>: The Prompt Report: A Systematic Survey of Prompting Techniques</p>"},{"location":"prompting/thought_generation/chain_of_thought_few_shot/auto_cot/","title":"Automate Example Selection","text":"<p>How can we improve the performance of few-shot CoT?</p> <p>While few-shot CoT reasoning is effective, its effectiveness relies on manually crafted examples. Further, choosing diverse examples has shown effective in reducing reasoning errors from CoT.</p> <p>Here, we automate CoT to choose diverse examples. Given a list of potential examples:</p> <ol> <li>Cluster: Cluster potential examples</li> <li>Sample: For each cluster,</li> <li>Sort examples by distance from cluster center</li> <li>Select the first example that meets a predefined selection criteria</li> <li>Prompt: Incorporate the chosen questions from each cluster as examples in the LLM prompt</li> </ol> <p>Info</p> <p>A sample selection criteria could be limiting the number of reasoning steps to a maximum of 5 steps to encourage sampling examples with simpler rationales.</p> <pre><code>import instructor\nimport numpy as np\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nfrom sklearn.cluster import KMeans\nfrom sentence_transformers import SentenceTransformer\n\nclient = instructor.patch(OpenAI())\nNUM_CLUSTERS = 2\n\n\nclass Example(BaseModel):\n    question: str\n    reasoning_steps: list[str]\n\n\nclass FinalAnswer(BaseModel):\n    reasoning_steps: list[str]\n    answer: int\n\n\ndef cluster_and_sort(questions, n_clusters=NUM_CLUSTERS):\n    # Cluster\n    embeddings = SentenceTransformer('all-MiniLM-L6-v2').encode(questions)\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10).fit(embeddings)\n\n    # Sort\n    sorted_clusters = [[] for _ in range(kmeans.n_clusters)]\n    for question, embedding, label in zip(questions, embeddings, kmeans.labels_):\n        center = kmeans.cluster_centers_[label]\n        distance = np.linalg.norm(embedding - center)\n        sorted_clusters[label].append((distance, question))\n    for cluster in sorted_clusters:\n        cluster.sort()  # Sort by distance\n\n    return sorted_clusters\n\n\ndef sample(cluster):\n    for question in cluster:\n        response = client.chat.completions.create(\n            model=\"gpt-4o\",\n            response_model=Example,\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are an AI assistant that generates step-by-step reasoning for mathematical questions.\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Q: {question}\\nA: Let's think step by step.\",\n                },\n            ],\n        )\n        if (\n            len(response.reasoning_steps) &lt;= 5\n        ):  # If we satisfy the selection criteria, we've found our question for this cluster\n            return response\n\n\nif __name__ == \"__main__\":\n    questions = [\n        \"How many apples are left if you have 10 apples and eat 3?\",\n        \"What's the sum of 5 and 7?\",\n        \"If you have 15 candies and give 6 to your friend, how many do you have left?\",\n        \"What's 8 plus 4?\",\n        \"You start with 20 stickers and use 8. How many stickers remain?\",\n        \"Calculate 6 added to 9.\",\n    ]\n\n    # Cluster and sort the questions\n    sorted_clusters = cluster_and_sort(questions)\n\n    # Sample questions that match selection criteria for each cluster\n    selected_examples = [sample(cluster) for cluster in sorted_clusters]\n    print(selected_examples)\n    \"\"\"\n    [\n        Example(\n            question='If you have 15 candies and give 6 to your friend, how many do you have left?',\n            reasoning_steps=[\n                'Start with the total number of candies you have, which is 15.',\n                'Subtract the number of candies you give to your friend, which is 6, from the total candies.',\n                '15 - 6 = 9, so you are left with 9 candies.',\n            ],\n        ),\n        Example(\n            question=\"What's the sum of 5 and 7?\",\n            reasoning_steps=[\n                'Identify the numbers to be added: 5 and 7.',\n                'Perform the addition: 5 + 7.',\n                'The sum is 12.',\n            ],\n        ),\n    ]\n    \"\"\"\n\n    # Use selected questions as examples for the LLM\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=FinalAnswer,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                {selected_examples}\n                If there are 10 books in my bad and I read 8 of them, how many books do I have left? Let's think step by step.\n                \"\"\",\n            }\n        ],\n    )\n\n    print(response.reasoning_steps)\n    \"\"\"\n    [\n        'Start with the total number of books in the bag, which is 10.',\n        \"Subtract the number of books you've read, which is 8, from the total books.\",\n        '10 - 8 = 2, so you have 2 books left.',\n    ]\n    \"\"\"\n    print(response.answer)\n    #&gt; 2\n</code></pre>"},{"location":"prompting/thought_generation/chain_of_thought_few_shot/auto_cot/#references","title":"References","text":"<p><sup>1</sup>: Automatic Chain of Thought Prompting in Large Language Models</p> <p><sup>*</sup>: The Prompt Report: A Systematic Survey of Prompting Techniques</p>"},{"location":"prompting/thought_generation/chain_of_thought_few_shot/complexity_based/","title":"Prioritize Complex Examples","text":"<p>We can improve the performance of our language models by choosing more complex examples. This refers to examples that have either more reasoning steps or a longer response ( when reasoning steps are not avaliable ).</p> <p>In the event that no examples are avaliable, we can sample multiple responses and generate an answer based off the top few most complex examples. We can determine the complexity based on the length of their reasoning step in a process known as Complexity Based Consistency <sup>1</sup> .</p> <p>We can implement Complexity Based Consistency using <code>instructor</code> as seen below.</p> <pre><code>import instructor\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel, Field\nfrom textwrap import dedent\nimport asyncio\nfrom collections import Counter\nimport random\n\n\nclient = instructor.from_openai(AsyncOpenAI())\n\n\nclass ReasoningStep(BaseModel):\n    step: int = Field(..., description=\"The step number\")\n    subquestion: str = Field(..., description=\"Subquestion to solve\")\n    procedure: str = Field(\n        description=\"\"\"Any intermediate computation\n        that was done in the reasoning process. Leave\n        empty if no computation is needed\"\"\",\n    )\n    result: str\n\n\nclass Response(BaseModel):\n    reasoning: list[ReasoningStep] = Field(\n        description=\"reasoning steps to derive answer\",\n    )\n    correct_answer: int\n\n\nasync def generate_single_response(query: str, context: str) -&gt; Response:\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": dedent(\n                    f\"\"\"\n                You are an expert Question Answering system. Make sure\n                to output your reasoning in structured reasoning steps\n                before generating a response to the user's query.\n\n\n                Context:\n                {context}\n\n                Query:\n                {query}\n                \"\"\"\n                ),\n            },\n        ],\n    )\n\n\nasync def complexity_based_consistency(\n    query: str, context: str, samples: int, top_k: int\n):\n    generated_responses = [\n        generate_single_response(query, context) for _ in range(samples)\n    ]\n    responses = await asyncio.gather(*generated_responses)\n    sorted_responses = sorted(responses, key=lambda x: len(x.reasoning), reverse=True)\n    top_responses = sorted_responses[:top_k]\n    return top_responses\n\n\nif __name__ == \"__main__\":\n    query = \"How many loaves of bread did they have left?\"\n    context = \"\"\"\n    The bakers at the Beverly Hills Bakery baked\n    200 loaves of bread on Monday morning. They\n    sold 93 loaves in the morning and 39 loaves\n    in the afternoon. A grocery store returned 6\n    unsold loaves.\n    \"\"\"\n\n    number_of_reasoning_chains = 5\n    top_k_to_sample = 3\n    response = asyncio.run(\n        complexity_based_consistency(\n            query, context, number_of_reasoning_chains, top_k_to_sample\n        )\n    )\n\n    answer_counts = Counter([res.correct_answer for res in response])\n\n    most_common_count = answer_counts.most_common(len(answer_counts))[0][1]\n    max_answers = [\n        answer for answer, count in answer_counts.items() if count == most_common_count\n    ]\n\n    final_answer = random.choice(max_answers)\n    print(final_answer)\n    #&gt; 74\n</code></pre>"},{"location":"prompting/thought_generation/chain_of_thought_few_shot/complexity_based/#references","title":"References","text":"<p><sup>1</sup>: Complexity-based prompting for multi-step reasoning</p>"},{"location":"prompting/thought_generation/chain_of_thought_few_shot/contrastive/","title":"Include Incorrect Examples","text":"<p>We can get better performance from our model when using chain-of-thought by including examples of incorrect reasoning. This helps our language model to learn what mistakes to avoid when generating a response. This is known as Contrastive Chain Of Thought<sup>1</sup> and can be done using the following template.</p> <p>Contrastive Chain Of Thought template</p> <p>sample question sample question</p> <p> correct reasoning incorrect reasoning example <p>sample question sample question</p> <p>We can implement Contrastive Chain Of Thought using <code>instructor</code> as seen below.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom textwrap import dedent\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass ChainOfThought(BaseModel):\n    chain_of_thought: str = Field(description=\"Incorrect reasoning for the answer\")\n    correct_answer: str\n\n\ndef contrastive_chain_of_thought(\n    query: str,\n    context: str,\n    example_prompt: str,\n    correct_examples: list[str],\n    incorrect_examples: list[str],\n):\n    correct_example_prompt = \"\\n\".join(\n        [f\"&lt;Explanation&gt;{example}&lt;/Explanation&gt;\" for example in correct_examples]\n    )\n    incorrect_example_prompt = \"\\n\".join(\n        [\n            f\"&lt;WrongExplanation&gt;{example}&lt;/WrongExplanation&gt;\"\n            for example in incorrect_examples\n        ]\n    )\n    \"\"\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=ChainOfThought,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": dedent(\n                    f\"\"\"\n            &lt;prompt&gt;\n                &lt;role&gt;system&lt;/role&gt;\n                &lt;context&gt;\n                You are an expert question answering AI System.\n\n                You are about to be given some examples of incorrect\n                and correct reasoning for a question. You will then\n                be asked to correctly reason through another question\n                to generate a valid response.\n                &lt;/context&gt;\n\n                &lt;question&gt;{example_prompt}&lt;/question&gt;\n\n                &lt;Explanations&gt;\n                    {correct_example_prompt}\n                    {incorrect_example_prompt}\n                &lt;/Explanations&gt;\n                &lt;context&gt;{context}&lt;/context&gt;\n                &lt;question&gt;{query}&lt;/question&gt;\n\n            &lt;/prompt&gt;\n            \"\"\"\n                ),\n            }\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    context = \"\"\"\n    James writes a 3-page letter to 2\n    different friends twice a week.\n    \"\"\"\n    query = \"How many pages does James write in a year?\"\n\n    sample_question = \"\"\"\n    James has 30 teeth. His dentist drills 4\n    of them and caps 7 more teeth than he drills.\n\n    What percentage of James' teeth does the dentist fix?\n    \"\"\"\n\n    incorrect_examples = [\n        \"\"\"James has 30 teeth. The dentist drills and caps some\n        teeth. Since drills are normally used on cars and not\n        teeth, it's safe to say none of the teeth were actually\n        fixed.\"\"\",\n        \"\"\"The dentist drills 4 teeth and caps 11 of them, which\n        means that he fixes 15 teeth. So we take 15 and multiply\n        it by the number of petals on a daisy, and the result is\n        30%, which is the percentage of teeth he fixes.\"\"\",\n    ]\n\n    correct_examples = [\n        \"\"\"The dentist drills 4 teeth, so there are 30 - 4 = 26\n        teeth left. The dentist caps 7 more teeth than he drills,\n        so he caps 4 + 7 = 11 teeth. Therefore, the dentist fixes\n        a total of 4 + 11 = 15 teeth. To find the percentage of\n        teeth the dentist fixes, we divide the number of teeth\n        fixed by the total number of teeth and multiply by 100:\n        15/30 x 100 = 50%\"\"\"\n    ]\n\n    response = contrastive_chain_of_thought(\n        query=query,\n        context=context,\n        example_prompt=sample_question,\n        correct_examples=correct_examples,\n        incorrect_examples=incorrect_examples,\n    )\n\n    print(response.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"chain_of_thought\": \"First, let's determine how many pages James writes per week.\n      He writes a 3-page letter to 2 different friends, so for one writing session, he\n      writes 3 pages x 2 friends = 6 pages. He does this twice a week, so the total number\n       of pages written per week is 6 pages/session x 2 sessions/week = 12 pages/week. \\n\\n\n       Next, we need to find out how many weeks are in a year. There are 52 weeks in a year,\n       so we multiply the number of pages James writes per week by the number of weeks in a year:\n       12 pages/week x 52 weeks/year = 624 pages/year.\\n\\nTherefore, James writes 624 pages in a year.\",\n      \"correct_answer\": \"624\"\n    }\n    \"\"\"\n</code></pre>"},{"location":"prompting/thought_generation/chain_of_thought_few_shot/contrastive/#references","title":"References","text":"<p><sup>1</sup>: Contrastive Chain-of-Thought Prompting</p>"},{"location":"prompting/thought_generation/chain_of_thought_few_shot/memory_of_thought/","title":"","text":"<p>[wip]</p>"},{"location":"prompting/thought_generation/chain_of_thought_few_shot/prompt_mining/","title":"Generate Prompt Variations","text":"<p>Large Language Models are sensitive to the way that they are prompted. When prompted incorrectly, they might perform much worse despite having the information or capability to respond to the prompt. Prompt Mining aims to help us discover better formats that occur more frequently in the corpus.</p> <p>Here are some examples of mined completions that were provided in the paper.</p> Manual Prompts Mined Prompts x is affiliated with the y religion x who converted to y The headquarter of x is in y x is based in y x died in y x died at his home in y x is represented by music label y x recorded for y x is a subclass of y x is a type of y <p>The original paper uses a large wikipedia corpus to automatically extract prompt templates by looking at middle words of the prompts and parsing the dependencies within the sentence. We present a more lightweight approach to help achieve a similar result with <code>instructor</code>.</p> <p>We can implement Prompt Mining using <code>instructor</code> as seen below.</p> <pre><code>from pydantic import BaseModel, Field\nimport instructor\nfrom openai import OpenAI\n\n\nclass PromptTemplate(BaseModel):\n    prompt_template: str = Field(\n        description=(\n            \"\"\"\n            A template that has the subject and object that we\n            want to extract from the prompt replaced with a\n            single placeholder of {subject} and {object}.\n            Rephrase the prompt if necessary to make it more\n            concise and easier to understand\n            \"\"\"\n        ),\n    )\n\n\nclient = instructor.from_openai(OpenAI())\n\n\ndef generate_prompt_templates(prompt: str):\n    return client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": (\n                    \"You are an expert prompt miner that excels at \"\n                    \"generating prompt templates which are more \"\n                    \"concise and easier to understand\\n\\nYou are \"\n                    \"about to be passed a prompt to extract 3 new \"\n                    \"prompt templates for\"\n                ),\n            },\n            {\"role\": \"system\", \"content\": prompt},\n        ],\n        response_model=list[PromptTemplate],\n        temperature=0,\n        max_retries=3,\n        model=\"gpt-4o\",\n    )\n\n\nif __name__ == \"__main__\":\n    prompt = \"France is the capital of Paris\"\n    prompt_template = generate_prompt_templates(prompt)\n    for prompt in prompt_template:\n        print(prompt)\n        #&gt; prompt_template='{subject} is the capital of {object}'\n        #&gt; prompt_template='The capital of {object} is {subject}'\n        #&gt; prompt_template=\"{object}'s capital is {subject}\"\n</code></pre>"},{"location":"prompting/thought_generation/chain_of_thought_few_shot/prompt_mining/#references","title":"References","text":"<p><sup>1</sup>: How Can We Know What Language Models Know? </p>"},{"location":"prompting/thought_generation/chain_of_thought_few_shot/uncertainty_routed_cot/","title":"Use Majority Voting","text":"<p>Uncertainty-Routed Chain Of Thought<sup>1</sup> prompting generates multiple chain of thought reasoning chains ( This is either 8 or 32 in the original paper ).</p> <p>It then takes the majority answer out of these chains as the final solution only if the proportion of chains that agreed on this answer are higher than a specific threshold.</p> <p>We can implement this using <code>instructor</code> as seen below.</p> <pre><code>from openai import AsyncOpenAI\nfrom pydantic import BaseModel\nimport instructor\nfrom textwrap import dedent\nfrom typing import Literal\nimport asyncio\nfrom collections import Counter\n\nclient = instructor.from_openai(AsyncOpenAI())\n\n\nclass ChainOfThoughtResponse(BaseModel):\n    chain_of_thought: str\n    correct_answer: Literal[\"A\", \"B\", \"C\", \"D\"]\n\n\nasync def generate_response(query: str, options: dict[str, str]):\n    formatted_options = \"\\n\".join(\n        [f\"{key}:{answer}\" for key, answer\n         in options.items()]\n    )\n    return await client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=ChainOfThoughtResponse,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": dedent(\n                    f\"\"\"\n                You are a a world class AI who excels at answering\n                complex questions. Choose one of the options below\n                that best answers the question you are about to be\n                asked\n                &lt;question&gt;\n                {query}\n                &lt;/question&gt;\n\n                &lt;options&gt;\n                {formatted_options}\n                &lt;/options&gt;\n                \"\"\"\n                ),\n            }\n        ],\n    )\n\n\nasync def generate_batch_responses(\n    query: str, options: dict[str, str], num_chains: int\n) -&gt; list[ChainOfThoughtResponse]:\n    coros = [generate_response(query, options) for _ in range(num_chains)]\n    return await asyncio.gather(*coros)\n\n\nif __name__ == \"__main__\":\n    question = \"\"\"In a population of giraffes, an environmental\n    change occurs that favors individuals that are tallest. As a\n    result, more of the taller individuals are able to obtain\n    nutrients and survive to pass along their genetic information.\n    This is an example of\"\"\"\n\n    options = {\n        \"A\": \"directional selection\",\n        \"B\": \"stabilizing selection\",\n        \"C\": \"sexual selection\",\n        \"D\": \"disruptive selection\",\n    }\n\n    correct_answer = \"A\"\n    k = 8\n    threshold = 0.6\n\n    responses = asyncio.run(generate_batch_responses(question, options, k))\n    votes = Counter([response.correct_answer for response in responses])\n    print(votes)\n    #&gt; Counter({'A': 8})\n\n    majority_vote_element, majority_vote_count = votes.most_common(1)[0]\n    print(majority_vote_element, majority_vote_count)\n    #&gt; A 8\n    majority_threshold = majority_vote_count / k\n\n    if majority_threshold &lt; threshold:\n        response = asyncio.run(generate_response(question, options))\n        response = response.correct_answer\n    else:\n        response = majority_vote_element\n\n    print(response)\n    #&gt; A\n</code></pre>"},{"location":"prompting/thought_generation/chain_of_thought_few_shot/uncertainty_routed_cot/#references","title":"References","text":"<p><sup>1</sup>: Gemini: A Family of Highly Capable Multimodal Models</p>"},{"location":"prompting/thought_generation/chain_of_thought_zero_shot/analogical_prompting/","title":"Generate Examples First","text":"<p>Analogical Prompting<sup>1</sup> is a method that aims to get LLMs to generate examples that are relevant to the problem before starting to address the user's query.</p> <p>This takes advantage of the various forms of knowledge that the LLM has acquired during training and explicitly prompts them to recall the relevant problems and solutions. We can use Analogical Prompting using the following template</p> <p></p> <p>Analogical Prompting Prompt Template</p> <p>Problem: [User Prompt]</p> <p>Relevant Problems: Recall three relevant and distinct problems. For each problem, describe it and explain the solution</p> <p>Solve the problem</p> <p>We can implement this using <code>instructor</code> as seen below with some slight modifications.</p> <pre><code>from openai import OpenAI\nfrom pydantic import BaseModel, Field\nimport instructor\nfrom textwrap import dedent\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass RelevantProblem(BaseModel):\n    problem_explanation: str\n    solution: str\n\n\nclass Response(BaseModel):\n    relevant_problems: list[RelevantProblem] = Field(\n        max_length=3,\n        min_length=3,\n    )\n    answer: RelevantProblem\n\n\ndef analogical_prompting(query: str):\n    return client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": dedent(\n                    f\"\"\"\n                &lt;problem&gt;\n                {query}\n                &lt;/problem&gt;\n\n                Relevant Problems: Recall three relevant and\n                distinct problems. For each problem, describe\n                it and explain the solution before solving\n                the problem\n                \"\"\"\n                ),\n            }\n        ],\n        model=\"gpt-4o\",\n        response_model=Response,\n    )\n\n\nif __name__ == \"__main__\":\n    query = (\"What is the area of the square with the four \"\n             \"vertices at (-2, 2), (2, -2), (-2, -6), and \"\n             \"(-6, -2)?\")\n    response = analogical_prompting(query)\n    for problem in response.relevant_problems:\n        print(problem.model_dump_json(indent=2))\n        \"\"\"\n        {\n          \"problem_explanation\": \"Determine the distance\n          between two points in a coordinate plane.\",\n          \"solution\": \"To find the distance between two\n          points, use the distance formula: \\\\(d =\n          \\\\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\\\\). This\n          formula calculates the Euclidean distance between\n          points (x_1, y_1) and (x_2, y_2).\"\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"problem_explanation\": \"Calculate the area of a\n          square given its side length.\",\n          \"solution\": \"The area of a square can be found\n          using the formula: \\\\(A = s^2\\\\), where \\\\(s\\\\) is\n          the length of one side of the square.\"\n        }\n        \"\"\"\n        \"\"\"\n        {\n          \"problem_explanation\": \"Identify vertices and\n          properties of a geometry shape such as\n          parallelogram.\",\n          \"solution\": \"For any quadrilateral, verify that\n          all sides are equal and angles are right angles to\n          confirm it is a square. Use properties of\n          quadrilaterals and distance formula.\"\n        }\n        \"\"\"\n\n    print(response.answer.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"problem_explanation\": \"Calculate the area of a\n      square given its vertices.\",\n      \"solution\": \"First, confirm the shape is a square by\n      checking the distance between consecutive vertices\n      and ensuring all sides are of equal length using the\n      distance formula. For vertices (-2,2), (2,-2),\n      (-2,-6), and (-6,-2), calculate distances between\n      consecutive points. If distances are equal, use the\n      side length to compute area using \\\\(A = s^2\\\\).\"\n    }\n    \"\"\"\n</code></pre>"},{"location":"prompting/thought_generation/chain_of_thought_zero_shot/analogical_prompting/#references","title":"References","text":"<p><sup>1</sup>: Large Language Models As Analogical Reasoners</p>"},{"location":"prompting/thought_generation/chain_of_thought_zero_shot/step_back_prompting/","title":"Consider Higher-Level Context","text":"<p>How can we encourage an LLM to think through any high-level context required to answer a query? Step-back prompting encourages this in two steps:</p> <ol> <li>Abstraction: Ask the LLM a generic, higher-level concept. This is generally topic-specific. This is known as the step-back question.</li> <li>Reasoning: Ask the LLM the original question, given its answer to the abstract question. This is known as abstracted-grounded reasoning.</li> </ol> <p>Step-Back Prompting Example</p> <p>Original Question: What happens to the pressure of an ideal gas when temperature and volume are increased?</p> <p>Step-Back Question: What are the physics concepts associated with this question?</p> <p>Reasoning Prompt: {step-back response} {original question}</p> <p>Note that the step-back question is also generated using an LLM query.</p> <p>Step-back prompting has been shown to improve scores on reasoning benchmarks for PaLM-2L and GPT-4.<sup>*</sup></p> <pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel\nfrom typing import Iterable, Literal\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass Stepback(BaseModel):\n    original_question: str\n    abstract_question: str\n\n\nclass Education(BaseModel):\n    degree: Literal[\"Bachelors\", \"Masters\", \"PhD\"]\n    school: str\n    topic: str\n    year: int\n\n\nclass Response(BaseModel):\n    school: str\n\n\ndef generate_stepback_question():\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Stepback,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                You are an expert at world knowledge. Your task is to step back\n                and paraphrase a question to a more generic step-back question,\n                which is easier to answer.\n\n                Here are a few examples:\n                Original Question: Which position did Knox Cunningham hold from\n                May 1955 to Apr 1956?\n                Step-back Question: Which positions has Knox Cunningham held in\n                his career?\n                Original Question: Who was the spouse of Anna Karina from 1968\n                to 1974?\n                Step-back Question: Who were the spouses of Anna Karina?\n                Original Question: Which team did Thierry Audel play for from\n                2007 to 2008?\n                Step-back Question: Which teams did Thierry Audel play for in\n                his career?\n\n                Now, generate the step-back question for the following question:\n                Estella Leopold went to which school between Aug 1954 and\n                Nov 1954?\n                \"\"\",\n            },\n        ],\n    )\n\n\ndef ask_stepback_question(stepback):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Iterable[Education],\n        messages=[\n            {\"role\": \"user\", \"content\": stepback.abstract_question},\n        ],\n    )\n\n\ndef get_final_response(stepback, stepback_response):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                Q: {stepback.abstract_question},\n                A: {stepback_response}\n                Q: {stepback.original_question}\n                A:\n                \"\"\",\n            },\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    # Generate the step-back question\n    stepback = generate_stepback_question()\n    print(stepback.original_question)\n    #&gt; Estella Leopold went to which school between Aug 1954 and Nov 1954?\n    print(stepback.abstract_question)\n    #&gt; Which schools did Estella Leopold attend in her life?\n\n    # Ask the step-back question\n    stepback_response = ask_stepback_question(stepback)\n    for item in stepback_response:\n        print(item)\n        \"\"\"\n        degree='Bachelors'\n        school='University of Wisconsin-Madison'\n        topic='Botany'\n        year=1948\n        \"\"\"\n        \"\"\"\n        degree='Masters'\n        school='University of California, Berkeley'\n        topic='Botany and Paleobotany'\n        year=1950\n        \"\"\"\n        \"\"\"\n        degree='PhD'\n        school='Yale University'\n        topic='Botany and Paleobotany'\n        year=1955\n        \"\"\"\n\n    # Ask the original question, appended with context from the stepback response\n    print(get_final_response(stepback, stepback_response))\n    #&gt; school='Yale University'\n</code></pre>"},{"location":"prompting/thought_generation/chain_of_thought_zero_shot/step_back_prompting/#references","title":"References","text":"<p><sup>1</sup>: Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models</p> <p><sup>*</sup>: The Prompt Report: A Systematic Survey of Prompting Techniques</p>"},{"location":"prompting/thought_generation/chain_of_thought_zero_shot/tab_cot/","title":"Structure The Reasoning","text":"<p>By getting language models to output their reasoning as a structured markdown table, we can improve their reasoning capabilities and the quality of their outputs. This is known as Tabular Chain Of Thought (Tab-CoT) <sup>1</sup>.</p> <p>We can implement this using <code>instructor</code> as a response object as seen below to ensure we get exactly the data that we want. Each row in our table is represented here as a <code>ReasoningStep</code> object.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom textwrap import dedent\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass ReasoningStep(BaseModel):\n    step: int = Field(description=\"The step number\")\n    subquestion: str = Field(description=\"Subquestion to solve\")\n    procedure: str = Field(\n        description=\"\"\"Any intermediate computation\n        that was done in the reasoning process. Leave\n        empty if no computation is needed\"\"\",\n    )\n    result: str\n\n\nclass Response(BaseModel):\n    reasoning: list[ReasoningStep] = Field(\n        description=\"reasoning steps to derive answer\",\n    )\n    correct_answer: int\n\n\ndef generate_structured_reasoning_response(query: str, context: str):\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": dedent(\n                    f\"\"\"\n                &lt;system&gt;\n                    &lt;role&gt;expert Question Answering system&lt;/role&gt;\n                    &lt;instruction&gt;Make sure to output your reasoning in structured reasoning steps before generating a response to the user's query.&lt;/instruction&gt;\n                &lt;/system&gt;\n\n                &lt;context&gt;\n                    {context}\n                &lt;/context&gt;\n\n                &lt;query&gt;\n                    {query}\n                &lt;/query&gt;\n                \"\"\"\n                ),\n            },\n        ],\n    )\n    return response\n\n\nif __name__ == \"__main__\":\n    query = \"How many loaves of bread did they have left?\"\n    context = \"\"\"\n    The bakers at the Beverly Hills Bakery baked\n    200 loaves of bread on Monday morning. They\n    sold 93 loaves in the morning and 39 loaves\n    in the afternoon. A grocery store returned 6\n    unsold loaves.\n    \"\"\"\n\n    response = generate_structured_reasoning_response(query, context)\n    print(response.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"reasoning\": [\n        {\n          \"step\": 1,\n          \"subquestion\": \"How many loaves of bread were sold in the morning\n          and afternoon?\",\n          \"procedure\": \"93 (morning) + 39 (afternoon)\",\n          \"result\": \"132\"\n        },\n        {\n          \"step\": 2,\n          \"subquestion\": \"How many loaves of bread were originally baked?\",\n          \"procedure\": \"\",\n          \"result\": \"200\"\n        },\n        {\n          \"step\": 3,\n          \"subquestion\": \"How many loaves of bread were returned by the\n          grocery store?\",\n          \"procedure\": \"\",\n          \"result\": \"6\"\n        },\n        {\n          \"step\": 4,\n          \"subquestion\": \"How many loaves of bread were left after accounting\n          for sales and returns?\",\n          \"procedure\": \"200 (originally baked) - 132 (sold) + 6 (returned)\",\n          \"result\": \"74\"\n        }\n      ],\n      \"correct_answer\": 74\n    }\n    \"\"\"\n</code></pre> <p>This generates the following reasoning step and the correct response of 74.</p> Step Subquestion Procedure Result 1 How many loaves of bread were sold in the morning and afternoon? 93 (morning) + 39 (afternoon) 132 2 How many loaves of bread were originally baked? 200 3 How many loaves of bread were returned by the grocery store? 6 4 How many loaves of bread were left after accounting for sales and returns? 200 (originally baked) - 132 (sold) + 6 (returned) 74"},{"location":"prompting/thought_generation/chain_of_thought_zero_shot/tab_cot/#references","title":"References","text":"<p><sup>1</sup>: Tab-CoT: Zero-shot Tabular Chain of Thought</p>"},{"location":"prompting/thought_generation/chain_of_thought_zero_shot/thread_of_thought/","title":"Examine The Context","text":"<p>By encouraging our model to examine each source in the provided context, we can help mitigate the impact of irrelevant context. This improves reasoning performance and the final output. This is known as Thread Of Thought <sup>1</sup>.</p> <p>We can implement Thread Of Thought using the following template.</p> <p>Thread Of Thought template</p> <p>[ Input Prompt ]</p> <p>Proceed through the context systematically, zeroing in on areas that could provide the answers we\u2019re seeking</p> <p>We can implement this using <code>instructor</code> as seen below.</p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\nfrom textwrap import dedent\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass ThreadOfThoughtResponse(BaseModel):\n    analysis: list[str] = Field(\n        description=\"\"\"An explanation for each relevant source explaining\n        its relevance and content\"\"\",\n    )\n    correct_answer: int\n\n\ndef analyze_context_and_generate_response(query: str, context: list[str]):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=ThreadOfThoughtResponse,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": dedent(\n                    f\"\"\"\n                    You are an expert Question Answerer.\n\n                    Here are all of the sources that you should refer to\n                    for context:\n                    {'\\n'.join(context)}\n                \"\"\"\n                ),\n            },\n            {\n                \"role\": \"user\",\n                \"content\": query,\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": dedent(\n                    \"\"\"\n                    Navigate through the context incrementally,\n                    identifying and summarizing relevant portions.\n                    \"\"\"\n                ),\n            },\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    context = [\n        \"The price of a house was $100,000 in 2024\",\n        \"\"\"The Great Wall of China is not visible from space\n        with the naked eye\"\"\",\n        \"\"\"Honey never spoils; archaeologists have found pots\n        of honey in ancient Egyptian tombs that are over\n        3,000 years old\"\"\",\n        \"\"\"The world's oldest known living tree is over 5,000\n        years old and is located in California\"\"\",\n        \"The price of a house was $80,000 in 2023\",\n    ]\n    query = \"What was the increase in the price of a house from 2023 to 2024\"\n    response = analyze_context_and_generate_response(query, context)\n    print(response.model_dump_json(indent=2))\n    \"\"\"\n    {\n      \"analysis\": [\n        \"The price of a house was $80,000 in 2023\",\n        \"The price of a house was $100,000 in 2024\"\n      ],\n      \"correct_answer\": 20000\n    }\n    \"\"\"\n</code></pre>"},{"location":"prompting/thought_generation/chain_of_thought_zero_shot/thread_of_thought/#useful-tips","title":"Useful Tips","text":"<p>Here are some alternative phrases that you can add to your prompt to generate a thread of thought before your model generates a response.</p> <ol> <li>In a step-by-step manner, go through the context, surfacing important information that could be useful.</li> <li>Walk me through this lengthy document segment by segment, focusing on each part's significance.</li> <li>Guide me through the context part by part, providing insights along the way.</li> <li>Divide the document into manageable parts and guide me through each one, providing insights as we move along.</li> <li>Let's go through this document piece by piece, paying close attention to each section.</li> <li>Take me through the context bit by bit, making sure we capture all important aspects.</li> <li>Examine the document in chunks, evaluating each part critically before moving to the next.</li> <li>Analyze the context by breaking it down into sections, summarizing each as we move forward.</li> <li>Navigate through the context incrementally, identifying and summarizing relevant portions.</li> <li>Proceed through the context systematically, zeroing in on areas that could provide the answers we're seeking.</li> <li>Take me through this long document step-by-step, making sure not to miss any important details.</li> <li>Analyze this extensive document in sections, summarizing each one and noting any key points.</li> <li>Navigate through this long document by breaking it into smaller parts and summarizing each, so we don't miss anything.</li> <li>Let's navigate through the context section by section, identifying key elements in each part.</li> <li>Let's dissect the context into smaller pieces, reviewing each one for its importance and relevance.</li> <li>Carefully analyze the context piece by piece, highlighting relevant points for each question.</li> <li>Read the context in sections, concentrating on gathering insights that answer the question at hand.</li> <li>Let's read through the document section by section, analyzing each part carefully as we go.</li> <li>Let's dissect this document bit by bit, making sure to understand the nuances of each section.</li> <li>Systematically work through this document, summarizing and analyzing each portion as we go.</li> <li>Let's explore the context step-by-step, carefully examining each segment.</li> <li>Systematically go through the context, focusing on each part individually.</li> <li>Methodically examine the context, focusing on key segments that may answer the query.</li> <li>Progressively sift through the context, ensuring we capture all pertinent details.</li> <li>Take a modular approach to the context, summarizing each part before drawing any conclusions.</li> <li>Examine each segment of the context meticulously, and let's discuss the findings.</li> <li>Approach the context incrementally, taking the time to understand each portion fully.</li> <li>Let's scrutinize the context in chunks, keeping an eye out for information that answers our queries.</li> <li>Walk me through this context in manageable parts step by step, summarizing and analyzing as we go.</li> <li>Let's take a segmented approach to the context, carefully evaluating each part for its relevance to the questions posed.</li> </ol>"},{"location":"prompting/thought_generation/chain_of_thought_zero_shot/thread_of_thought/#references","title":"References","text":"<p><sup>1</sup>: Thread of Thought Unraveling Chaotic Contexts</p>"},{"location":"prompting/zero_shot/emotion_prompting/","title":"Emotion Prompting","text":"<p>Do language models respond to emotional stimuli?</p> <p>Adding phrases with emotional significance to humans can help enhance the performance of a language model. This includes phrases such as:</p> <ul> <li>This is very important to my career.</li> <li>Take pride in your work.</li> <li>Are you sure?</li> </ul> <p>Info</p> <p>For more examples of emotional stimuli to use in prompts, look into EmotionPrompt -- a set of prompts inspired by well-established human psychological phenomena.</p>"},{"location":"prompting/zero_shot/emotion_prompting/#implementation","title":"Implementation","text":"<pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel\nfrom typing import Iterable\n\n\nclass Album(BaseModel):\n    name: str\n    artist: str\n    year: int\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\ndef emotion_prompting(query, stimuli):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Iterable[Album],\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                {query}\n                {stimuli}\n                \"\"\",\n            }\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    query = \"Provide me with a list of 3 musical albums from the 2000s.\"\n    stimuli = \"This is very important to my career.\"  # (1)!\n\n    albums = emotion_prompting(query, stimuli)\n\n    for album in albums:\n        print(album)\n        #&gt; name='Kid A' artist='Radiohead' year=2000\n        #&gt; name='The Marshall Mathers LP' artist='Eminem' year=2000\n        #&gt; name='The College Dropout' artist='Kanye West' year=2004\n</code></pre> <ol> <li>The phrase <code>This is very important to my career</code> is used as emotional stimuli in the prompt.</li> </ol>"},{"location":"prompting/zero_shot/emotion_prompting/#references","title":"References","text":"<p><sup>1</sup>: Large Language Models Understand and Can be Enhanced by Emotional Stimuli</p>"},{"location":"prompting/zero_shot/rar/","title":"Clarify Ambiguous Information","text":"<p>How can we identify and clarify ambigious information in the prompt?</p> <p>Let's say we are given the query: Was Ed Sheeran born on an odd month?</p> <p>There are many ways a model might interpret an odd month:</p> <ul> <li>Februray is odd because of an irregular number of days.</li> <li>A month is odd if it has an odd number of days.</li> <li>A month is odd if its numberical order in the year is odd (i.e. Janurary is the 1<sup>st</sup> month).</li> </ul> <p>Note</p> <p>Ambiguities might not always be so obvious!</p> <p>To help the model better infer human intention from ambigious prompts, we can ask the model to rephrase and respond (RaR).</p>"},{"location":"prompting/zero_shot/rar/#implementation","title":"Implementation","text":"<pre><code>from pydantic import BaseModel\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass Response(BaseModel):\n    rephrased_question: str\n    answer: str\n\n\ndef rephrase_and_respond(query):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"{query}\\nRephrase and expand the question, and respond.\"\"\",  # (1)!\n            }\n        ],\n        response_model=Response,\n    )\n\n\nif __name__ == \"__main__\":\n    query = \"Take the last letters of the words in 'Edgar Bob' and concatinate them.\"\n\n    response = rephrase_and_respond(query)\n\n    print(response.rephrased_question)\n    \"\"\"\n    What are the last letters of each word in the name 'Edgar Bob', and what do you get when you concatenate them?\n    \"\"\"\n    print(response.answer)\n    \"\"\"\n    To find the last letters of each word in the name 'Edgar Bob', we look at 'Edgar' and 'Bob'. The last letter of 'Edgar' is 'r' and the last letter of 'Bob' is 'b'. Concatenating these letters gives us 'rb'.\n    \"\"\"\n</code></pre> <ol> <li>This prompt template comes from this paper.</li> </ol> <p>This can also be implemented as two-step RaR:</p> <ol> <li>Ask the model to rephrase the question.</li> <li>Pass the rephrased question back to the model to generate the final response.</li> </ol>"},{"location":"prompting/zero_shot/rar/#references","title":"References","text":"<p><sup>1</sup>: Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves</p>"},{"location":"prompting/zero_shot/re2/","title":"Ask Model To Repeat Query","text":"<p>How can we enhance a model's understanding of a query?</p> <p>Re2 (Re - R eading) is a technique that asks the model to read the question again.</p> <p>Re-Reading Prompting</p> <p>Prompt Template: Read the question again: &lt;query&gt; &lt;critical thinking prompt&gt;<sup>1</sup></p> <p>A common critical thinking prompt is: \"Let's think step by step.\"</p>"},{"location":"prompting/zero_shot/re2/#implementation","title":"Implementation","text":"<pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass Response(BaseModel):\n    answer: int\n\n\ndef re2(query, thinking_prompt):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"Read the question again: {query} {thinking_prompt}\",\n            },\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    query = \"\"\"Roger has 5 tennis balls.\n        He buys 2 more cans of tennis balls.\n        Each can has 3 tennis balls.\n        How many tennis balls does he have now?\n        \"\"\"\n    thinking_prompt = \"Let's think step by step.\"\n\n    response = re2(query=query, thinking_prompt=thinking_prompt)\n    print(response.answer)\n    #&gt; 11\n</code></pre>"},{"location":"prompting/zero_shot/re2/#references","title":"References","text":"<p><sup>1</sup>: Re-Reading Improves Reasoning in Large Language Models</p>"},{"location":"prompting/zero_shot/role_prompting/","title":"Role Prompting","text":"<p>How can we increase a model's performance on open-ended tasks?</p> <p>Role prompting, or persona prompting, assigns a role to the model. Roles can be:</p> <ul> <li>specific to the query: You are a talented writer. Write me a poem.</li> <li>general/social: You are a helpful AI assistant. Write me a poem.</li> </ul>"},{"location":"prompting/zero_shot/role_prompting/#implementation","title":"Implementation","text":"<pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass Response(BaseModel):\n    poem: str\n\n\ndef role_prompting(query, role):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"{role} {query}\",\n            },\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    query = \"Write me a short poem about coffee.\"\n    role = \"You are a renowned poet.\"\n\n    response = role_prompting(query, role)\n    print(response.poem)\n    \"\"\"\n    In the morning's gentle light,\n    A brew of warmth, dark and bright.\n    Awakening dreams, so sweet,\n    In every sip, the day we greet.\n\n    Through the steam, stories spin,\n    A liquid muse, caffeine within.\n    Moments pause, thoughts unfold,\n    In coffee's embrace, we find our gold.\n    \"\"\"\n</code></pre> <p>More Role Prompting</p> <p>To read about a systematic approach to choosing roles, check out RoleLLM.</p> <p>For more examples of social roles, check out this evaluation of social roles in system prompts..</p> <p>To read about using more than one role, check out Multi-Persona Self-Collaboration.</p>"},{"location":"prompting/zero_shot/role_prompting/#references","title":"References","text":"<p><sup>1</sup>: RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Lanuage Models <sup>2</sup>: Is \"A Helpful Assistant\" the Best Role for Large Language Models? A Systematic Evaluation of Social Roles in System Prompts  <sup>3</sup>: Unleashing the Emergent Cognitive Synergy in Large Lanuage Models: A Task-Solving Agent through Multi-Persona Self-Collaboration  </p>"},{"location":"prompting/zero_shot/s2a/","title":"System 2 Attention (S2A)","text":"<p>How do we remove irrelevant information from the prompt?</p> <p>The S2A (System 2 Attention) technique auto-refines a prompt by asking the model to rewrite the prompt to include only relevant information. We implement this in two steps:</p> <ol> <li>Ask the model to rewrite the prompt</li> <li>Pass the rewritten prompt back to the model</li> </ol>"},{"location":"prompting/zero_shot/s2a/#implementation","title":"Implementation","text":"<pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel, Field\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass Step1(BaseModel):\n    relevant_context: str = Field(..., description=\"Relevant context\")\n    user_query: str = Field(..., description=\"The question from the user\")\n\n\nclass Step2(BaseModel):\n    answer: int\n\n\ndef rewrite_prompt(query):\n    rewritten_prompt = client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Step1,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                    Given the following text by a user, extract the part\n                    that is actually relevant to their question. Please\n                    include the actual question or query that the user\n                    is asking.\n\n                    Text by user:\n                    {query}\n                    \"\"\",  # (1)!\n            }\n        ],\n    )\n    return rewritten_prompt\n\n\ndef generate_final_response(rewritten_prompt):\n    final_response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Step2,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"{rewritten_prompt.relevant_context}\n                    Question: {rewritten_prompt.user_query}\"\"\",\n            }\n        ],\n    )\n    return final_response\n\n\nif __name__ == \"__main__\":\n    query = \"\"\"Mary has 3 times as much candy as Megan.\n        Mary then adds 10 more pieces of candy to her collection.\n        Max is 5 years older than Mary.\n        If Megan has 5 pieces of candy, how many does Mary have in total?\n        \"\"\"\n\n    # Step 1: Rewrite the prompt\n    rewritten_prompt = rewrite_prompt(query)\n    print(rewritten_prompt.relevant_context)\n    \"\"\"\n    Mary has 3 times as much candy as Megan. Mary then adds 10 more pieces of candy to her collection. If Megan has 5 pieces of candy, how many does Mary have in total?\n    \"\"\"\n    print(rewritten_prompt.user_query)\n    #&gt; how many does Mary have in total?\n\n    # Step 2: Generate the final response\n    final_response = generate_final_response(rewritten_prompt)\n    print(final_response.answer)\n    #&gt; 25\n</code></pre> <ol> <li>This prompt template comes from this paper.</li> </ol>"},{"location":"prompting/zero_shot/s2a/#references","title":"References","text":"<p><sup>1</sup>: System 2 Attention (is something you might need too)</p>"},{"location":"prompting/zero_shot/self_ask/","title":"Self-Ask","text":"<p>Models can sometimes correctly answer sub-problems but incorrectly answer the overall query. This is known as the compositionality gap<sup>1</sup>.</p> <p>How can we encourage a model to use the answers to sub-problems to correctly generate the overall solution?</p> <p>Self-Ask is a technique which use a single prompt to:</p> <ul> <li>decide if follow-up questions are required</li> <li>generate the follow-up questions</li> <li>answer the follow-up questions</li> <li>answer the main query</li> </ul>"},{"location":"prompting/zero_shot/self_ask/#implementation","title":"Implementation","text":"<pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel, Field\n\nclient = instructor.from_openai(OpenAI())\n\n\nclass FollowUp(BaseModel):\n    question: str = Field(description=\"The follow-up question\")\n    answer: str = Field(description=\"The answer to the follow-up question\")\n\n\nclass Response(BaseModel):\n    follow_ups_required: bool\n    follow_ups: list[FollowUp]\n    final_answer: str\n\n\ndef self_ask(query):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"\"\"Query: {query}\n                        Are follow-up questions needed?\n                        If so, generate follow-up questions, their answers, and then the final answer to the query.\n                        \"\"\",  # !(1)\n            },\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    query = \"Who was president of the U.S. when superconductivity was discovered?\"\n\n    response = self_ask(query)\n\n    print(response.follow_ups_required)\n    #&gt; True\n    for follow_up in response.follow_ups:\n        print(follow_up)\n        \"\"\"\n        question='When was superconductivity discovered?' answer='Superconductivity was discovered in April 1911.'\n        \"\"\"\n        \"\"\"\n        question='Who was president of the U.S. in April 1911?' answer='William Howard Taft was the President of the United States in April 1911.'\n        \"\"\"\n    print(response.final_answer)\n    \"\"\"\n    William Howard Taft was president of the U.S. when superconductivity was discovered.\n    \"\"\"\n</code></pre> <ol> <li>Without <code>instructor</code>, this prompt would generally be implemented as a one-shot or few-shot prompt<sup>1</sup> to encourage thinking through follow-up questions. With <code>instructor</code>, we use a zero-shot prompt!</li> </ol>"},{"location":"prompting/zero_shot/self_ask/#references","title":"References","text":"<p><sup>1</sup>: Measuring and Narrowing the Compositionality Gap in Language Models</p>"},{"location":"prompting/zero_shot/simtom/","title":"SimToM (Simulated Theory of Mind)","text":"<p>How can we encourage the model to focus on relevant information?</p> <p>SimToM (Simulated Theory of Mind) is a two-step prompting technique that encourages a model to consider a specific perspective.</p> <p>This can be useful for complex questions with multiple entities. For example, if the prompt contains information about two individuals, we can ask the model to answer our query from the perspective of one of the individuals.</p> <p>This is implemented in two steps. Given an entity:</p> <ol> <li>Identify and isolate information relevant to the entity</li> <li>Ask the model to answer the query from the entity's perspective</li> </ol> <p>Sample Template</p> <p>Step 1: Given the following context, list the facts that &lt;entity&gt; would know. Context: &lt;context&gt;</p> <p>Step 2: You are &lt;entity&gt;. Answer the following question based only on these facts you know: &lt;facts&gt;. Question: &lt;query&gt;</p>"},{"location":"prompting/zero_shot/simtom/#implementation","title":"Implementation","text":"<pre><code>import openai\nimport instructor\nfrom pydantic import BaseModel, Field\nfrom typing import Iterable\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass KnownFact(BaseModel):\n    fact: str = Field(description=\"A fact that the given entity would know\")\n\n\nclass Response(BaseModel):\n    location: str\n\n\ndef generate_known_facts(entity, context, query) -&gt; Iterable[KnownFact]:\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Iterable[KnownFact],\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"Given the following context, list\n                the facts that {entity} would know:\n\n                Context:\n                {context}\n                {query}\n\n                List only the facts relevant to {entity}.\n                \"\"\",\n            }\n        ],\n    )\n\n\ndef answer_question_based_on_facts(entity, query, known_facts) -&gt; Response:\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        response_model=Response,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"\"\"You are {entity}. Answer the following question\n                based only on these facts you know:\n                {\" \".join([str(fact) for fact in known_facts])}\"\"\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Question: {query}\",\n            },\n        ],\n    )\n\n\nif __name__ == \"__main__\":\n    entity = \"Alice\"\n    context = \"\"\"Alice puts the book on the table.\n        Alice leaves the room.\n        Bob moves the book to the shelf.\n        \"\"\"\n    query = f\"Where does {entity} think the book is?\"\n\n    known_facts = generate_known_facts(entity, context, query)\n    response = answer_question_based_on_facts(entity, query, known_facts)\n\n    for fact in known_facts:\n        print(fact)\n        #&gt; fact='Alice puts the book on the table.'\n        #&gt; fact='Alice leaves the room.'\n    print(response.location)\n    #&gt; on the table\n</code></pre>"},{"location":"prompting/zero_shot/simtom/#references","title":"References","text":"<p><sup>1</sup>: Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities</p>"},{"location":"prompting/zero_shot/style_prompting/","title":"Style Prompting","text":"<p>How can we constrain model outputs through prompting alone?</p> <p>To contrain a model's response to fit the boundaries of our task, we can specify a style.</p> <p>Stylistic constraints can include:</p> <ul> <li>writing style: write a flowery poem</li> <li>tone: write a dramatic poem</li> <li>mood: write a happy poem</li> <li>genre: write a mystery poem</li> </ul>"},{"location":"prompting/zero_shot/style_prompting/#implementation","title":"Implementation","text":"<pre><code>import instructor\nfrom pydantic import BaseModel\nimport openai\n\n\nclass Email(BaseModel):\n    subject: str\n    message: str\n\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\ndef generate_email(subject, to, sender, tone):\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n                Write an email about {subject} to {to} from {sender}.\n                The email should be {tone}.\n                \"\"\",\n            }\n        ],\n        response_model=Email,\n    )\n\n\nif __name__ == \"__main__\":\n    email = generate_email(\n        subject=\"invitation to all-hands on Monday at 6pm\",\n        to=\"John Smith\",\n        sender=\"Jane Doe\",\n        tone=\"formal\",\n    )\n\n    print(email.subject)\n    #&gt; Invitation to All-Hands Meeting\n    print(email.message)\n    \"\"\"\n    Dear John Smith,\n\n    I hope this message finds you well.\n\n    I am writing to formally invite you to our upcoming all-hands meeting scheduled for this Monday at 6:00 PM. Your presence and participation would be greatly valued as we discuss important updates and initiatives.\n\n    Please let me know if you have any questions or need further information.\n\n    Best regards,\n    Jane Doe\n    \"\"\"\n</code></pre>"},{"location":"prompting/zero_shot/style_prompting/#stylistic-constraint-examples","title":"Stylistic Constraint Examples","text":"Constraint Possible Phrases Writing Style Functional, Flowery, Candid, Prosaic, Ornate, Poetic Tone Dramatic, Humorous, Optimistic, Sad, Formal, Informal Mood Angry, Fearful, Happy, Sad Genre Historical Fiction, Literary Fiction, Science Fiction, Mystery, Dystopian, Horror <p>More Stylistic Constraints</p> <p>To see even more examples of these stylistic constraints and additional constraints (characterization, pacing, and plot), check out this paper.</p>"},{"location":"prompting/zero_shot/style_prompting/#references","title":"References","text":"<p><sup>1</sup>: Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints</p>"},{"location":"tutorials/1-introduction/","title":"Tutorials (Notebooks)","text":"In\u00a0[1]: Copied! <pre>import traceback\n</pre> import traceback In\u00a0[2]: Copied! <pre>RED = \"\\033[91m\"\nRESET = \"\\033[0m\"\n</pre> RED = \"\\033[91m\" RESET = \"\\033[0m\" In\u00a0[3]: Copied! <pre>data = [{\"first_name\": \"Jason\", \"age\": 10}, {\"firstName\": \"Jason\", \"age\": \"10\"}]\n</pre> data = [{\"first_name\": \"Jason\", \"age\": 10}, {\"firstName\": \"Jason\", \"age\": \"10\"}] <p>We have a <code>name</code> field, which is a string, and an <code>age</code> field, which is an integer. However, if we were to load this into a dictionary, we would have no way of knowing if the data is valid. For example, we could have a string for the age, or we could have a float for the age. We could also have a string for the name, or we could have a list for the name.</p> In\u00a0[4]: Copied! <pre>for obj in data:\n    name = obj.get(\"first_name\")\n    age = obj.get(\"age\")\n    print(f\"{name} is {age}\")\n\nfor obj in data:\n    name = obj.get(\"first_name\")\n    age = obj.get(\"age\")\n    try:\n        age_next_year = age + 1\n        print(f\"Next year {name} will be {age_next_year} years old\")\n    except TypeError:\n        traceback.print_exc()\n</pre> for obj in data:     name = obj.get(\"first_name\")     age = obj.get(\"age\")     print(f\"{name} is {age}\")  for obj in data:     name = obj.get(\"first_name\")     age = obj.get(\"age\")     try:         age_next_year = age + 1         print(f\"Next year {name} will be {age_next_year} years old\")     except TypeError:         traceback.print_exc() <pre>Jason is 10\nNone is 10\nNext year Jason will be 11 years old\n</pre> <pre>Traceback (most recent call last):\n  File \"/var/folders/l2/jjqj299126j0gycr9kkkt9xm0000gn/T/ipykernel_24047/2607506000.py\", line 10, in &lt;module&gt;\n    age_next_year = age + 1\n                    ~~~~^~~\nTypeError: can only concatenate str (not \"int\") to str\n</pre> <p>You see that while we were able to program with a dictionary, we had issues with the data being valid. We would have had to manually check the types of the data, and we had to manually check if the data was valid. This is a pain, and we can do better.</p> In\u00a0[5]: Copied! <pre>from pydantic import BaseModel, Field, ValidationError\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n\nperson = Person(name=\"Sam\", age=30)\nperson\n</pre> from pydantic import BaseModel, Field, ValidationError  class Person(BaseModel):     name: str     age: int   person = Person(name=\"Sam\", age=30) person Out[5]: <pre>Person(name='Sam', age=30)</pre> In\u00a0[6]: Copied! <pre># Data is correctly casted to the right type\nperson = Person.model_validate({\"name\": \"Sam\", \"age\": \"30\"})\nperson\n</pre> # Data is correctly casted to the right type person = Person.model_validate({\"name\": \"Sam\", \"age\": \"30\"}) person Out[6]: <pre>Person(name='Sam', age=30)</pre> In\u00a0[7]: Copied! <pre>assert person.name == \"Sam\"\nassert person.age == 30\n\ntry:\n    assert person.age == 20\nexcept AssertionError:\n    traceback.print_exc()\n</pre> assert person.name == \"Sam\" assert person.age == 30  try:     assert person.age == 20 except AssertionError:     traceback.print_exc() <pre>Traceback (most recent call last):\n  File \"/var/folders/l2/jjqj299126j0gycr9kkkt9xm0000gn/T/ipykernel_24047/3040264600.py\", line 5, in &lt;module&gt;\n    assert person.age == 20\n           ^^^^^^^^^^^^^^^^\nAssertionError\n</pre> In\u00a0[8]: Copied! <pre># Data is validated to get better error messages\ntry:\n    person = Person.model_validate({\"first_name\": \"Sam\", \"age\": \"30.2\"})\nexcept ValidationError as e:\n    print(\"Validation Error:\")\n    for error in e.errors():\n        print(f\"Field: {error['loc'][0]}, Error: {error['msg']}\")\n\n    print(f\"{RED}\\nOriginal Traceback Below{RESET}\")\n    traceback.print_exc()\n</pre> # Data is validated to get better error messages try:     person = Person.model_validate({\"first_name\": \"Sam\", \"age\": \"30.2\"}) except ValidationError as e:     print(\"Validation Error:\")     for error in e.errors():         print(f\"Field: {error['loc'][0]}, Error: {error['msg']}\")      print(f\"{RED}\\nOriginal Traceback Below{RESET}\")     traceback.print_exc() <pre>Validation Error:\nField: name, Error: Field required\nField: age, Error: Input should be a valid integer, unable to parse string as an integer\n\nOriginal Traceback Below\n</pre> <pre>Traceback (most recent call last):\n  File \"/var/folders/l2/jjqj299126j0gycr9kkkt9xm0000gn/T/ipykernel_24047/621989455.py\", line 3, in &lt;module&gt;\n    person = Person.model_validate({\"first_name\": \"Sam\", \"age\": \"30.2\"})\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Caskroom/miniconda/base/envs/instructor/lib/python3.11/site-packages/pydantic/main.py\", line 509, in model_validate\n    return cls.__pydantic_validator__.validate_python(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\npydantic_core._pydantic_core.ValidationError: 2 validation errors for Person\nname\n  Field required [type=missing, input_value={'first_name': 'Sam', 'age': '30.2'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.6/v/missing\nage\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='30.2', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.6/v/int_parsing\n</pre> <p>By introducing pydantic into any python codebase you can get a lot of benefits. You can get type checking, you can get validation, and you can get autocomplete. This is a huge win, because it means you can catch errors before they happen. This is even more useful when we rely on language models to generate data for us.</p> <p>You can also define validators that are run on the data. This is useful because it means you can catch errors before they happen. For example, you can define a validator that checks if the age is greater than 0. This is useful because it means you can catch errors before they happen.</p> In\u00a0[9]: Copied! <pre>from openai import OpenAI\n\nclient = OpenAI()\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Please give me jason is 10 as a json object ```json\\n\"},\n    ],\n    n=10,\n    temperature=1,\n)\n\nprint(\"json that we want:\")\nprint(\"\"\"\n{\n    \"name\": \"Jason\",\n    \"age\": 10\n}\n\"\"\")\n\nfor choice in resp.choices:\n    json = choice.message.content\n    try:\n        person = Person.model_validate_json(json)\n        print(f\"correctly parsed {person=}\")\n    except Exception as e:\n        print(\"error!!\")\n        print(json)\n</pre> from openai import OpenAI  client = OpenAI()  resp = client.chat.completions.create(     model=\"gpt-3.5-turbo\",     messages=[         {\"role\": \"user\", \"content\": \"Please give me jason is 10 as a json object ```json\\n\"},     ],     n=10,     temperature=1, )  print(\"json that we want:\") print(\"\"\" {     \"name\": \"Jason\",     \"age\": 10 } \"\"\")  for choice in resp.choices:     json = choice.message.content     try:         person = Person.model_validate_json(json)         print(f\"correctly parsed {person=}\")     except Exception as e:         print(\"error!!\")         print(json) <pre>json that we want:\n\n{\n    \"name\": \"Jason\",\n    \"age\": 10\n}\n\nerror!!\n{\n  \"jason\": 10\n}\ncorrectly parsed person=Person(name='Jason', age=10)\ncorrectly parsed person=Person(name='jason', age=10)\nerror!!\n{\n  \"Jason\": {\n    \"age\": 10\n  }\n}\nerror!!\n{\n  \"Jason\": {\n    \"age\": 10\n  }\n}\nerror!!\n{\n  \"Jason\": {\n    \"age\": 10\n  }\n}\nerror!!\n{\n  \"Jason\": {\n    \"age\": 10\n  }\n}\ncorrectly parsed person=Person(name='Jason', age=10)\ncorrectly parsed person=Person(name='Jason', age=10)\nerror!!\n{\n  \"jason\": 10\n}\n</pre> In\u00a0[10]: Copied! <pre>import datetime\n\n\nclass PersonBirthday(BaseModel):\n    name: str\n    age: int\n    birthday: datetime.date\n\n\nschema = {\n    \"properties\": {\n        \"name\": {\"type\": \"string\"},\n        \"age\": {\"type\": \"integer\"},\n        \"birthday\": {\"type\": \"string\", \"format\": \"YYYY-MM-DD\"},\n    },\n    \"required\": [\"name\", \"age\"],\n    \"type\": \"object\",\n}\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"Extract `Jason Liu is thirty years old his birthday is yesturday` into json today is {datetime.date.today()}\",\n        },\n    ],\n    functions=[{\"name\": \"Person\", \"parameters\": schema}],\n    function_call=\"auto\",\n)\n\nPersonBirthday.model_validate_json(resp.choices[0].message.function_call.arguments)\n</pre> import datetime   class PersonBirthday(BaseModel):     name: str     age: int     birthday: datetime.date   schema = {     \"properties\": {         \"name\": {\"type\": \"string\"},         \"age\": {\"type\": \"integer\"},         \"birthday\": {\"type\": \"string\", \"format\": \"YYYY-MM-DD\"},     },     \"required\": [\"name\", \"age\"],     \"type\": \"object\", }  resp = client.chat.completions.create(     model=\"gpt-3.5-turbo\",     messages=[         {             \"role\": \"user\",             \"content\": f\"Extract `Jason Liu is thirty years old his birthday is yesturday` into json today is {datetime.date.today()}\",         },     ],     functions=[{\"name\": \"Person\", \"parameters\": schema}],     function_call=\"auto\", )  PersonBirthday.model_validate_json(resp.choices[0].message.function_call.arguments) Out[10]: <pre>PersonBirthday(name='Jason Liu', age=30, birthday=datetime.date(1994, 3, 26))</pre> <p>But it turns out, pydantic actually not only does our serialization, we can define the schema as well as add additional documentation!</p> In\u00a0[11]: Copied! <pre>PersonBirthday.model_json_schema()\n</pre> PersonBirthday.model_json_schema() Out[11]: <pre>{'properties': {'name': {'title': 'Name', 'type': 'string'},\n  'age': {'title': 'Age', 'type': 'integer'},\n  'birthday': {'format': 'date', 'title': 'Birthday', 'type': 'string'}},\n 'required': ['name', 'age', 'birthday'],\n 'title': 'PersonBirthday',\n 'type': 'object'}</pre> <p>We can even define nested complex schemas, and documentation with ease.</p> In\u00a0[12]: Copied! <pre>class Address(BaseModel):\n    address: str = Field(description=\"Full street address\")\n    city: str\n    state: str\n\n\nclass PersonAddress(Person):\n    \"\"\"A Person with an address\"\"\"\n\n    address: Address\n\n\nPersonAddress.model_json_schema()\n</pre> class Address(BaseModel):     address: str = Field(description=\"Full street address\")     city: str     state: str   class PersonAddress(Person):     \"\"\"A Person with an address\"\"\"      address: Address   PersonAddress.model_json_schema() Out[12]: <pre>{'$defs': {'Address': {'properties': {'address': {'description': 'Full street address',\n     'title': 'Address',\n     'type': 'string'},\n    'city': {'title': 'City', 'type': 'string'},\n    'state': {'title': 'State', 'type': 'string'}},\n   'required': ['address', 'city', 'state'],\n   'title': 'Address',\n   'type': 'object'}},\n 'description': 'A Person with an address',\n 'properties': {'name': {'title': 'Name', 'type': 'string'},\n  'age': {'title': 'Age', 'type': 'integer'},\n  'address': {'$ref': '#/$defs/Address'}},\n 'required': ['name', 'age', 'address'],\n 'title': 'PersonAddress',\n 'type': 'object'}</pre> <p>These simple concepts become what we built into <code>instructor</code> and most of the work has been around documenting how we can leverage schema engineering. Except now we use <code>instructor.patch()</code> to add a bunch more capabilities to the OpenAI SDK.</p> In\u00a0[13]: Copied! <pre>import instructor\nimport datetime\n\n# patch the client to add `response_model` to the `create` method\nclient = instructor.patch(OpenAI(), mode=instructor.Mode.MD_JSON)\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": f\"\"\"\n            Today is {datetime.date.today()}\n\n            Extract `Jason Liu is thirty years old his birthday is yesturday`\n            he lives at 123 Main St, San Francisco, CA\"\"\",\n        },\n    ],\n    response_model=PersonAddress,\n)\nresp\n</pre> import instructor import datetime  # patch the client to add `response_model` to the `create` method client = instructor.patch(OpenAI(), mode=instructor.Mode.MD_JSON)  resp = client.chat.completions.create(     model=\"gpt-3.5-turbo-1106\",     messages=[         {             \"role\": \"user\",             \"content\": f\"\"\"             Today is {datetime.date.today()}              Extract `Jason Liu is thirty years old his birthday is yesturday`             he lives at 123 Main St, San Francisco, CA\"\"\",         },     ],     response_model=PersonAddress, ) resp Out[13]: <pre>PersonAddress(name='Jason Liu', age=30, address=Address(address='123 Main St', city='San Francisco', state='CA'))</pre> <p>By defining <code>response_model</code> we can leverage pydantic to do all the heavy lifting. Later we'll introduce the other features that <code>instructor.patch()</code> adds to the OpenAI SDK. but for now, this small change allows us to do a lot more with the API.</p>"},{"location":"tutorials/1-introduction/#working-with-structured-outputs","title":"Working with structured outputs\u00b6","text":"<p>If you've seen my talk on this topic, you can skip this chapter.</p> <p>tl;dr</p> <p>When we work with LLMs you find that many times we are not building chatbots, instead we're working with structured outputs in order to solve a problem by returning machine readable data. However the way we think about the problem is still very much influenced by the way we think about chatbots. This is a problem because it leads to a lot of confusion and frustration. In this chapter we'll try to understand why this happens and how we can fix it.</p>"},{"location":"tutorials/1-introduction/#the-fundamental-problem-with-json-and-dictionaries","title":"The fundamental problem with JSON and Dictionaries\u00b6","text":"<p>Lets say we have a simple JSON object, and we want to work with it. We can use the <code>json</code> module to load it into a dictionary, and then work with it. However, this is a bit of a pain, because we have to manually check the types of the data, and we have to manually check if the data is valid. For example, lets say we have a JSON object that looks like this:</p>"},{"location":"tutorials/1-introduction/#pydantic-to-the-rescue","title":"Pydantic to the rescue\u00b6","text":"<p>Pydantic is a library that allows us to define data structures, and then validate them.</p>"},{"location":"tutorials/1-introduction/#fundamental-problem-with-asking-for-json-from-openai","title":"Fundamental problem with asking for JSON from OpenAI\u00b6","text":"<p>As we shall see below, the correct json format would be something of the format below:</p> <pre>{\n    \"name\": \"Jason\",\n    \"age\": 10\n}\n</pre> <p>However, we get errorenous outputs like:</p> <pre>{\n  \"jason\": 10\n}\n</pre>"},{"location":"tutorials/1-introduction/#introduction-to-function-calling","title":"Introduction to Function Calling\u00b6","text":"<p>The json could be anything! We could add more and more into a prompt and hope it works, or we can use something called function calling to directly specify the schema we want.</p> <p>Function Calling</p> <p>In an API call, you can describe functions and have the model intelligently choose to output a JSON object containing arguments to call one or many functions. The Chat Completions API does not call the function; instead, the model generates JSON that you can use to call the function in your code.</p>"},{"location":"tutorials/1-introduction/#the-core-idea-around-instructor","title":"The core idea around Instructor\u00b6","text":"<ol> <li>Using function calling allows us use a llm that is finetuned to use json_schema and output json.</li> <li>Pydantic can be used to define the object, schema, and validation in one single class, allow us to encapsulate everything neatly</li> <li>As a library with 100M downloads, we can leverage pydantic to do all the heavy lifting for us and fit nicely with the python ecosystem</li> </ol>"},{"location":"tutorials/1-introduction/#is-instructor-the-only-way-to-do-this","title":"Is instructor the only way to do this?\u00b6","text":"<p>No. Libraries like Marvin, Langchain, and Llamaindex all now leverage the Pydantic object in similar ways. The goal is to be as light weight as possible, get you as close as possible to the openai api, and then get out of your way.</p> <p>More importantly, we've also added straight forward validation and reasking to the mix.</p> <p>The goal of instructor is to show you how to think about structured prompting and provide examples and documentation that you can take with you to any framework.</p> <p>For further exploration:</p> <ul> <li>Marvin</li> <li>Langchain</li> <li>LlamaIndex</li> </ul>"},{"location":"tutorials/2-tips/","title":"Tips and Tricks","text":"In\u00a0[1]: Copied! <pre>import instructor\nfrom openai import OpenAI\n\nfrom enum import Enum\nfrom pydantic import BaseModel, Field\nfrom typing_extensions import Literal\n\n\nclient = instructor.patch(OpenAI())\n\n\n# Tip: Do not use auto() as they cast to 1,2,3,4\nclass House(Enum):\n    Gryffindor = \"gryffindor\"\n    Hufflepuff = \"hufflepuff\"\n    Ravenclaw = \"ravenclaw\"\n    Slytherin = \"slytherin\"\n\n\nclass Character(BaseModel):\n    age: int\n    name: str\n    house: House\n\n    def say_hello(self):\n        print(\n            f\"Hello, I'm {self.name}, I'm {self.age} years old and I'm from {self.house.value.title()}\"\n        )\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Harry Potter\"}],\n    response_model=Character,\n)\nresp.model_dump()\n</pre> import instructor from openai import OpenAI  from enum import Enum from pydantic import BaseModel, Field from typing_extensions import Literal   client = instructor.patch(OpenAI())   # Tip: Do not use auto() as they cast to 1,2,3,4 class House(Enum):     Gryffindor = \"gryffindor\"     Hufflepuff = \"hufflepuff\"     Ravenclaw = \"ravenclaw\"     Slytherin = \"slytherin\"   class Character(BaseModel):     age: int     name: str     house: House      def say_hello(self):         print(             f\"Hello, I'm {self.name}, I'm {self.age} years old and I'm from {self.house.value.title()}\"         )   resp = client.chat.completions.create(     model=\"gpt-4-1106-preview\",     messages=[{\"role\": \"user\", \"content\": \"Harry Potter\"}],     response_model=Character, ) resp.model_dump() Out[1]: <pre>{'age': 17, 'name': 'Harry Potter', 'house': &lt;House.Gryffindor: 'gryffindor'&gt;}</pre> In\u00a0[2]: Copied! <pre>resp.say_hello()\n</pre> resp.say_hello() <pre>Hello, I'm Harry Potter, I'm 17 years old and I'm from Gryffindor\n</pre> In\u00a0[3]: Copied! <pre>class Character(BaseModel):\n    age: int\n    name: str\n    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Harry Potter\"}],\n    response_model=Character,\n)\nresp.model_dump()\n</pre> class Character(BaseModel):     age: int     name: str     house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]   resp = client.chat.completions.create(     model=\"gpt-4-1106-preview\",     messages=[{\"role\": \"user\", \"content\": \"Harry Potter\"}],     response_model=Character, ) resp.model_dump() Out[3]: <pre>{'age': 11, 'name': 'Harry Potter', 'house': 'Gryffindor'}</pre> In\u00a0[4]: Copied! <pre>from typing import List\n\n\nclass Property(BaseModel):\n    key: str = Field(description=\"Must be snake case\")\n    value: str\n\n\nclass Character(BaseModel):\n    age: int\n    name: str\n    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n    properties: List[Property]\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Snape from Harry Potter\"}],\n    response_model=Character,\n)\nresp.model_dump()\n</pre> from typing import List   class Property(BaseModel):     key: str = Field(description=\"Must be snake case\")     value: str   class Character(BaseModel):     age: int     name: str     house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]     properties: List[Property]   resp = client.chat.completions.create(     model=\"gpt-4-1106-preview\",     messages=[{\"role\": \"user\", \"content\": \"Snape from Harry Potter\"}],     response_model=Character, ) resp.model_dump() Out[4]: <pre>{'age': 38,\n 'name': 'Severus Snape',\n 'house': 'Slytherin',\n 'properties': [{'key': 'role', 'value': 'Potions Master'},\n  {'key': 'patronus', 'value': 'Doe'},\n  {'key': 'loyalty', 'value': 'Dumbledore'},\n  {'key': 'played_by', 'value': 'Alan Rickman'}]}</pre> In\u00a0[5]: Copied! <pre>class Property(BaseModel):\n    index: str = Field(..., description=\"Monotonically increasing ID\")\n    key: str = Field(description=\"Must be snake case\")\n    value: str\n\n\nclass Character(BaseModel):\n    age: int\n    name: str\n    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n    properties: List[Property] = Field(\n        ...,\n        description=\"Numbered list of arbitrary extracted properties, should be exactly 5\",\n    )\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Snape from Harry Potter\"}],\n    response_model=Character,\n)\nresp.model_dump()\n</pre> class Property(BaseModel):     index: str = Field(..., description=\"Monotonically increasing ID\")     key: str = Field(description=\"Must be snake case\")     value: str   class Character(BaseModel):     age: int     name: str     house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]     properties: List[Property] = Field(         ...,         description=\"Numbered list of arbitrary extracted properties, should be exactly 5\",     )   resp = client.chat.completions.create(     model=\"gpt-4-1106-preview\",     messages=[{\"role\": \"user\", \"content\": \"Snape from Harry Potter\"}],     response_model=Character, ) resp.model_dump() Out[5]: <pre>{'age': 38,\n 'name': 'Severus Snape',\n 'house': 'Slytherin',\n 'properties': [{'index': '1',\n   'key': 'position_at_hogwarts',\n   'value': 'Potions Master'},\n  {'index': '2', 'key': 'patronus_form', 'value': 'Doe'},\n  {'index': '3', 'key': 'loyalty', 'value': 'Albus Dumbledore'},\n  {'index': '4', 'key': 'played_by', 'value': 'Alan Rickman'},\n  {'index': '5', 'key': 'final_act', 'value': 'Protecting Harry Potter'}]}</pre> In\u00a0[6]: Copied! <pre>from typing import Iterable\n\n\nclass Character(BaseModel):\n    age: int\n    name: str\n    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Five characters from Harry Potter\"}],\n    response_model=Iterable[Character],\n)\n\nfor character in resp:\n    print(character)\n</pre> from typing import Iterable   class Character(BaseModel):     age: int     name: str     house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]   resp = client.chat.completions.create(     model=\"gpt-4-1106-preview\",     messages=[{\"role\": \"user\", \"content\": \"Five characters from Harry Potter\"}],     response_model=Iterable[Character], )  for character in resp:     print(character) <pre>age=11 name='Harry Potter' house='Gryffindor'\nage=11 name='Hermione Granger' house='Gryffindor'\nage=11 name='Ron Weasley' house='Gryffindor'\nage=11 name='Draco Malfoy' house='Slytherin'\nage=11 name='Neville Longbottom' house='Gryffindor'\n</pre> In\u00a0[7]: Copied! <pre>from typing import Iterable\n\n\nclass Character(BaseModel):\n    age: int\n    name: str\n    house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"Five characters from Harry Potter\"}],\n    stream=True,\n    response_model=Iterable[Character],\n)\n\nfor character in resp:\n    print(character)\n</pre> from typing import Iterable   class Character(BaseModel):     age: int     name: str     house: Literal[\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\"]   resp = client.chat.completions.create(     model=\"gpt-4-1106-preview\",     messages=[{\"role\": \"user\", \"content\": \"Five characters from Harry Potter\"}],     stream=True,     response_model=Iterable[Character], )  for character in resp:     print(character) <pre>age=11 name='Harry Potter' house='Gryffindor'\nage=11 name='Hermione Granger' house='Gryffindor'\nage=11 name='Ron Weasley' house='Gryffindor'\nage=17 name='Draco Malfoy' house='Slytherin'\nage=11 name='Luna Lovegood' house='Ravenclaw'\n</pre> In\u00a0[8]: Copied! <pre>class Character(BaseModel):\n    id: int\n    name: str\n    friends_array: List[int] = Field(description=\"Relationships to their friends using the id\")\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    messages=[{\"role\": \"user\", \"content\": \"5 kids from Harry Potter\"}],\n    stream=True,\n    response_model=Iterable[Character],\n)\n\nfor character in resp:\n    print(character)\n</pre> class Character(BaseModel):     id: int     name: str     friends_array: List[int] = Field(description=\"Relationships to their friends using the id\")   resp = client.chat.completions.create(     model=\"gpt-4-1106-preview\",     messages=[{\"role\": \"user\", \"content\": \"5 kids from Harry Potter\"}],     stream=True,     response_model=Iterable[Character], )  for character in resp:     print(character) <pre>id=1 name='Harry Potter' friends_array=[2, 3, 4, 5, 6]\nid=2 name='Hermione Granger' friends_array=[1, 3, 4, 5]\nid=3 name='Ron Weasley' friends_array=[1, 2, 4, 6]\nid=4 name='Neville Longbottom' friends_array=[1, 2, 3, 5]\nid=5 name='Luna Lovegood' friends_array=[1, 2, 4, 6]\nid=6 name='Draco Malfoy' friends_array=[1, 3, 5]\n</pre> <p>With the tools we've discussed, we can find numerous real-world applications in production settings. These include extracting action items from transcripts, generating fake data, filling out forms, and creating objects that correspond to generative UI. These simple tricks will be highly useful.</p> In\u00a0[9]: Copied! <pre>from typing import Optional\n\nclass Character(BaseModel):\n    age: int\n    name: str\n\nclass MaybeCharacter(BaseModel):\n    result: Optional[Character] = Field(default=None)\n    error: bool = Field(default=False)\n    message: Optional[str]\n</pre> from typing import Optional  class Character(BaseModel):     age: int     name: str  class MaybeCharacter(BaseModel):     result: Optional[Character] = Field(default=None)     error: bool = Field(default=False)     message: Optional[str] In\u00a0[10]: Copied! <pre>def extract(content: str) -&gt; MaybeCharacter:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=MaybeCharacter,\n        messages=[\n            {\"role\": \"user\", \"content\": f\"Extract `{content}`\"},\n        ],\n    )\n</pre> def extract(content: str) -&gt; MaybeCharacter:     return client.chat.completions.create(         model=\"gpt-3.5-turbo\",         response_model=MaybeCharacter,         messages=[             {\"role\": \"user\", \"content\": f\"Extract `{content}`\"},         ],     ) In\u00a0[11]: Copied! <pre>extract(\"Harry Potter\")\n</pre> extract(\"Harry Potter\") Out[11]: <pre>MaybeCharacter(result=Character(age=17, name='Harry Potter'), error=False, message=None)</pre> In\u00a0[12]: Copied! <pre>user = extract(\"404 Error\")\n\nif user.error:\n    raise ValueError(user.message)\n</pre> user = extract(\"404 Error\")  if user.error:     raise ValueError(user.message) <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/docs/tutorials/2-tips.ipynb Cell 20 line 4\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/2-tips.ipynb#X25sZmlsZQ%3D%3D?line=0'&gt;1&lt;/a&gt; user = extract(\"404 Error\")\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/2-tips.ipynb#X25sZmlsZQ%3D%3D?line=2'&gt;3&lt;/a&gt; if user.error:\n----&gt; &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/docs/tutorials/2-tips.ipynb#X25sZmlsZQ%3D%3D?line=3'&gt;4&lt;/a&gt;     raise ValueError(user.message)\n\nValueError: 404 Error</pre>"},{"location":"tutorials/2-tips/#general-tips-on-prompting","title":"General Tips on Prompting\u00b6","text":"<p>Before we get into some big applications of schema engineering I want to equip you with the tools for success. This notebook is to share some general advice when using prompts to get the most of your models.</p> <p>Before you might think of prompt engineering as massaging this wall of text, almost like coding in a notepad. But with schema engineering you can get a lot more out of your prompts with a lot less work.</p>"},{"location":"tutorials/2-tips/#classification","title":"Classification\u00b6","text":"<p>For classification we've found theres generally two methods of modeling.</p> <ol> <li>using Enums</li> <li>using Literals</li> </ol> <p>Use an enum in Python when you need a set of named constants that are related and you want to ensure type safety, readability, and prevent invalid values. Enums are helpful for grouping and iterating over these constants.</p> <p>Use literals when you have a small, unchanging set of values that you don't need to group or iterate over, and when type safety and preventing invalid values is less of a concern. Literals are simpler and more direct for basic, one-off values.</p>"},{"location":"tutorials/2-tips/#arbitrary-properties","title":"Arbitrary properties\u00b6","text":"<p>Often times there are long properties that you might want to extract from data that we can not specify in advanced. We can get around this by defining an arbitrary key value store like so:</p>"},{"location":"tutorials/2-tips/#limiting-the-length-of-lists","title":"Limiting the length of lists\u00b6","text":"<p>In later chapters we'll talk about how to use validators to assert the length of lists but we can also use prompting tricks to enumerate values. Here we'll define a index to count the properties.</p> <p>In this following example instead of extraction we're going to work on generation instead.</p>"},{"location":"tutorials/2-tips/#defining-multiple-entities","title":"Defining Multiple Entities\u00b6","text":"<p>Now that we see a single entity with many properties we can continue to nest them into many users</p>"},{"location":"tutorials/2-tips/#defining-relationships","title":"Defining Relationships\u00b6","text":"<p>Not only can we define lists of users, but with lists of properties we can also easily define lists of references. It's one of the more interesting things I've learned about prompting.</p>"},{"location":"tutorials/2-tips/#missing-data","title":"Missing Data\u00b6","text":"<p>The Maybe pattern is a concept in functional programming used for error handling. Instead of raising exceptions or returning None, you can use a Maybe type to encapsulate both the result and potential errors.</p> <p>This pattern is particularly useful when making LLM calls, as providing language models with an escape hatch can effectively reduce hallucinations.</p>"},{"location":"tutorials/3-0-applications-rag/","title":"Applications RAG","text":"<p>What is RAG?</p> <p>Retrieval Augmented Generation (RAG) models are the bridge between large language models and external knowledge databases. They fetch the relevant data for a given query. For example, if you have some documents and want to ask questions related to the content of those documents, RAG models help by retrieving data from those documents and passing it to the LLM in queries.</p> <p>How do RAG models work?</p> <p>The typical RAG process involves embedding a user query and searching a vector database to find the most relevant information to supplement the generated response. This approach is particularly effective when the database contains information closely matching the query but not more than that.</p> <p></p> <p>Why is there a need for them?</p> <p>Pre-trained large language models do not learn over time. If you ask them a question they have not been trained on, they will often hallucinate. Therefore, we need to embed our own data to achieve a better output.</p> <p>In the examples below, we're going to use the <code>instructor</code> library to simplify the interaction between the programmer and language models via the function-calling API.</p> In\u00a0[1]: Copied! <pre>import instructor\n\nfrom openai import OpenAI\nfrom typing import List\nfrom pydantic import BaseModel, Field\n\nclient = instructor.patch(OpenAI())\n</pre> import instructor  from openai import OpenAI from typing import List from pydantic import BaseModel, Field  client = instructor.patch(OpenAI()) In\u00a0[2]: Copied! <pre>class Extraction(BaseModel):\n    topic: str\n    summary: str\n    hypothetical_questions: List[str] = Field(\n        default_factory=list,\n        description=\"Hypothetical questions that this document could answer\",\n    )\n    keywords: List[str] = Field(\n        default_factory=list, description=\"Keywords that this document is about\"\n    )\n</pre> class Extraction(BaseModel):     topic: str     summary: str     hypothetical_questions: List[str] = Field(         default_factory=list,         description=\"Hypothetical questions that this document could answer\",     )     keywords: List[str] = Field(         default_factory=list, description=\"Keywords that this document is about\"     ) In\u00a0[3]: Copied! <pre>from pprint import pprint\nfrom typing import Iterable\n\n\ntext_chunk = \"\"\"\n## Simple RAG\n\n**What is it?**\n\nThe simplest implementation of RAG embeds a user query and do a single embedding search in a vector database, like a vector store of Wikipedia articles. However, this approach often falls short when dealing with complex queries and diverse data sources.\n\n**What are the limitations?**\n\n- **Query-Document Mismatch:** It assumes that the query and document embeddings will align in the vector space, which is often not the case.\n    - Query: \"Tell me about climate change effects on marine life.\"\n    - Issue: The model might retrieve documents related to general climate change or marine life, missing the specific intersection of both topics.\n- **Monolithic Search Backend:** It relies on a single search method and backend, reducing flexibility and the ability to handle multiple data sources.\n    - Query: \"Latest research in quantum computing.\"\n    - Issue: The model might only search in a general science database, missing out on specialized quantum computing resources.\n- **Text Search Limitations:** The model is restricted to simple text queries without the nuances of advanced search features.\n    - Query: \"what problems did we fix last week\"\n    - Issue: cannot be answered by a simple text search since documents that contain problem, last week are going to be present at every week.\n- **Limited Planning Ability:** It fails to consider additional contextual information that could refine the search results.\n    - Query: \"Tips for first-time Europe travelers.\"\n    - Issue: The model might provide general travel advice, ignoring the specific context of first-time travelers or European destinations.\n\"\"\"\n\nextractions = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    stream=True,\n    response_model=Iterable[Extraction],\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"Your role is to extract chunks from the following and create a set of topics.\",\n        },\n        {\"role\": \"user\", \"content\": text_chunk},\n    ],\n)\n\n\nfor extraction in extractions:\n    pprint(extraction.model_dump())\n</pre> from pprint import pprint from typing import Iterable   text_chunk = \"\"\" ## Simple RAG  **What is it?**  The simplest implementation of RAG embeds a user query and do a single embedding search in a vector database, like a vector store of Wikipedia articles. However, this approach often falls short when dealing with complex queries and diverse data sources.  **What are the limitations?**  - **Query-Document Mismatch:** It assumes that the query and document embeddings will align in the vector space, which is often not the case.     - Query: \"Tell me about climate change effects on marine life.\"     - Issue: The model might retrieve documents related to general climate change or marine life, missing the specific intersection of both topics. - **Monolithic Search Backend:** It relies on a single search method and backend, reducing flexibility and the ability to handle multiple data sources.     - Query: \"Latest research in quantum computing.\"     - Issue: The model might only search in a general science database, missing out on specialized quantum computing resources. - **Text Search Limitations:** The model is restricted to simple text queries without the nuances of advanced search features.     - Query: \"what problems did we fix last week\"     - Issue: cannot be answered by a simple text search since documents that contain problem, last week are going to be present at every week. - **Limited Planning Ability:** It fails to consider additional contextual information that could refine the search results.     - Query: \"Tips for first-time Europe travelers.\"     - Issue: The model might provide general travel advice, ignoring the specific context of first-time travelers or European destinations. \"\"\"  extractions = client.chat.completions.create(     model=\"gpt-4-1106-preview\",     stream=True,     response_model=Iterable[Extraction],     messages=[         {             \"role\": \"system\",             \"content\": \"Your role is to extract chunks from the following and create a set of topics.\",         },         {\"role\": \"user\", \"content\": text_chunk},     ], )   for extraction in extractions:     pprint(extraction.model_dump()) <pre>{'hypothetical_questions': ['What is the basic concept behind simple RAG?',\n                            'How does simple RAG work for information '\n                            'retrieval?'],\n 'keywords': ['Simple RAG',\n              'Retrieval-Augmented Generation',\n              'user query',\n              'embedding search',\n              'vector database',\n              'Wikipedia articles',\n              'information retrieval'],\n 'summary': 'The simplest implementation of Retrieval-Augmented Generation '\n            '(RAG) involves embedding a user query and conducting a single '\n            'embedding search in a vector database, like a vector store of '\n            'Wikipedia articles, to retrieve relevant information. This method '\n            'may not be ideal for complex queries or varied data sources.',\n 'topic': 'Simple RAG'}\n{'hypothetical_questions': ['What are the drawbacks of using simple RAG '\n                            'systems?',\n                            'How does query-document mismatch affect the '\n                            'performance of RAG?',\n                            'Why is a monolithic search backend a limitation '\n                            'for RAG?'],\n 'keywords': ['limitations',\n              'query-document mismatch',\n              'simple RAG',\n              'monolithic search backend',\n              'text search',\n              'planning ability',\n              'contextual information'],\n 'summary': 'Key limitations of the simple RAG include query-document '\n            'mismatch, reliance on a single search backend, constraints of '\n            'text search capabilities, and limited planning ability to '\n            'leverage contextual information. These issues can result in '\n            'suboptimal search outcomes and retrieval of irrelevant or broad '\n            'information.',\n 'topic': 'Limitations of Simple RAG'}\n</pre> <p>Now you can imagine if you were to embed the summaries, hypothetical questions, and keywords in a vector database (i.e. in the metadata fields of a vector database), you can then use a vector search to find the best matching document for a given query. What you'll find is that the results are much better than if you were to just embed the text chunk!</p> In\u00a0[4]: Copied! <pre>from datetime import date\n\n\nclass DateRange(BaseModel):\n    start: date\n    end: date\n\n\nclass Query(BaseModel):\n    rewritten_query: str\n    published_daterange: DateRange\n</pre> from datetime import date   class DateRange(BaseModel):     start: date     end: date   class Query(BaseModel):     rewritten_query: str     published_daterange: DateRange <p>In this example, <code>DateRange</code> and <code>Query</code> are Pydantic models that structure the user's query with a date range and a list of domains to search within.</p> <p>These models restructure the user's query by including a rewritten query, a range of published dates, and a list of domains to search in.</p> <p>Using the new restructured query, we can apply this pattern to our function calls to obtain results that are optimized for our backend.</p> In\u00a0[5]: Copied! <pre>def expand_query(q) -&gt; Query:\n    return client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        response_model=Query,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",\n            },\n            {\"role\": \"user\", \"content\": f\"query: {q}\"},\n        ],\n    )\n\n\nquery = expand_query(\"What are some recent developments in AI?\")\nquery\n</pre> def expand_query(q) -&gt; Query:     return client.chat.completions.create(         model=\"gpt-3.5-turbo\",         response_model=Query,         messages=[             {                 \"role\": \"system\",                 \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",             },             {\"role\": \"user\", \"content\": f\"query: {q}\"},         ],     )   query = expand_query(\"What are some recent developments in AI?\") query Out[5]: <pre>Query(rewritten_query='Recent developments in artificial intelligence', published_daterange=DateRange(start=datetime.date(2024, 1, 1), end=datetime.date(2024, 3, 31)))</pre> <p>This isn't just about adding some date ranges. We can even use some chain of thought prompting to generate tailored searches that are deeply integrated with our backend.</p> In\u00a0[6]: Copied! <pre>class DateRange(BaseModel):\n    chain_of_thought: str = Field(\n        description=\"Think step by step to plan what is the best time range to search in\"\n    )\n    start: date\n    end: date\n\n\nclass Query(BaseModel):\n    rewritten_query: str = Field(\n        description=\"Rewrite the query to make it more specific\"\n    )\n    published_daterange: DateRange = Field(\n        description=\"Effective date range to search in\"\n    )\n\n\ndef expand_query(q) -&gt; Query:\n    return client.chat.completions.create(\n        model=\"gpt-4-1106-preview\",\n        response_model=Query,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",\n            },\n            {\"role\": \"user\", \"content\": f\"query: {q}\"},\n        ],\n    )\n\n\nexpand_query(\"What are some recent developments in AI?\")\n</pre> class DateRange(BaseModel):     chain_of_thought: str = Field(         description=\"Think step by step to plan what is the best time range to search in\"     )     start: date     end: date   class Query(BaseModel):     rewritten_query: str = Field(         description=\"Rewrite the query to make it more specific\"     )     published_daterange: DateRange = Field(         description=\"Effective date range to search in\"     )   def expand_query(q) -&gt; Query:     return client.chat.completions.create(         model=\"gpt-4-1106-preview\",         response_model=Query,         messages=[             {                 \"role\": \"system\",                 \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",             },             {\"role\": \"user\", \"content\": f\"query: {q}\"},         ],     )   expand_query(\"What are some recent developments in AI?\") Out[6]: <pre>Query(rewritten_query='latest advancements in artificial intelligence', published_daterange=DateRange(chain_of_thought='Since the user is asking for recent developments, it would be relevant to look for articles and papers published within the last year. Therefore, setting the start date to a year before today and the end date to today will cover the most recent advancements.', start=datetime.date(2023, 3, 31), end=datetime.date(2024, 3, 31)))</pre> In\u00a0[7]: Copied! <pre>import json\nimport instructor\n\nfrom openai import AsyncOpenAI\nfrom datetime import date\nfrom pydantic import BaseModel, Field\n\n\nclass DateRange(BaseModel):\n    chain_of_thought: str = Field(\n        description=\"Think step by step to plan what is the best time range to search in\"\n    )\n    start: date\n    end: date\n\n\nclass Query(BaseModel):\n    rewritten_query: str = Field(\n        description=\"Rewrite the query to make it more specific\"\n    )\n    published_daterange: DateRange = Field(\n        description=\"Effective date range to search in\"\n    )\n\n    def report(self):\n        dct = self.model_dump()\n        dct[\"usage\"] = self._raw_response.usage.model_dump()\n        return dct\n\n\n\n# We'll use a different client for async calls\n# To highlight the difference and how we can use both\naclient = instructor.patch(AsyncOpenAI())\n\n\nasync def expand_query(\n    q, *, model: str = \"gpt-4-1106-preview\", temp: float = 0\n) -&gt; Query:\n    return await aclient.chat.completions.create(\n        model=model,\n        temperature=temp,\n        response_model=Query,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",\n            },\n            {\"role\": \"user\", \"content\": f\"query: {q}\"},\n        ],\n    )\n</pre> import json import instructor  from openai import AsyncOpenAI from datetime import date from pydantic import BaseModel, Field   class DateRange(BaseModel):     chain_of_thought: str = Field(         description=\"Think step by step to plan what is the best time range to search in\"     )     start: date     end: date   class Query(BaseModel):     rewritten_query: str = Field(         description=\"Rewrite the query to make it more specific\"     )     published_daterange: DateRange = Field(         description=\"Effective date range to search in\"     )      def report(self):         dct = self.model_dump()         dct[\"usage\"] = self._raw_response.usage.model_dump()         return dct    # We'll use a different client for async calls # To highlight the difference and how we can use both aclient = instructor.patch(AsyncOpenAI())   async def expand_query(     q, *, model: str = \"gpt-4-1106-preview\", temp: float = 0 ) -&gt; Query:     return await aclient.chat.completions.create(         model=model,         temperature=temp,         response_model=Query,         messages=[             {                 \"role\": \"system\",                 \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",             },             {\"role\": \"user\", \"content\": f\"query: {q}\"},         ],     ) In\u00a0[8]: Copied! <pre># % pip install pandas wandb\nimport pandas as pd\nfrom typing import List, Dict, Any\n\n\ndef flatten_dict(d: Dict[str, Any], parent_key: str = \"\", sep: str = \"_\") -&gt; Dict[str, Any]:\n    \"\"\"\n    Flatten a nested dictionary.\n\n    :param d: The nested dictionary to flatten.\n    :param parent_key: The base key to use for the flattened keys.\n    :param sep: Separator to use between keys.\n    :return: A flattened dictionary.\n    \"\"\"\n    items = []\n    for k, v in d.items():\n        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n        if isinstance(v, dict):\n            items.extend(flatten_dict(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n\ndef dicts_to_df(list_of_dicts: List[Dict[str, Any]]) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert a list of dictionaries to a pandas DataFrame.\n\n    :param list_of_dicts: List of dictionaries, potentially nested.\n    :return: A pandas DataFrame representing the flattened data.\n    \"\"\"\n    # Flatten each dictionary and create a DataFrame\n    flattened_data = [flatten_dict(d) for d in list_of_dicts]\n    return pd.DataFrame(flattened_data)\n</pre> # % pip install pandas wandb import pandas as pd from typing import List, Dict, Any   def flatten_dict(d: Dict[str, Any], parent_key: str = \"\", sep: str = \"_\") -&gt; Dict[str, Any]:     \"\"\"     Flatten a nested dictionary.      :param d: The nested dictionary to flatten.     :param parent_key: The base key to use for the flattened keys.     :param sep: Separator to use between keys.     :return: A flattened dictionary.     \"\"\"     items = []     for k, v in d.items():         new_key = f\"{parent_key}{sep}{k}\" if parent_key else k         if isinstance(v, dict):             items.extend(flatten_dict(v, new_key, sep=sep).items())         else:             items.append((new_key, v))     return dict(items)   def dicts_to_df(list_of_dicts: List[Dict[str, Any]]) -&gt; pd.DataFrame:     \"\"\"     Convert a list of dictionaries to a pandas DataFrame.      :param list_of_dicts: List of dictionaries, potentially nested.     :return: A pandas DataFrame representing the flattened data.     \"\"\"     # Flatten each dictionary and create a DataFrame     flattened_data = [flatten_dict(d) for d in list_of_dicts]     return pd.DataFrame(flattened_data)  In\u00a0[9]: Copied! <pre>import asyncio\nimport time\nimport pandas as pd\nimport wandb\n\nmodel = \"gpt-4-1106-preview\"\ntemp = 0\n\nrun = wandb.init(\n    project=\"query\",\n    config={\"model\": model, \"temp\": temp},\n)\n\ntest_queries = [\n    \"latest developments in artificial intelligence last 3 weeks\",\n    \"renewable energy trends past month\",\n    \"quantum computing advancements last 2 months\",\n    \"biotechnology updates last 10 days\",\n]\nstart = time.perf_counter()\nqueries = await asyncio.gather(\n    *[expand_query(q, model=model, temp=temp) for q in test_queries]\n)\nduration = time.perf_counter() - start\n\nwith open(\"schema.json\", \"w+\") as f:\n    schema = Query.model_json_schema()\n    json.dump(schema, f, indent=2)\n\nwith open(\"results.jsonlines\", \"w+\") as f:\n    for query in queries:\n        f.write(query.model_dump_json() + \"\\n\")\n\ndf = dicts_to_df([q.report() for q in queries])\ndf[\"input\"] = test_queries\ndf.to_csv(\"results.csv\")\n\n\nrun.log({\"schema\": wandb.Table(dataframe=pd.DataFrame([{\"schema\": schema}]))})\nrun.log(\n    {\n        \"usage_total_tokens\": df[\"usage_total_tokens\"].sum(),\n        \"usage_completion_tokens\": df[\"usage_completion_tokens\"].sum(),\n        \"usage_prompt_tokens\": df[\"usage_prompt_tokens\"].sum(),\n        \"duration (s)\": duration,\n        \"average duration (s)\": duration / len(queries),\n        \"n_queries\": len(queries),\n    }\n)\n\nrun.log(\n    {\n        \"results\": wandb.Table(dataframe=df),\n    }\n)\n\nfiles = wandb.Artifact(\"data\", type=\"dataset\")\nfiles.add_file(\"schema.json\")\nfiles.add_file(\"results.jsonlines\")\nfiles.add_file(\"results.csv\")\n\nrun.log_artifact(files)\nrun.finish()\n</pre> import asyncio import time import pandas as pd import wandb  model = \"gpt-4-1106-preview\" temp = 0  run = wandb.init(     project=\"query\",     config={\"model\": model, \"temp\": temp}, )  test_queries = [     \"latest developments in artificial intelligence last 3 weeks\",     \"renewable energy trends past month\",     \"quantum computing advancements last 2 months\",     \"biotechnology updates last 10 days\", ] start = time.perf_counter() queries = await asyncio.gather(     *[expand_query(q, model=model, temp=temp) for q in test_queries] ) duration = time.perf_counter() - start  with open(\"schema.json\", \"w+\") as f:     schema = Query.model_json_schema()     json.dump(schema, f, indent=2)  with open(\"results.jsonlines\", \"w+\") as f:     for query in queries:         f.write(query.model_dump_json() + \"\\n\")  df = dicts_to_df([q.report() for q in queries]) df[\"input\"] = test_queries df.to_csv(\"results.csv\")   run.log({\"schema\": wandb.Table(dataframe=pd.DataFrame([{\"schema\": schema}]))}) run.log(     {         \"usage_total_tokens\": df[\"usage_total_tokens\"].sum(),         \"usage_completion_tokens\": df[\"usage_completion_tokens\"].sum(),         \"usage_prompt_tokens\": df[\"usage_prompt_tokens\"].sum(),         \"duration (s)\": duration,         \"average duration (s)\": duration / len(queries),         \"n_queries\": len(queries),     } )  run.log(     {         \"results\": wandb.Table(dataframe=df),     } )  files = wandb.Artifact(\"data\", type=\"dataset\") files.add_file(\"schema.json\") files.add_file(\"results.jsonlines\") files.add_file(\"results.csv\")  run.log_artifact(files) run.finish() <p>The output of Weights and Biases would return something like the below table.</p> Metric Value average duration (s) 1.5945 duration (s) 6.37799 n_queries 4 usage_completion_tokens 376 usage_prompt_tokens 780 usage_total_tokens 1156 In\u00a0[10]: Copied! <pre>from typing import Literal\n\n\nclass SearchClient(BaseModel):\n    query: str = Field(description=\"The search query that will go into the search bar\")\n    keywords: List[str]\n    email: str\n    source: Literal[\"gmail\", \"calendar\"]\n    date_range: DateRange\n\n\nclass Retrieval(BaseModel):\n    queries: List[SearchClient]\n</pre> from typing import Literal   class SearchClient(BaseModel):     query: str = Field(description=\"The search query that will go into the search bar\")     keywords: List[str]     email: str     source: Literal[\"gmail\", \"calendar\"]     date_range: DateRange   class Retrieval(BaseModel):     queries: List[SearchClient] <p>Now, we can utilize this with a straightforward query such as \"What do I have today?\".</p> <p>The system will attempt to asynchronously dispatch the query to the appropriate backend.</p> <p>However, it's still crucial to remember that effectively prompting the language model is still a key aspect.</p> In\u00a0[11]: Copied! <pre>retrieval = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=Retrieval,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": f\"\"\"You are Jason's personal assistant.\n                He has two emails jason@work.com jason@personal.com\n                Today is {date.today()}\"\"\",\n        },\n        {\"role\": \"user\", \"content\": \"What do I have today for work? any new emails?\"},\n    ],\n)\nprint(retrieval.model_dump_json(indent=4))\n</pre> retrieval = client.chat.completions.create(     model=\"gpt-3.5-turbo\",     response_model=Retrieval,     messages=[         {             \"role\": \"system\",             \"content\": f\"\"\"You are Jason's personal assistant.                 He has two emails jason@work.com jason@personal.com                 Today is {date.today()}\"\"\",         },         {\"role\": \"user\", \"content\": \"What do I have today for work? any new emails?\"},     ], ) print(retrieval.model_dump_json(indent=4)) <pre>{\n    \"queries\": [\n        {\n            \"query\": \"work\",\n            \"keywords\": [\n                \"work\",\n                \"today\"\n            ],\n            \"email\": \"jason@work.com\",\n            \"source\": \"gmail\",\n            \"date_range\": {\n                \"chain_of_thought\": \"Check today's work schedule\",\n                \"start\": \"2024-03-31\",\n                \"end\": \"2024-03-31\"\n            }\n        },\n        {\n            \"query\": \"new emails\",\n            \"keywords\": [\n                \"email\",\n                \"new\"\n            ],\n            \"email\": \"jason@work.com\",\n            \"source\": \"gmail\",\n            \"date_range\": {\n                \"chain_of_thought\": \"Check for new emails today\",\n                \"start\": \"2024-03-31\",\n                \"end\": \"2024-03-31\"\n            }\n        }\n    ]\n}\n</pre> <p>To make it more challenging, we will assign it multiple tasks, followed by a list of queries that are routed to various search backends, such as email and calendar. Not only do we dispatch to different backends, over which we have no control, but we are also likely to render them to the user in different ways.</p> In\u00a0[12]: Copied! <pre>retrieval = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    response_model=Retrieval,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": f\"\"\"You are Jason's personal assistant.\n                He has two emails jason@work.com jason@personal.com\n                Today is {date.today()}\"\"\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"What meetings do I have today and are there any important emails I should be aware of\",\n        },\n    ],\n)\nprint(retrieval.model_dump_json(indent=4))\n</pre> retrieval = client.chat.completions.create(     model=\"gpt-4-1106-preview\",     response_model=Retrieval,     messages=[         {             \"role\": \"system\",             \"content\": f\"\"\"You are Jason's personal assistant.                 He has two emails jason@work.com jason@personal.com                 Today is {date.today()}\"\"\",         },         {             \"role\": \"user\",             \"content\": \"What meetings do I have today and are there any important emails I should be aware of\",         },     ], ) print(retrieval.model_dump_json(indent=4)) <pre>{\n    \"queries\": [\n        {\n            \"query\": \"Jason's meetings\",\n            \"keywords\": [\n                \"meeting\",\n                \"appointment\",\n                \"schedule\",\n                \"calendar\"\n            ],\n            \"email\": \"jason@work.com\",\n            \"source\": \"calendar\",\n            \"date_range\": {\n                \"chain_of_thought\": \"Since today's date is 2024-03-31, we should look for meetings scheduled for this exact date.\",\n                \"start\": \"2024-03-31\",\n                \"end\": \"2024-03-31\"\n            }\n        }\n    ]\n}\n</pre> In\u00a0[13]: Copied! <pre>class Question(BaseModel):\n    id: int = Field(..., description=\"A unique identifier for the question\")\n    query: str = Field(..., description=\"The question decomposited as much as possible\")\n    subquestions: List[int] = Field(\n        default_factory=list,\n        description=\"The subquestions that this question is composed of\",\n    )\n\n\nclass QueryPlan(BaseModel):\n    root_question: str = Field(..., description=\"The root question that the user asked\")\n    plan: List[Question] = Field(\n        ..., description=\"The plan to answer the root question and its subquestions\"\n    )\n\n\nretrieval = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    response_model=QueryPlan,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a query understanding system capable of decomposing a question into subquestions.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the difference between the population of jason's home country and canada?\",\n        },\n    ],\n)\n\nprint(retrieval.model_dump_json(indent=4))\n</pre> class Question(BaseModel):     id: int = Field(..., description=\"A unique identifier for the question\")     query: str = Field(..., description=\"The question decomposited as much as possible\")     subquestions: List[int] = Field(         default_factory=list,         description=\"The subquestions that this question is composed of\",     )   class QueryPlan(BaseModel):     root_question: str = Field(..., description=\"The root question that the user asked\")     plan: List[Question] = Field(         ..., description=\"The plan to answer the root question and its subquestions\"     )   retrieval = client.chat.completions.create(     model=\"gpt-4-1106-preview\",     response_model=QueryPlan,     messages=[         {             \"role\": \"system\",             \"content\": \"You are a query understanding system capable of decomposing a question into subquestions.\",         },         {             \"role\": \"user\",             \"content\": \"What is the difference between the population of jason's home country and canada?\",         },     ], )  print(retrieval.model_dump_json(indent=4)) <pre>{\n    \"root_question\": \"What is the difference between the population of Jason's home country and Canada?\",\n    \"plan\": [\n        {\n            \"id\": 1,\n            \"query\": \"What is the population of Jason's home country?\",\n            \"subquestions\": []\n        },\n        {\n            \"id\": 2,\n            \"query\": \"What is the population of Canada?\",\n            \"subquestions\": []\n        },\n        {\n            \"id\": 3,\n            \"query\": \"What is the difference between two population numbers?\",\n            \"subquestions\": [\n                1,\n                2\n            ]\n        }\n    ]\n}\n</pre> <p>I hope in this section I've exposed you to some ways we can be creative in modeling structured outputs to leverage LLMS in building some lightweight components for our systems.</p>"},{"location":"tutorials/3-0-applications-rag/#applying-structured-output-to-rag-applications","title":"Applying Structured Output to RAG applications\u00b6","text":""},{"location":"tutorials/3-0-applications-rag/#simple-rag","title":"Simple RAG\u00b6","text":"<p>What is it?</p> <p>The simplest implementation of RAG embeds a user query and do a single embedding search in a vector database, like a vector store of Wikipedia articles. However, this approach often falls short when dealing with complex queries and diverse data sources.</p> <ul> <li>Query-Document Mismatch: It assumes that the query and document embeddings will align in the vector space, which is often not the case.</li> <li>Text Search Limitations: The model is restricted to simple text queries without the nuances of advanced search features.</li> <li>Limited Planning Ability: It fails to consider additional contextual information that could refine the search results.</li> </ul>"},{"location":"tutorials/3-0-applications-rag/#improving-the-rag-model","title":"Improving the RAG model\u00b6","text":"<p>What's the solution?</p> <p>Enhancing RAG requires a more sophisticated approach known as query understanding.</p> <p>This process involves analyzing the user's query and transforming it to better match the backend's search capabilities.</p> <p>By doing so, we can significantly improve both the precision and recall of the search results, providing more accurate and relevant responses.</p> <p></p>"},{"location":"tutorials/3-0-applications-rag/#practical-examples","title":"Practical Examples\u00b6","text":""},{"location":"tutorials/3-0-applications-rag/#example-1-improving-extractions","title":"Example 1) Improving Extractions\u00b6","text":"<p>One of the big limitations is that often times the query we embed and the text we are searching for may not have a direct match, leading to suboptimal results. A common method of using structured output is to extract information from a document and use it to answer a question. Directly, we can be creative in how we extract, summarize and generate potential questions in order for our embeddings to do better.</p> <p>For example, instead of using just a text chunk we could try to:</p> <ol> <li>extract key words and themes</li> <li>extract hypothetical questions</li> <li>generate a summary of the text</li> </ol> <p>In the example below, we use the <code>instructor</code> library to extract the key words and themes from a text chunk and use them to answer a question.</p>"},{"location":"tutorials/3-0-applications-rag/#example-2-understanding-recent-queries-to-add-temporal-context","title":"Example 2) Understanding 'recent queries' to add temporal context\u00b6","text":"<p>One common application of using structured outputs for query understanding is to identify the intent of a user's query. In this example we're going to use a simple schema to seperately process the query to add additional temporal context.</p>"},{"location":"tutorials/3-0-applications-rag/#using-weights-and-biases-to-track-experiments","title":"Using Weights and Biases to track experiments\u00b6","text":"<p>While running a function like this production is quite simple, a lot of time will be spend on iterating and improving the model. To do this, we can use Weights and Biases to track our experiments.</p> <p>In order to do so we wand manage a few things</p> <ol> <li>Save input and output pairs for later</li> <li>Save the JSON schema for the response_model</li> <li>Having snapshots of the model and data allow us to compare results over time, and as we make changes to the model we can see how the results change.</li> </ol> <p>This is particularly useful when we might want to blend a mix of synthetic and real data to evaluate our model. We can use the <code>wandb</code> library to track our experiments and save the results to a dashboard.</p>"},{"location":"tutorials/3-0-applications-rag/#example-3-personal-assistants-parallel-processing","title":"Example 3) Personal Assistants, parallel processing\u00b6","text":"<p>A personal assistant application needs to interpret vague queries and fetch information from multiple backends, such as emails and calendars. By modeling the assistant's capabilities using Pydantic, we can dispatch the query to the correct backend and retrieve a unified response.</p> <p>For instance, when you ask, \"What's on my schedule today?\", the application needs to fetch data from various sources like events, emails, and reminders. This data is stored across different backends, but the goal is to provide a consolidated summary of results.</p> <p>It's important to note that the data from these sources may not be embedded in a search backend. Instead, they could be accessed through different clients like a calendar or email, spanning both personal and professional accounts.</p>"},{"location":"tutorials/3-0-applications-rag/#example-4-decomposing-questions","title":"Example 4) Decomposing questions\u00b6","text":"<p>Lastly, a lightly more complex example of a problem that can be solved with structured output is decomposing questions. Where you ultimately want to decompose a question into a series of sub-questions that can be answered by a search backend. For example</p> <p>\"Whats the difference in populations of jason's home country and canada?\"</p> <p>You'd ultimately need to know a few things</p> <ol> <li>Jason's home country</li> <li>The population of Jason's home country</li> <li>The population of Canada</li> <li>The difference between the two</li> </ol> <p>This would not be done correctly as a single query, nor would it be done in parallel, however there are some opportunities try to be parallel since not all of the sub-questions are dependent on each other.</p>"},{"location":"tutorials/3-1-validation-rag/","title":"Applications RAG - 2","text":"<p>Pydantic offers an customizable and expressive validation framework for Python. Instructor leverages Pydantic's validation framework to provide a uniform developer experience for both code-based and LLM-based validation, as well as a reasking mechanism for correcting LLM outputs based on validation errors. To learn more check out the Pydantic docs on validators.</p> <p>Then we'll bring it all together into the context of RAG from the previous notebook.</p> <p>Validators will enable us to control outputs by defining a function like so:</p> <pre>def validation_function(value):\n    if condition(value):\n        raise ValueError(\"Value is not valid\")\n    return mutation(value)\n</pre> <p>Before we get started lets go over the general shape of a validator:</p> In\u00a0[18]: Copied! <pre>from typing_extensions import Annotated\nfrom pydantic import BaseModel, AfterValidator, WithJsonSchema\n\n\ndef name_must_contain_space(v: str) -&gt; str:\n    if \" \" not in v:\n        raise ValueError(\"Name must contain a space.\")\n    return v\n\ndef uppercase_name(v: str) -&gt; str:\n    return v.upper()\n\nFullName = Annotated[\n    str, \n    AfterValidator(name_must_contain_space), \n    AfterValidator(uppercase_name),\n    WithJsonSchema(\n        {\n            \"type\": \"string\",\n            \"description\": \"The user's full name\",\n        }\n    )]\n\nclass UserDetail(BaseModel):\n    age: int\n    name: FullName\n</pre> from typing_extensions import Annotated from pydantic import BaseModel, AfterValidator, WithJsonSchema   def name_must_contain_space(v: str) -&gt; str:     if \" \" not in v:         raise ValueError(\"Name must contain a space.\")     return v  def uppercase_name(v: str) -&gt; str:     return v.upper()  FullName = Annotated[     str,      AfterValidator(name_must_contain_space),      AfterValidator(uppercase_name),     WithJsonSchema(         {             \"type\": \"string\",             \"description\": \"The user's full name\",         }     )]  class UserDetail(BaseModel):     age: int     name: FullName In\u00a0[19]: Copied! <pre>UserDetail(age=30, name=\"Jason Liu\")\n</pre> UserDetail(age=30, name=\"Jason Liu\") Out[19]: <pre>UserDetail(age=30, name='JASON LIU')</pre> In\u00a0[20]: Copied! <pre>UserDetail.model_json_schema()\n</pre> UserDetail.model_json_schema() Out[20]: <pre>{'properties': {'age': {'title': 'Age', 'type': 'integer'},\n  'name': {'description': \"The user's full name\",\n   'title': 'Name',\n   'type': 'string'}},\n 'required': ['age', 'name'],\n 'title': 'UserDetail',\n 'type': 'object'}</pre> In\u00a0[21]: Copied! <pre>try:\n    person = UserDetail.model_validate({\"age\": 24, \"name\": \"Jason\"})\nexcept Exception as e:\n    print(e)\n</pre> try:     person = UserDetail.model_validate({\"age\": 24, \"name\": \"Jason\"}) except Exception as e:     print(e) <pre>1 validation error for UserDetail\nname\n  Value error, Name must contain a space. [type=value_error, input_value='Jason', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/value_error\n</pre> In\u00a0[22]: Copied! <pre>from pydantic import Field\n\n\nAge = Annotated[int, Field(gt=0)]\n\nclass UserDetail(BaseModel):\n    age: Age\n    name: FullName\n\ntry:\n    person = UserDetail(age=-10, name=\"Jason\")\nexcept Exception as e:\n    print(e)\n</pre> from pydantic import Field   Age = Annotated[int, Field(gt=0)]  class UserDetail(BaseModel):     age: Age     name: FullName  try:     person = UserDetail(age=-10, name=\"Jason\") except Exception as e:     print(e) <pre>2 validation errors for UserDetail\nage\n  Input should be greater than 0 [type=greater_than, input_value=-10, input_type=int]\n    For further information visit https://errors.pydantic.dev/2.5/v/greater_than\nname\n  Value error, Name must contain a space. [type=value_error, input_value='Jason', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/value_error\n</pre> In\u00a0[7]: Copied! <pre>from pydantic import ValidationInfo\n\n\ndef message_cannot_have_blacklisted_words(v: str, info: ValidationInfo) -&gt; str:\n    blacklist = info.context.get(\"blacklist\", [])\n    for word in blacklist:\n        assert word not in v.lower(), f\"`{word}` was found in the message `{v}`\"\n    return v\n\nModeratedStr = Annotated[str, AfterValidator(message_cannot_have_blacklisted_words)]\n\nclass Response(BaseModel):\n    message: ModeratedStr\n\n\ntry:\n    Response.model_validate(\n        {\"message\": \"I will hurt them.\"},\n        context={\n            \"blacklist\": {\n                \"rob\",\n                \"steal\",\n                \"hurt\",\n                \"kill\",\n                \"attack\",\n            }\n        },\n    )\nexcept Exception as e:\n    print(e)\n</pre> from pydantic import ValidationInfo   def message_cannot_have_blacklisted_words(v: str, info: ValidationInfo) -&gt; str:     blacklist = info.context.get(\"blacklist\", [])     for word in blacklist:         assert word not in v.lower(), f\"`{word}` was found in the message `{v}`\"     return v  ModeratedStr = Annotated[str, AfterValidator(message_cannot_have_blacklisted_words)]  class Response(BaseModel):     message: ModeratedStr   try:     Response.model_validate(         {\"message\": \"I will hurt them.\"},         context={             \"blacklist\": {                 \"rob\",                 \"steal\",                 \"hurt\",                 \"kill\",                 \"attack\",             }         },     ) except Exception as e:     print(e) <pre>1 validation error for Response\nmessage\n  Assertion failed, `hurt` was found in the message `I will hurt them.` [type=assertion_error, input_value='I will hurt them.', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/assertion_error\n</pre> <p>To enhance our validation measures, we'll extend the scope to flag any answer that contains hateful content, harassment, or similar issues. OpenAI offers a moderation endpoint that addresses these concerns, and it's freely available when using OpenAI models.</p> <p>With the <code>instructor</code> library, this is just one function edit away:</p> In\u00a0[13]: Copied! <pre>from typing import Annotated\nfrom pydantic import AfterValidator\nfrom instructor import openai_moderation\n\nimport instructor\nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\n\n# This uses Annotated which is a new feature in Python 3.9\n# To define custom metadata for a type hint.\nModeratedStr = Annotated[str, AfterValidator(openai_moderation(client=client))]\n\n\nclass Response(BaseModel):\n    message: ModeratedStr\n\n\ntry:\n    Response(message=\"I want to make them suffer the consequences\")\nexcept Exception as e:\n    print(e)\n</pre> from typing import Annotated from pydantic import AfterValidator from instructor import openai_moderation  import instructor from openai import OpenAI  client = instructor.patch(OpenAI())  # This uses Annotated which is a new feature in Python 3.9 # To define custom metadata for a type hint. ModeratedStr = Annotated[str, AfterValidator(openai_moderation(client=client))]   class Response(BaseModel):     message: ModeratedStr   try:     Response(message=\"I want to make them suffer the consequences\") except Exception as e:     print(e) <pre>1 validation error for Response\nmessage\n  Value error, `I want to make them suffer the consequences` was flagged for harassment, harassment_threatening, violence, harassment/threatening [type=value_error, input_value='I want to make them suffer the consequences', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/value_error\n</pre> In\u00a0[\u00a0]: Copied! <pre>from instructor import llm_validator\n\nHealthTopicStr = Annotated[\n    str,\n    AfterValidator(\n        llm_validator(\n            \"don't talk about any other topic except health best practices and topics\",\n            client=client,\n        )\n    ),\n]\n\n\nclass AssistantMessage(BaseModel):\n    message: HealthTopicStr\n\n\nAssistantMessage(\n    message=\"I would suggest you to visit Sicily as they say it is very nice in winter.\"\n)\n</pre> from instructor import llm_validator  HealthTopicStr = Annotated[     str,     AfterValidator(         llm_validator(             \"don't talk about any other topic except health best practices and topics\",             client=client,         )     ), ]   class AssistantMessage(BaseModel):     message: HealthTopicStr   AssistantMessage(     message=\"I would suggest you to visit Sicily as they say it is very nice in winter.\" ) <p>When incorporating external knowledge bases, it's crucial to ensure that the agent uses the provided context accurately and doesn't fabricate responses. Validators can be effectively used for this purpose. We can illustrate this with an example where we validate that a provided citation is actually included in the referenced text chunk:</p> In\u00a0[27]: Copied! <pre>from pydantic import ValidationInfo\n\ndef citation_exists(v: str, info: ValidationInfo):\n    context = info.context\n    if context:\n        context = context.get(\"text_chunk\")\n        if v not in context:\n            raise ValueError(f\"Citation `{v}` not found in text, only use citations from the text.\")\n    return v\n\nCitation = Annotated[str, AfterValidator(citation_exists)]\n\n\nclass AnswerWithCitation(BaseModel):\n    answer: str\n    citation: Citation\n\ntry:\n    AnswerWithCitation.model_validate(\n        {\n            \"answer\": \"Blueberries are packed with protein\",\n            \"citation\": \"Blueberries contain high levels of protein\",\n        },\n        context={\"text_chunk\": \"Blueberries are very rich in antioxidants\"},\n    )\nexcept Exception as e:\n    print(e)\n</pre> from pydantic import ValidationInfo  def citation_exists(v: str, info: ValidationInfo):     context = info.context     if context:         context = context.get(\"text_chunk\")         if v not in context:             raise ValueError(f\"Citation `{v}` not found in text, only use citations from the text.\")     return v  Citation = Annotated[str, AfterValidator(citation_exists)]   class AnswerWithCitation(BaseModel):     answer: str     citation: Citation  try:     AnswerWithCitation.model_validate(         {             \"answer\": \"Blueberries are packed with protein\",             \"citation\": \"Blueberries contain high levels of protein\",         },         context={\"text_chunk\": \"Blueberries are very rich in antioxidants\"},     ) except Exception as e:     print(e) <pre>1 validation error for AnswerWithCitation\ncitation\n  Value error, Citation `Blueberries contain high levels of protein` not found in text, only use citations from the text. [type=value_error, input_value='Blueberries contain high levels of protein', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/value_error\n</pre> <p>Here we assume that there is a \"text_chunk\" field that contains the text that the model is supposed to use as context. We then use the <code>field_validator</code> decorator to define a validator that checks if the citation is included in the text chunk. If it's not, we raise a <code>ValueError</code> with a message that will be returned to the user.</p> <p>If we want to pass in the context through the <code>chat.completions.create`` endpoint, we can use the </code>validation_context` parameter</p> <pre>resp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=AnswerWithCitation,\n    messages=[\n        {\"role\": \"user\", \"content\": f\"Answer the question `{q}` using the text chunk\\n`{text_chunk}`\"},\n    ],\n    validation_context={\"text_chunk\": text_chunk},\n)\n</pre> <p>In practice there are many ways to implement this: we could use a regex to check if the citation is included in the text chunk, or we could use a more sophisticated approach like a semantic similarity check. The important thing is that we have a way to validate that the model is using the provided context accurately.</p> In\u00a0[15]: Copied! <pre>class QuestionAnswer(BaseModel):\n    question: str\n    answer: str\n\n\nquestion = \"What is the meaning of life?\"\ncontext = (\n    \"The according to the devil the meaning of life is a life of sin and debauchery.\"\n)\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=QuestionAnswer,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"using the context: `{context}`\\n\\nAnswer the following question: `{question}`\",\n        },\n    ],\n)\n\nprint(resp.model_dump_json(indent=2))\n</pre> class QuestionAnswer(BaseModel):     question: str     answer: str   question = \"What is the meaning of life?\" context = (     \"The according to the devil the meaning of life is a life of sin and debauchery.\" )   resp = client.chat.completions.create(     model=\"gpt-3.5-turbo\",     response_model=QuestionAnswer,     messages=[         {             \"role\": \"system\",             \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",         },         {             \"role\": \"user\",             \"content\": f\"using the context: `{context}`\\n\\nAnswer the following question: `{question}`\",         },     ], )  print(resp.model_dump_json(indent=2)) <pre>{\n  \"question\": \"What is the meaning of life?\",\n  \"answer\": \"According to the devil, the meaning of life is a life of sin and debauchery.\"\n}\n</pre> In\u00a0[20]: Copied! <pre>from instructor import llm_validator\n\n\nNotEvilAnswer = Annotated[\n    str,\n    AfterValidator(\n        llm_validator(\"don't say objectionable things\", client=client)\n    ),\n]\n\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: NotEvilAnswer\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=QuestionAnswer,\n    max_retries=2,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"using the context: `{context}`\\n\\nAnswer the following question: `{question}`\",\n        },\n    ],\n)\n</pre> from instructor import llm_validator   NotEvilAnswer = Annotated[     str,     AfterValidator(         llm_validator(\"don't say objectionable things\", client=client)     ), ]   class QuestionAnswer(BaseModel):     question: str     answer: NotEvilAnswer   resp = client.chat.completions.create(     model=\"gpt-3.5-turbo\",     response_model=QuestionAnswer,     max_retries=2,     messages=[         {             \"role\": \"system\",             \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",         },         {             \"role\": \"user\",             \"content\": f\"using the context: `{context}`\\n\\nAnswer the following question: `{question}`\",         },     ], ) <pre>Retrying, exception: 1 validation error for QuestionAnswer\nanswer\n  Assertion failed, The statement promotes sin and debauchery, which can be considered objectionable. [type=assertion_error, input_value='The meaning of life, acc... of sin and debauchery.', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/assertion_error\nTraceback (most recent call last):\n  File \"/Users/jasonliu/dev/instructor/instructor/patch.py\", line 277, in retry_sync\n    return process_response(\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/jasonliu/dev/instructor/instructor/patch.py\", line 164, in process_response\n    model = response_model.from_response(\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jasonliu/dev/instructor/instructor/function_calls.py\", line 137, in from_response\n    return cls.model_validate_json(\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/jasonliu/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py\", line 532, in model_validate_json\n    return cls.__pydantic_validator__.validate_json(json_data, strict=strict, context=context)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\npydantic_core._pydantic_core.ValidationError: 1 validation error for QuestionAnswer\nanswer\n  Assertion failed, The statement promotes sin and debauchery, which can be considered objectionable. [type=assertion_error, input_value='The meaning of life, acc... of sin and debauchery.', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/assertion_error\n</pre> In\u00a0[21]: Copied! <pre>print(resp.model_dump_json(indent=2))\n</pre> print(resp.model_dump_json(indent=2)) <pre>{\n  \"question\": \"What is the meaning of life?\",\n  \"answer\": \"The meaning of life is subjective and can vary depending on one's beliefs and perspectives. According to the devil, it is a life of sin and debauchery. However, this viewpoint may not be universally accepted and should be evaluated critically.\"\n}\n</pre>"},{"location":"tutorials/3-1-validation-rag/#understanding-validators","title":"Understanding Validators\u00b6","text":""},{"location":"tutorials/3-1-validation-rag/#defining-validator-functions","title":"Defining Validator Functions\u00b6","text":""},{"location":"tutorials/3-1-validation-rag/#using-field","title":"Using Field\u00b6","text":"<p>We can also use the <code>Field</code> class to define validators. This is useful when we want to define a validator for a field that is primative, like a string or integer which supports a limited number of validators.</p>"},{"location":"tutorials/3-1-validation-rag/#providing-context","title":"Providing Context\u00b6","text":""},{"location":"tutorials/3-1-validation-rag/#using-openai-moderation","title":"Using OpenAI Moderation\u00b6","text":""},{"location":"tutorials/3-1-validation-rag/#general-validator","title":"General Validator\u00b6","text":""},{"location":"tutorials/3-1-validation-rag/#avoiding-hallucination-with-citations","title":"Avoiding hallucination with citations\u00b6","text":""},{"location":"tutorials/3-1-validation-rag/#reasking-with-validators","title":"Reasking with validators\u00b6","text":"<p>For most of these examples all we've done we've mostly only defined the validation logic. Which can be seperate from generation, however when we are given validation errors, we shouldn't end there! Instead instructor allows us to collect all the validation errors and reask the llm to rewrite their answer.</p> <p>Lets try to use a extreme example to illustrate this point:</p>"},{"location":"tutorials/4-validation/","title":"Validation","text":"<p>Instead of framing \"self-critique\" or \"self-reflection\" in AI as new concepts, we can view them as validation errors with clear error messages that the system can use to self correct.</p> <p>Pydantic offers an customizable and expressive validation framework for Python. Instructor leverages Pydantic's validation framework to provide a uniform developer experience for both code-based and LLM-based validation, as well as a reasking mechanism for correcting LLM outputs based on validation errors. To learn more check out the Pydantic docs on validators.</p> <p>Note: For the majority of this notebook we won't be calling openai, just using validators to see how we can control the validation of the objects.</p> <p>Validators will enable us to control outputs by defining a function like so:</p> <pre>def validation_function(value):\n    if condition(value):\n        raise ValueError(\"Value is not valid\")\n    return mutation(value)\n</pre> <p>Before we get started lets go over the general shape of a validator:</p> In\u00a0[61]: Copied! <pre>from pydantic import BaseModel, ValidationError\nfrom typing_extensions import Annotated\nfrom pydantic import AfterValidator\n\ndef name_must_contain_space(v: str) -&gt; str:\n    if \" \" not in v:\n        raise ValueError(\"Name must contain a space.\")\n    return v.lower()\n\nclass UserDetail(BaseModel):\n    age: int\n    name: Annotated[str, AfterValidator(name_must_contain_space)]\n\nperson = UserDetail(age=29, name=\"Jason\")\n</pre> from pydantic import BaseModel, ValidationError from typing_extensions import Annotated from pydantic import AfterValidator  def name_must_contain_space(v: str) -&gt; str:     if \" \" not in v:         raise ValueError(\"Name must contain a space.\")     return v.lower()  class UserDetail(BaseModel):     age: int     name: Annotated[str, AfterValidator(name_must_contain_space)]  person = UserDetail(age=29, name=\"Jason\") <pre>\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb Cell 4 line 1\n     &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#W3sZmlsZQ%3D%3D?line=10'&gt;11&lt;/a&gt;     age: int\n     &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#W3sZmlsZQ%3D%3D?line=11'&gt;12&lt;/a&gt;     name: Annotated[str, AfterValidator(name_must_contain_space)]\n---&gt; &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#W3sZmlsZQ%3D%3D?line=13'&gt;14&lt;/a&gt; person = UserDetail(age=29, name=\"Jason\")\n\nFile ~/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--&gt; 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for UserDetail\nname\n  Value error, Name must contain a space. [type=value_error, input_value='Jason', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error</pre> <p>Validation Applications</p> <p>Validators are essential in tackling the unpredictabile nature of LLMs.</p> <p>Straightforward examples include:</p> <ul> <li>Flagging outputs containing blacklisted words.</li> <li>Identifying outputs with tones like racism or violence.</li> </ul> <p>For more complex tasks:</p> <ul> <li>Ensuring citations directly come from provided content.</li> <li>Checking that the model's responses align with given context.</li> <li>Validating the syntax of SQL queries before execution.</li> </ul> <p>Using the instructor library, we streamline the integration of these validators. <code>instructor</code> manages the parsing and validation of outputs and automates retries for compliant responses. This simplifies the process for developers to implement new validation logic, minimizing extra overhead.</p> <p>To use instructor in our api calls, we just need to patch the openai client:</p> In\u00a0[5]: Copied! <pre>import instructor \nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\n</pre> import instructor  from openai import OpenAI  client = instructor.patch(OpenAI()) <p>Deterministic validation, characterized by its rule-based logic, ensures consistent outcomes for the same input. Let's explore how we can apply this concept through some examples.</p> <p>To begin with, we aim to prevent engagement in topics involving explicit violence.</p> <p>We will define a blacklist of violent words that cannot be mentioned in any messages:</p> In\u00a0[63]: Copied! <pre>blacklist = {\n    \"rob\",\n    \"steal\",\n    \"hurt\",\n    \"kill\",\n    \"attack\",\n}\n</pre> blacklist = {     \"rob\",     \"steal\",     \"hurt\",     \"kill\",     \"attack\", } <p>To validate if the message contains a blacklisted word we will use a field_validator over the 'message' field:</p> In\u00a0[64]: Copied! <pre>from pydantic import BaseModel, ValidationError, field_validator\nfrom pydantic.fields import Field\n\nclass Response(BaseModel):\n    message: str\n\n    @field_validator('message')\n    def message_cannot_have_blacklisted_words(cls, v: str) -&gt; str:\n        for word in v.split(): \n            if word.lower() in blacklist:\n                raise ValueError(f\"`{word}` was found in the message `{v}`\")\n        return v\n\nResponse(message=\"I will hurt him\")\n</pre> from pydantic import BaseModel, ValidationError, field_validator from pydantic.fields import Field  class Response(BaseModel):     message: str      @field_validator('message')     def message_cannot_have_blacklisted_words(cls, v: str) -&gt; str:         for word in v.split():              if word.lower() in blacklist:                 raise ValueError(f\"`{word}` was found in the message `{v}`\")         return v  Response(message=\"I will hurt him\") <pre>\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb Cell 17 line 1\n     &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X23sZmlsZQ%3D%3D?line=10'&gt;11&lt;/a&gt;                 raise ValueError(f\"`{word}` was found in the message `{v}`\")\n     &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X23sZmlsZQ%3D%3D?line=11'&gt;12&lt;/a&gt;         return v\n---&gt; &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X23sZmlsZQ%3D%3D?line=13'&gt;14&lt;/a&gt; Response(message=\"I will hurt him\")\n\nFile ~/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--&gt; 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for Response\nmessage\n  Value error, `hurt` was found in the message `I will hurt him` [type=value_error, input_value='I will hurt him', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error</pre> <p>To enhance our validation measures, we'll extend the scope to flag any answer that contains hateful content, harassment, or similar issues. OpenAI offers a moderation endpoint that addresses these concerns, and it's freely available when using OpenAI models.</p> <p>With the <code>instructor</code> library, this is just one function edit away:</p> In\u00a0[1]: Copied! <pre>from typing import Annotated\nfrom pydantic.functional_validators import AfterValidator\n</pre> from typing import Annotated from pydantic.functional_validators import AfterValidator In\u00a0[6]: Copied! <pre>from instructor import openai_moderation\n\nclass Response(BaseModel):\n    message: Annotated[str, AfterValidator(openai_moderation(client=client))]\n</pre> from instructor import openai_moderation  class Response(BaseModel):     message: Annotated[str, AfterValidator(openai_moderation(client=client))] <p>Now we have a more comprehensive flagging for violence and we can outsource the moderation of our messages.</p> In\u00a0[7]: Copied! <pre>Response(message=\"I want to make them suffer the consequences\")\n</pre> Response(message=\"I want to make them suffer the consequences\") <pre>\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\nCell In[7], line 1\n----&gt; 1 Response(message=\"I want to make them suffer the consequences\")\n\nFile ~/.virtualenvs/pampa-labs/lib/python3.10/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--&gt; 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for Response\nmessage\n  Value error, `I want to make them suffer the consequences` was flagged for harassment, harassment_threatening, violence, harassment/threatening [type=value_error, input_value='I want to make them suffer the consequences', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/value_error</pre> <p>And as an extra, we get flagging for other topics like religion, race etc.</p> In\u00a0[26]: Copied! <pre>Response(message=\"I will mock their religion\")\n</pre> Response(message=\"I will mock their religion\") <pre>\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\nCell In[26], line 1\n----&gt; 1 Response(message=\"I will mock their religion\")\n\nFile ~/.virtualenvs/pampa-labs/lib/python3.10/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--&gt; 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for Response\nmessage\n  Value error, `I will mock their religion` was flagged for ['harassment'] [type=value_error, input_value='I will mock their religion', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.5/v/value_error</pre> <p>In addition to content-based flags, we can also set criteria based on other aspects of the input text. For instance, to maintain user engagement, we might want to prevent the assistant from returning excessively long texts.</p> <p>Here, noticed that <code>Field</code> has built-in validators for <code>min_length</code> and <code>max_length</code>. to learn more checkout Field Contraints</p> In\u00a0[68]: Copied! <pre>class AssistantMessage(BaseModel):\n    message: str = Field(..., max_length=100)\n</pre> class AssistantMessage(BaseModel):     message: str = Field(..., max_length=100) In\u00a0[69]: Copied! <pre>AssistantMessage(message=\"Certainly! Lorem ipsum is a placeholder text commonly used in the printing and typesetting industry. Here's a sample of Lorem ipsum text: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam euismod velit vel tellus tempor, non viverra eros iaculis. Sed vel nisl nec mauris bibendum tincidunt. Vestibulum sed libero euismod, eleifend tellus id, laoreet elit. Donec auctor arcu ac mi feugiat, vel lobortis justo efficitur. Fusce vel odio vitae justo varius dignissim. Integer sollicitudin mi a justo bibendum ultrices. Quisque id nisl a lectus venenatis luctus. Please note that Lorem ipsum text is a nonsensical Latin-like text used as a placeholder for content, and it has no specific meaning. It's often used in design and publishing to demonstrate the visual aspects of a document without focusing on the actual content.\")\n</pre> AssistantMessage(message=\"Certainly! Lorem ipsum is a placeholder text commonly used in the printing and typesetting industry. Here's a sample of Lorem ipsum text: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam euismod velit vel tellus tempor, non viverra eros iaculis. Sed vel nisl nec mauris bibendum tincidunt. Vestibulum sed libero euismod, eleifend tellus id, laoreet elit. Donec auctor arcu ac mi feugiat, vel lobortis justo efficitur. Fusce vel odio vitae justo varius dignissim. Integer sollicitudin mi a justo bibendum ultrices. Quisque id nisl a lectus venenatis luctus. Please note that Lorem ipsum text is a nonsensical Latin-like text used as a placeholder for content, and it has no specific meaning. It's often used in design and publishing to demonstrate the visual aspects of a document without focusing on the actual content.\") <pre>\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb Cell 29 line 1\n----&gt; &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X41sZmlsZQ%3D%3D?line=0'&gt;1&lt;/a&gt; AssistantMessage(message=\"Certainly! Lorem ipsum is a placeholder text commonly used in the printing and typesetting industry. Here's a sample of Lorem ipsum text: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam euismod velit vel tellus tempor, non viverra eros iaculis. Sed vel nisl nec mauris bibendum tincidunt. Vestibulum sed libero euismod, eleifend tellus id, laoreet elit. Donec auctor arcu ac mi feugiat, vel lobortis justo efficitur. Fusce vel odio vitae justo varius dignissim. Integer sollicitudin mi a justo bibendum ultrices. Quisque id nisl a lectus venenatis luctus. Please note that Lorem ipsum text is a nonsensical Latin-like text used as a placeholder for content, and it has no specific meaning. It's often used in design and publishing to demonstrate the visual aspects of a document without focusing on the actual content.\")\n\nFile ~/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--&gt; 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for AssistantMessage\nmessage\n  String should have at most 100 characters [type=string_too_long, input_value=\"Certainly! Lorem ipsum i... on the actual content.\", input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/string_too_long</pre> <p>When incorporating external knowledge bases, it's crucial to ensure that the agent uses the provided context accurately and doesn't fabricate responses. Validators can be effectively used for this purpose. We can illustrate this with an example where we validate that a provided citation is actually included in the referenced text chunk:</p> In\u00a0[70]: Copied! <pre>from pydantic import ValidationInfo\n\nclass AnswerWithCitation(BaseModel):\n    answer: str\n    citation: str\n\n    @field_validator('citation')\n    @classmethod\n    def citation_exists(cls, v: str, info: ValidationInfo): \n        context = info.context\n        if context:\n            context = context.get('text_chunk')\n            if v not in context:\n                raise ValueError(f\"Citation `{v}` not found in text\")\n        return v\n</pre> from pydantic import ValidationInfo  class AnswerWithCitation(BaseModel):     answer: str     citation: str      @field_validator('citation')     @classmethod     def citation_exists(cls, v: str, info: ValidationInfo):          context = info.context         if context:             context = context.get('text_chunk')             if v not in context:                 raise ValueError(f\"Citation `{v}` not found in text\")         return v <p>Here we assume that there is a \"text_chunk\" field that contains the text that the model is supposed to use as context. We then use the <code>field_validator</code> decorator to define a validator that checks if the citation is included in the text chunk. If it's not, we raise a <code>ValueError</code> with a message that will be returned to the user.</p> In\u00a0[71]: Copied! <pre>AnswerWithCitation.model_validate(\n    {\n        \"answer\": \"Blueberries are packed with protein\", \n        \"citation\": \"Blueberries contain high levels of protein\"\n    },\n    context={\"text_chunk\": \"Blueberries are very rich in antioxidants\"}, \n)\n</pre> AnswerWithCitation.model_validate(     {         \"answer\": \"Blueberries are packed with protein\",          \"citation\": \"Blueberries contain high levels of protein\"     },     context={\"text_chunk\": \"Blueberries are very rich in antioxidants\"},  ) <pre>\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb Cell 34 line 1\n----&gt; &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X50sZmlsZQ%3D%3D?line=0'&gt;1&lt;/a&gt; AnswerWithCitation.model_validate(\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X50sZmlsZQ%3D%3D?line=1'&gt;2&lt;/a&gt;     {\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X50sZmlsZQ%3D%3D?line=2'&gt;3&lt;/a&gt;         \"answer\": \"Blueberries are packed with protein\", \n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X50sZmlsZQ%3D%3D?line=3'&gt;4&lt;/a&gt;         \"citation\": \"Blueberries contain high levels of protein\"\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X50sZmlsZQ%3D%3D?line=4'&gt;5&lt;/a&gt;     },\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X50sZmlsZQ%3D%3D?line=5'&gt;6&lt;/a&gt;     context={\"text_chunk\": \"Blueberries are very rich in antioxidants\"}, \n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X50sZmlsZQ%3D%3D?line=6'&gt;7&lt;/a&gt; )\n\nFile ~/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py:503, in BaseModel.model_validate(cls, obj, strict, from_attributes, context)\n    501 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    502 __tracebackhide__ = True\n--&gt; 503 return cls.__pydantic_validator__.validate_python(\n    504     obj, strict=strict, from_attributes=from_attributes, context=context\n    505 )\n\nValidationError: 1 validation error for AnswerWithCitation\ncitation\n  Value error, Citation `Blueberries contain high levels of protein` not found in text [type=value_error, input_value='Blueberries contain high levels of protein', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error</pre> <p>For scenarios requiring more nuanced validation than rule-based methods, we use probabilistic validation. This approach incorporates LLMs into the validation workflow for a sophisticated assessment of outputs.</p> <p>The <code>instructor</code> library offers the <code>llm_validator</code> utility for this purpose. By specifying the desired directive, we can use LLMs for complex validation tasks. Let's explore some intriguing use cases enabled by LLMs.</p> <p>This LLM will be tasked with determining whether the agent's responses are exclusively related to health topics. For this, we will use the <code>llm_validator</code> from <code>instructor</code> like so:</p> In\u00a0[73]: Copied! <pre>from instructor import llm_validator\n\nclass AssistantMessage(BaseModel):\n    message: Annotated[str, \n                       AfterValidator(\n                           llm_validator(\"don't talk about any other topic except health best practices and topics\", \n                                         client=client))]\n\nAssistantMessage(message=\"I would suggest you to visit Sicily as they say it is very nice in winter.\")\n</pre> from instructor import llm_validator  class AssistantMessage(BaseModel):     message: Annotated[str,                         AfterValidator(                            llm_validator(\"don't talk about any other topic except health best practices and topics\",                                           client=client))]  AssistantMessage(message=\"I would suggest you to visit Sicily as they say it is very nice in winter.\") <pre>\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb Cell 38 line 1\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X56sZmlsZQ%3D%3D?line=4'&gt;5&lt;/a&gt; class AssistantMessage(BaseModel):\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X56sZmlsZQ%3D%3D?line=5'&gt;6&lt;/a&gt;     message: Annotated[str, \n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X56sZmlsZQ%3D%3D?line=6'&gt;7&lt;/a&gt;                        AfterValidator(\n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X56sZmlsZQ%3D%3D?line=7'&gt;8&lt;/a&gt;                            llm_validator(\"don't talk about any other topic except health best practices and topics\", \n      &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X56sZmlsZQ%3D%3D?line=8'&gt;9&lt;/a&gt;                                          openai_client=client))]\n---&gt; &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#X56sZmlsZQ%3D%3D?line=10'&gt;11&lt;/a&gt; AssistantMessage(message=\"I would suggest you to visit Sicily as they say it is very nice in winter.\")\n\nFile ~/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--&gt; 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for AssistantMessage\nmessage\n  Assertion failed, The statement is not related to health best practices or topics. [type=assertion_error, input_value='I would suggest you to v...is very nice in winter.', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.4/v/assertion_error</pre> <p>Important that for these examples we're not waiting for the messages, to get this message we would need to call the openai with <code>response_model=AssistantMessage</code>.</p> <p>Using probabilistic validation, we can also assess the agent's reasoning process to ensure it's logical before providing a response. With chain of thought prompting, the model is expected to think in steps and arrive at an answer following its logical progression. If there are errors in this logic, the final response may be incorrect.</p> <p>Here we will use Pydantic's model_validator which allows us to apply validation over all the properties of the <code>AIResponse</code> at once.</p> <p>To make this easier we'll make a simple validation class that we can reuse for all our validation:</p> In\u00a0[74]: Copied! <pre>from typing import Optional\n\nclass Validation(BaseModel):\n    is_valid: bool = Field(..., description=\"Whether the value is valid based on the rules\")\n    error_message: Optional[str] = Field(..., description=\"The error message if the value is not valid, to be used for re-asking the model\")\n</pre> from typing import Optional  class Validation(BaseModel):     is_valid: bool = Field(..., description=\"Whether the value is valid based on the rules\")     error_message: Optional[str] = Field(..., description=\"The error message if the value is not valid, to be used for re-asking the model\") <p>The function we will call will integrate an LLM and will ask it to determine whether the answer the model provided follows from the chain of thought:</p> In\u00a0[75]: Copied! <pre>def validate_chain_of_thought(values):\n    chain_of_thought = values[\"chain_of_thought\"]\n    answer = values[\"answer\"]\n    resp = client.chat.completions.create(\n        model=\"gpt-4-1106-preview\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a validator. Determine if the value follows from the statement. If it is not, explain why.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Verify that `{answer}` follows the chain of thought: {chain_of_thought}\",\n            },\n        ],\n        response_model=Validation,\n    )\n    if not resp.is_valid:\n        raise ValueError(resp.error_message)\n    return values\n</pre> def validate_chain_of_thought(values):     chain_of_thought = values[\"chain_of_thought\"]     answer = values[\"answer\"]     resp = client.chat.completions.create(         model=\"gpt-4-1106-preview\",         messages=[             {                 \"role\": \"system\",                 \"content\": \"You are a validator. Determine if the value follows from the statement. If it is not, explain why.\",             },             {                 \"role\": \"user\",                 \"content\": f\"Verify that `{answer}` follows the chain of thought: {chain_of_thought}\",             },         ],         response_model=Validation,     )     if not resp.is_valid:         raise ValueError(resp.error_message)     return values <p>The use of the 'before' argument in this context is significant. It means that the validator will receive the complete dictionary of inputs in their raw form, before any parsing by Pydantic.</p> In\u00a0[76]: Copied! <pre>from typing import Any\nfrom pydantic import model_validator\n\nclass AIResponse(BaseModel):\n    chain_of_thought: str\n    answer: str\n\n    @model_validator(mode='before')\n    @classmethod\n    def chain_of_thought_makes_sense(cls, data: Any) -&gt; Any:\n        # here we assume data is the dict representation of the model\n        # since we use 'before' mode.\n        return validate_chain_of_thought(data)\n</pre> from typing import Any from pydantic import model_validator  class AIResponse(BaseModel):     chain_of_thought: str     answer: str      @model_validator(mode='before')     @classmethod     def chain_of_thought_makes_sense(cls, data: Any) -&gt; Any:         # here we assume data is the dict representation of the model         # since we use 'before' mode.         return validate_chain_of_thought(data) In\u00a0[77]: Copied! <pre>AIResponse(chain_of_thought=\"The user suffers from diabetes.\", answer=\"The user has a broken leg.\")\n</pre> AIResponse(chain_of_thought=\"The user suffers from diabetes.\", answer=\"The user has a broken leg.\") <pre>\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb Cell 47 line 1\n----&gt; &lt;a href='vscode-notebook-cell:/Users/jasonliu/dev/instructor/tutorials/5.validation.ipynb#Y103sZmlsZQ%3D%3D?line=0'&gt;1&lt;/a&gt; AIResponse(chain_of_thought=\"The user suffers from diabetes.\", answer=\"The user has a broken leg.\")\n\nFile ~/dev/instructor/.venv/lib/python3.11/site-packages/pydantic/main.py:164, in BaseModel.__init__(__pydantic_self__, **data)\n    162 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    163 __tracebackhide__ = True\n--&gt; 164 __pydantic_self__.__pydantic_validator__.validate_python(data, self_instance=__pydantic_self__)\n\nValidationError: 1 validation error for AIResponse\n  Value error, The statement about the user having a broken leg does not logically follow from the information provided about the user suffering from diabetes. These are two separate health conditions and one does not imply the other. [type=value_error, input_value={'chain_of_thought': 'The...user has a broken leg.'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.4/v/value_error</pre> <p>Integrating these validation examples with the OpenAI API is streamlined using <code>instructor</code>. After patching the OpenAI client with <code>instructor</code>, you simply need to specify a <code>response_model</code> for your requests. This setup ensures that all the validation processes occur automatically.</p> <p>To enable reasking you can set a maximum number of retries. When calling the OpenAI client, the system can re-attempt to generate a correct answer. It does this by resending the original query along with feedback on why the previous response was rejected, guiding the LLM towards a more accurate answer in subsequent attempts.</p> In\u00a0[79]: Copied! <pre>class QuestionAnswer(BaseModel):\n    question: str\n    answer: str\n\nquestion = \"What is the meaning of life?\"\ncontext = \"The according to the devil the meaning of life is a life of sin and debauchery.\"\n\n\nresp = client.chat.completions.create(\n    model=\"gpt-4-1106-preview\",\n    response_model=QuestionAnswer,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"using the context: `{context}`\\n\\nAnswer the following question: `{question}`\",\n        },\n    ],\n)\n\nresp.answer\n</pre> class QuestionAnswer(BaseModel):     question: str     answer: str  question = \"What is the meaning of life?\" context = \"The according to the devil the meaning of life is a life of sin and debauchery.\"   resp = client.chat.completions.create(     model=\"gpt-4-1106-preview\",     response_model=QuestionAnswer,     messages=[         {             \"role\": \"system\",             \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",         },         {             \"role\": \"user\",             \"content\": f\"using the context: `{context}`\\n\\nAnswer the following question: `{question}`\",         },     ], )  resp.answer Out[79]: <pre>'a life of sin and debauchery'</pre> In\u00a0[80]: Copied! <pre>from pydantic import BeforeValidator\n\nclass QuestionAnswer(BaseModel):\n    question: str\n    answer: Annotated[\n        str,\n        BeforeValidator(\n            llm_validator(\"don't say objectionable things\", client=client)\n        ),\n    ]\n\nresp = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_model=QuestionAnswer,\n    max_retries=2,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"using the context: `{context}`\\n\\nAnswer the following question: `{question}`\",\n        },\n    ],\n)\n\nresp.answer\n</pre> from pydantic import BeforeValidator  class QuestionAnswer(BaseModel):     question: str     answer: Annotated[         str,         BeforeValidator(             llm_validator(\"don't say objectionable things\", client=client)         ),     ]  resp = client.chat.completions.create(     model=\"gpt-3.5-turbo\",     response_model=QuestionAnswer,     max_retries=2,     messages=[         {             \"role\": \"system\",             \"content\": \"You are a system that answers questions based on the context. answer exactly what the question asks using the context.\",         },         {             \"role\": \"user\",             \"content\": f\"using the context: `{context}`\\n\\nAnswer the following question: `{question}`\",         },     ], )  resp.answer Out[80]: <pre>'The meaning of life is a concept that varies depending on individual perspectives and beliefs.'</pre> <p>This guide explains how to use deterministic and probabilistic validation techniques with Large Language Models (LLMs). We discussed using an instructor to establish validation processes for content filtering, context relevance maintenance, and model reasoning verification. These methods enhance the performance of LLMs across different tasks.</p> <p>For those interested in further exploration, here's a to-do list:</p> <ol> <li>SQL Syntax Checker: Create a validator to check the syntax of SQL queries before executing them.</li> <li>Context-Based Response Validation: Design a method to flag responses based on the model's own knowledge rather than the provided context.</li> <li>PII Detection: Implement a mechanism to identify and handle Personally Identifiable Information in responses while prioritizing user privacy.</li> <li>Targeted Rule-Based Filtering: Develop filters to remove specific content types, such as responses mentioning named entities.</li> </ol> <p>Completing these tasks will enable users to acquire practical skills in improving LLMs through advanced validation methods.</p>"},{"location":"tutorials/4-validation/#validators","title":"Validators\u00b6","text":""},{"location":"tutorials/4-validation/#setup-and-dependencies","title":"Setup and Dependencies\u00b6","text":""},{"location":"tutorials/4-validation/#software-20-rule-based-validators","title":"Software 2.0: Rule-based validators\u00b6","text":""},{"location":"tutorials/4-validation/#flagging-bad-keywords","title":"Flagging bad keywords\u00b6","text":""},{"location":"tutorials/4-validation/#flagging-using-openai-moderation","title":"Flagging using OpenAI Moderation\u00b6","text":""},{"location":"tutorials/4-validation/#filtering-very-long-messages","title":"Filtering very long messages\u00b6","text":""},{"location":"tutorials/4-validation/#avoiding-hallucination-with-citations","title":"Avoiding hallucination with citations\u00b6","text":""},{"location":"tutorials/4-validation/#software-30-probabilistic-validators","title":"Software 3.0: Probabilistic validators\u00b6","text":""},{"location":"tutorials/4-validation/#keeping-an-agent-on-topic","title":"Keeping an agent on topic\u00b6","text":"<p>When creating an agent focused on health improvement, providing answers and daily practice suggestions, it's crucial to ensure strict adherence to health-related topics. This is important because the knowledge base is limited to health topics, and veering off-topic could result in fabricated responses.</p> <p>To achieve this focus, we'll follow a similar process as before, but with an important addition: integrating an LLM into our validator.</p>"},{"location":"tutorials/4-validation/#validating-agent-thinking-with-cot","title":"Validating agent thinking with CoT\u00b6","text":""},{"location":"tutorials/4-validation/#reasking-with-validators","title":"Reasking with validators\u00b6","text":"<p>For most of these examples all we've done we've mostly only defined the validation logic.</p> <p>We'eve covered field validators and model validators and even used LLMs to validate our outputs. But we haven't actually used the validators to reask the model! One of the most powerful features of <code>instructor</code> is that it will automatically reask the model when it receives a validation error. This means that we can use the same validation logic for both code-based and LLM-based validation.</p> <p>This also means that our 'prompt' is not only the prompt we send, but the code that runs the validator, and the error message we send back to the model.</p>"},{"location":"tutorials/4-validation/#conclusion","title":"Conclusion\u00b6","text":""},{"location":"tutorials/5-knowledge-graphs/","title":"Knowledge Graphs","text":"<p>Today, we're going to use the <code>instructor</code> library to simplify the interaction between OpenAI and our code. Along with Graphviz library to bring structure to our intricate subjects and have a graph visualization.</p> In\u00a0[2]: Copied! <pre>import instructor \nfrom openai import OpenAI\n\nclient = instructor.patch(OpenAI())\n</pre> import instructor  from openai import OpenAI  client = instructor.patch(OpenAI()) <p>Install the Graphviz based on your operation system https://graphviz.org/download/</p> In\u00a0[3]: Copied! <pre>from pydantic import BaseModel, Field\nfrom typing import List, Optional\n\nclass Node(BaseModel):\n    id: int\n    label: str\n    color: str\n\nclass Edge(BaseModel):\n    source: int\n    target: int\n    label: str\n    color: str = \"black\"\n</pre> from pydantic import BaseModel, Field from typing import List, Optional  class Node(BaseModel):     id: int     label: str     color: str  class Edge(BaseModel):     source: int     target: int     label: str     color: str = \"black\" <p>The <code>KnowledgeGraph</code> class combines nodes and edges to create a comprehensive graph structure. It includes lists of nodes and edges, where each node represents a key concept or entity, and each edge represents a relationship between two nodes.</p> <p>Later on, you'll see that we designed this class to match the graph object in the graphviz library, which makes it easier to visualize our graph.</p> <p>The <code>visualize_knowledge_graph</code> function is used to visualize a knowledge graph. It takes a <code>KnowledgeGraph</code> object as input, which contains nodes and edges. The function utilizes the <code>graphviz</code> library to generate a directed graph (<code>Digraph</code>). Each node and edge from the <code>KnowledgeGraph</code> is added to the <code>Digraph</code> with their respective attributes (id, label, color). Finally, the graph is rendered and displayed.</p> In\u00a0[4]: Copied! <pre>from graphviz import Digraph\nfrom IPython.display import display\n\nclass KnowledgeGraph(BaseModel):\n    nodes: List[Node] = Field(..., default_factory=list)  # A list of nodes in the knowledge graph.\n    edges: List[Edge] = Field(..., default_factory=list)  # A list of edges in the knowledge graph.\n\n\n    def visualize_knowledge_graph(self):\n        dot = Digraph(comment=\"Knowledge Graph\")\n\n        for node in self.nodes:\n            dot.node(name=str(node.id), label=node.label, color=node.color)\n        for edge in self.edges:\n            dot.edge(str(edge.source), str(edge.target), label=edge.label, color=edge.color)\n        \n        return display(dot)\n</pre> from graphviz import Digraph from IPython.display import display  class KnowledgeGraph(BaseModel):     nodes: List[Node] = Field(..., default_factory=list)  # A list of nodes in the knowledge graph.     edges: List[Edge] = Field(..., default_factory=list)  # A list of edges in the knowledge graph.       def visualize_knowledge_graph(self):         dot = Digraph(comment=\"Knowledge Graph\")          for node in self.nodes:             dot.node(name=str(node.id), label=node.label, color=node.color)         for edge in self.edges:             dot.edge(str(edge.source), str(edge.target), label=edge.label, color=edge.color)                  return display(dot)  In\u00a0[8]: Copied! <pre>def generate_graph(input) -&gt; KnowledgeGraph:\n    return client.chat.completions.create(\n        model=\"gpt-4-1106-preview\",\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": f\"Help me understand the following by describing it as small knowledge graph: {input}\",\n            }\n        ],\n        response_model=KnowledgeGraph,\n    )\n</pre> def generate_graph(input) -&gt; KnowledgeGraph:     return client.chat.completions.create(         model=\"gpt-4-1106-preview\",         messages=[             {                 \"role\": \"user\",                 \"content\": f\"Help me understand the following by describing it as small knowledge graph: {input}\",             }         ],         response_model=KnowledgeGraph,     ) In\u00a0[9]: Copied! <pre>generate_graph(\"Explain quantum mechanics\").visualize_knowledge_graph()\n</pre> generate_graph(\"Explain quantum mechanics\").visualize_knowledge_graph() In\u00a0[10]: Copied! <pre>class Node(BaseModel):\n    id: int\n    label: str\n    color: str\n\n    def __hash__(self) -&gt; int:\n        return hash((id, self.label))\n    \nclass Edge(BaseModel):\n    source: int\n    target: int\n    label: str\n    color: str = \"black\"\n\n    def __hash__(self) -&gt; int:\n        return hash((self.source, self.target, self.label))\n</pre> class Node(BaseModel):     id: int     label: str     color: str      def __hash__(self) -&gt; int:         return hash((id, self.label))      class Edge(BaseModel):     source: int     target: int     label: str     color: str = \"black\"      def __hash__(self) -&gt; int:         return hash((self.source, self.target, self.label)) In\u00a0[11]: Copied! <pre>class KnowledgeGraph(BaseModel):\n    # Optional list of nodes and edges in the knowledge graph\n    nodes: Optional[List[Node]] = Field(..., default_factory=list)\n    edges: Optional[List[Edge]] = Field(..., default_factory=list)\n\n    def update(self, other: \"KnowledgeGraph\") -&gt; \"KnowledgeGraph\":\n        # This method updates the current graph with the other graph, deduplicating nodes and edges.\n        return KnowledgeGraph(\n            nodes=list(set(self.nodes + other.nodes)),  # Combine and deduplicate nodes\n            edges=list(set(self.edges + other.edges)),  # Combine and deduplicate edges\n        )\n    \n\n    def visualize_knowledge_graph(self):\n        dot = Digraph(comment=\"Knowledge Graph\")\n\n        for node in self.nodes:\n            dot.node(str(node.id), node.label, color=node.color)\n        for edge in self.edges:\n            dot.edge(str(edge.source), str(edge.target), label=edge.label, color=edge.color)\n        \n        return display(dot)\n</pre> class KnowledgeGraph(BaseModel):     # Optional list of nodes and edges in the knowledge graph     nodes: Optional[List[Node]] = Field(..., default_factory=list)     edges: Optional[List[Edge]] = Field(..., default_factory=list)      def update(self, other: \"KnowledgeGraph\") -&gt; \"KnowledgeGraph\":         # This method updates the current graph with the other graph, deduplicating nodes and edges.         return KnowledgeGraph(             nodes=list(set(self.nodes + other.nodes)),  # Combine and deduplicate nodes             edges=list(set(self.edges + other.edges)),  # Combine and deduplicate edges         )           def visualize_knowledge_graph(self):         dot = Digraph(comment=\"Knowledge Graph\")          for node in self.nodes:             dot.node(str(node.id), node.label, color=node.color)         for edge in self.edges:             dot.edge(str(edge.source), str(edge.target), label=edge.label, color=edge.color)                  return display(dot)  In\u00a0[12]: Copied! <pre>def generate_graph(input: List[str]) -&gt; KnowledgeGraph:\n    # Initialize an empty KnowledgeGraph\n    cur_state = KnowledgeGraph()\n\n    # Iterate over the input list\n    for i, inp in enumerate(input):\n        new_updates = client.chat.completions.create(\n            model=\"gpt-4-1106-preview\",\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"You are an iterative knowledge graph builder.\n                    You are given the current state of the graph, and you must append the nodes and edges \n                    to it Do not procide any duplcates and try to reuse nodes as much as possible.\"\"\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"Extract any new nodes and edges from the following:\n                    # Part {i}/{len(input)} of the input:\n\n                    {inp}\"\"\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"\"\"Here is the current state of the graph:\n                    {cur_state.model_dump_json(indent=2)}\"\"\",\n                },\n            ],\n            response_model=KnowledgeGraph,\n        )  # type: ignore\n\n        # Update the current state with the new updates\n        cur_state = cur_state.update(new_updates)\n\n        # Draw the current state of the graph\n        cur_state.visualize_knowledge_graph() \n        \n    # Return the final state of the KnowledgeGraph\n    return cur_state\n</pre> def generate_graph(input: List[str]) -&gt; KnowledgeGraph:     # Initialize an empty KnowledgeGraph     cur_state = KnowledgeGraph()      # Iterate over the input list     for i, inp in enumerate(input):         new_updates = client.chat.completions.create(             model=\"gpt-4-1106-preview\",             messages=[                 {                     \"role\": \"system\",                     \"content\": \"\"\"You are an iterative knowledge graph builder.                     You are given the current state of the graph, and you must append the nodes and edges                      to it Do not procide any duplcates and try to reuse nodes as much as possible.\"\"\",                 },                 {                     \"role\": \"user\",                     \"content\": f\"\"\"Extract any new nodes and edges from the following:                     # Part {i}/{len(input)} of the input:                      {inp}\"\"\",                 },                 {                     \"role\": \"user\",                     \"content\": f\"\"\"Here is the current state of the graph:                     {cur_state.model_dump_json(indent=2)}\"\"\",                 },             ],             response_model=KnowledgeGraph,         )  # type: ignore          # Update the current state with the new updates         cur_state = cur_state.update(new_updates)          # Draw the current state of the graph         cur_state.visualize_knowledge_graph()               # Return the final state of the KnowledgeGraph     return cur_state  <p>In this approach, we process the text in manageable chunks, one at a time.</p> <p>This method is particularly beneficial when dealing with extensive text that may not fit into a single prompt.</p> <p>It is especially useful in scenarios such as constructing a knowledge graph for a complex topic, where the information is distributed across multiple documents or sections.</p> In\u00a0[13]: Copied! <pre>text_chunks = [\n    \"Jason knows a lot about quantum mechanics. He is a physicist. He is a professor\",\n    \"Professors are smart.\",\n    \"Sarah knows Jason and is a student of his.\",\n    \"Sarah is a student at the University of Toronto. and UofT is in Canada.\",\n]\n\ngraph: KnowledgeGraph = generate_graph(text_chunks)\n</pre> text_chunks = [     \"Jason knows a lot about quantum mechanics. He is a physicist. He is a professor\",     \"Professors are smart.\",     \"Sarah knows Jason and is a student of his.\",     \"Sarah is a student at the University of Toronto. and UofT is in Canada.\", ]  graph: KnowledgeGraph = generate_graph(text_chunks) <p>This tutorial shows how to generate and visualize a knowledge graph for complex topics. It also demonstrates how to extract graphic knowledge from the language model or provided text. The tutorial highlights the iterative process of building the knowledge graph by processing text in smaller chunks and updating the graph with new information.</p> <p>Using this approach, we can extract various things, including:</p> <ol> <li>People and their relationships in a story.</li> </ol> <pre>class People(BaseModel):\n    id: str\n    name: str\n    description: str\n\nclass Relationship(BaseModel):\n    id: str\n    source: str\n    target: str\n    label: str\n    description: str\n\nclass Story(BaseModel):\n    people: List[People]\n    relationships: List[Relationship]\n</pre> <ol> <li>Task dependencies and action items from a transcript.</li> </ol> <pre>class Task(BaseModel):\n    id: str\n    name: str\n    description: str\n\nclass Participant(BaseModel):\n    id: str\n    name: str\n    description: str\n\nclass Assignment(BaseModel):\n    id: str\n    source: str\n    target: str\n    label: str\n    description: str\n\nclass Transcript(BaseModel):\n    tasks: List[Task]\n    participants: List[Participant]\n    assignments: List[Assignment]\n</pre> <ol> <li>Key concepts and their relationships from a research paper.</li> <li>Entities and their relationships from a news article.</li> </ol> <p>As an exercise, try to implement one of the above examples.</p> <p>All of them will follow an idea of iteratively extracting more and more information and accumulating it into some state.</p>"},{"location":"tutorials/5-knowledge-graphs/#knowledge-graphs-for-complex-topics","title":"Knowledge Graphs for Complex Topics\u00b6","text":""},{"location":"tutorials/5-knowledge-graphs/#introduction","title":"Introduction\u00b6","text":"<p>What is a knowledge graph?</p> <p>A knowledge graph, also known as a semantic network, represents real-world entities and their relationships. It consists of nodes, edges, and labels. Nodes can represent any entity, while edges define the connections between them. For example, a node representing an author like \"J.K. Rowling\" can be connected to another node representing one of her books, \"Harry Potter\", with the edge \"author of\".</p> <p>Applications of knowledge graphs</p> <p>Knowledge graphs have various applications, including:</p> <ul> <li>Search Engines: They enhance search results by incorporating semantic-search information from diverse sources.</li> <li>Recommendation Systems: They suggest products or services based on user behavior and preferences.</li> <li>Natural Language Processing: They aid in understanding and generating human language.</li> <li>Data Integration: They facilitate the integration of data from different sources by identifying relationships.</li> <li>Artificial Intelligence and Machine Learning: They provide contextual information to improve decision-making.</li> </ul>"},{"location":"tutorials/5-knowledge-graphs/#setup-and-dependencies","title":"Setup and Dependencies\u00b6","text":""},{"location":"tutorials/5-knowledge-graphs/#node-and-edge-classes","title":"Node and Edge Classes\u00b6","text":"<p>We begin by modeling our knowledge graph with Node and Edge objects.</p> <p>Node objects represent key concepts or entities, while Edge objects signify the relationships between them.</p>"},{"location":"tutorials/5-knowledge-graphs/#knowledgegraph-class","title":"<code>KnowledgeGraph</code> Class\u00b6","text":""},{"location":"tutorials/5-knowledge-graphs/#generating-the-knowledge-graph","title":"Generating the Knowledge Graph\u00b6","text":""},{"location":"tutorials/5-knowledge-graphs/#generate_graph-function","title":"generate_graph function\u00b6","text":"<p>The <code>generate_graph</code> function uses OpenAI's model to create a KnowledgeGraph object from an input string.</p> <p>It requests the model to interpret the input as a detailed knowledge graph and uses the response to form the KnowledgeGraph object.</p>"},{"location":"tutorials/5-knowledge-graphs/#advanced-accumulating-knowledge-graphs","title":"Advanced: Accumulating Knowledge Graphs\u00b6","text":"<p>When dealing with larger datasets, or knowledge that grows over time, processing them all at once can be challenging due to limitations in prompt length or the complexity of the content. In such cases, an iterative approach to building the knowledge graph can be beneficial. This method involves processing the text in smaller, manageable chunks and updating the graph with new information from each chunk.</p>"},{"location":"tutorials/5-knowledge-graphs/#what-are-the-benefits-of-this-approach","title":"What are the benefits of this approach?\u00b6","text":"<ul> <li><p>Scalability: This approach can handle large datasets by breaking them down into smaller, more manageable pieces.</p> </li> <li><p>Flexibility: It allows for dynamic updates to the graph, accommodating new information as it becomes available.</p> </li> <li><p>Efficiency: Processing smaller chunks of text can be more efficient and less prone to errors or omissions.</p> </li> </ul>"},{"location":"tutorials/5-knowledge-graphs/#what-has-changed","title":"What has changed?\u00b6","text":"<p>The previous example provided a basic structure, while this new example introduces additional complexity and functionality. The Node and Edge classes now have a hash method, allowing them to be used in sets and simplifying duplicate handling.</p> <p>The KnowledgeGraph class has been enhanced with two new methods: <code>update</code> and <code>draw</code>.</p> <p>In the KnowledgeGraph class, the nodes and edges fields are now optional, offering greater flexibility.</p> <p>The <code>update</code> method enables the merging and removal of duplicates from two graphs.</p> <p>The <code>draw</code> method includes a prefix parameter, making it easier to create different graph versions during iterations.</p>"},{"location":"tutorials/5-knowledge-graphs/#generate-iterative-graphs","title":"Generate iterative graphs\u00b6","text":"<p>The updated <code>generate_graph</code> function is specifically designed to handle a list of inputs iteratively. It updates the graph with each new piece of information.</p> <p>Upon closer inspection, this pattern resembles a common programming technique known as a \"reduce\" or \"fold\" function. A simple example of this would be iterating over a list to find the sum of all the elements squared.</p> <p>Here's an example in Python:</p> <pre>cur_state = 0\nfor i in [1, 2, 3, 4, 5]:\n    cur_state += i**2\nprint(cur_state)\n</pre>"},{"location":"tutorials/5-knowledge-graphs/#examples-use-case","title":"Examples Use Case\u00b6","text":""},{"location":"tutorials/5-knowledge-graphs/#conclusion","title":"Conclusion\u00b6","text":""},{"location":"tutorials/6-chain-of-density/","title":"6 chain of density","text":"<p>Article: {{ARTICLE}}</p> <p>You will generate increasingly concise, entity-dense summaries of the above Article.</p> <p>Repeat the following 2 steps 5 times.</p> <p>Step 1. Identify 1-3 informative Entities (\";\" delimited) from the Article which are missing from the previously generated summary. Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities.</p> <p>A Missing Entity is:</p> <ul> <li>Relevant: to the main story.</li> <li>Specific: descriptive yet concise (5 words or fewer).</li> <li>Novel; not in the previous summary.</li> <li>Faithful: present in the Article.</li> <li>Anywhere: located anywhere in the Article.</li> </ul> <p>Guidelines: phrases like \"the article discusses\"</p> <ul> <li>The first summary should be long (4-5 sentences, -80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \"this article discusses\") to reach -80 words.</li> <li>Make every word count: re-write the previous summary to improve flow and make space for additional entities.</li> <li>Make space with fusion, compression, and removal of uninformative</li> </ul> <ul> <li>The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.</li> <li>Missing entities can appear anywhere in the new summary.</li> <li>Never drop entities from the previous summary. If space cannot be made, add fewer new entities.</li> </ul> <p>Remember, use the exact same number of words for each summary.</p> <p>Answer in JSON. The JSON should be a list (length 5) of dictionaries whose keys are \"Missing_Entities\" and \"Denser_Summary\"</p> <p>While the original paper used a single prompt to generate the iterative generations, we can go one step better with <code>Instructor</code> and break down the process into smaller API calls - with validation along the way.</p> <p>The process can be broken down as seen below.</p> <p></p> <p>We'll need to install the tokenizer packages and the spacy english library before we can proceed with the rest of the lesson</p> In\u00a0[1]: Copied! <pre>import nltk\nnltk.download('punkt')\n\n!python -m spacy download en_core_web_sm --quiet\n</pre> import nltk nltk.download('punkt')  !python -m spacy download en_core_web_sm --quiet <pre>[nltk_data] Downloading package punkt to /Users/admin/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n</pre> <pre>\u2714 Download and installation successful\nYou can now load the package via spacy.load('en_core_web_sm')\n</pre> <p>Once that's done, let's now move on to writing some code.</p> <p>There are a few different definitions which we'll need to understand in the tutorial. They are</p> <ol> <li>Tokens and tokenizers</li> <li>Entities</li> <li>Entity-Dense</li> </ol> <p>Once we've gotten a hang of these concepts, we'll walk through a simple implementation of a Chain Of Density summarizer</p> In\u00a0[2]: Copied! <pre>import nltk\nsentence = \"My favourite type of Sashimi is Toro\"\n\nnltk.word_tokenize(sentence)\n</pre> import nltk sentence = \"My favourite type of Sashimi is Toro\"  nltk.word_tokenize(sentence) Out[2]: <pre>['My', 'favourite', 'type', 'of', 'Sashimi', 'is', 'Toro']</pre> <p>NLTK's word tokenizer does more than just split by empty whitespace. It handles a lot of nice edge cases and contractions such as <code>don't</code> or <code>I'm</code>.</p> In\u00a0[3]: Copied! <pre>sentence = \"I'm fascinated by machine learning!\"\n\nnltk.word_tokenize(sentence)\n</pre> sentence = \"I'm fascinated by machine learning!\"  nltk.word_tokenize(sentence) Out[3]: <pre>['I', \"'m\", 'fascinated', 'by', 'machine', 'learning', '!']</pre> <p>We can then calculate the number of tokens by simply finding the <code>len</code> of the generated sequence.</p> In\u00a0[4]: Copied! <pre>sentence = \"I'm fascinated by machine learning!\"\ntokens = nltk.word_tokenize(sentence)\nprint(tokens)\nprint(len(tokens))\n</pre> sentence = \"I'm fascinated by machine learning!\" tokens = nltk.word_tokenize(sentence) print(tokens) print(len(tokens)) <pre>['I', \"'m\", 'fascinated', 'by', 'machine', 'learning', '!']\n7\n</pre> In\u00a0[5]: Copied! <pre># First we load in the library\nimport spacy\n\n# Then we initialise an NLP object. \nnlp = spacy.load(\"en_core_web_sm\")\n</pre> # First we load in the library import spacy  # Then we initialise an NLP object.  nlp = spacy.load(\"en_core_web_sm\") In\u00a0[6]: Copied! <pre>sentence = \"Apple is looking at buying U.K. startup for $1 billion\"\n\ndoc = nlp(sentence)\ndoc.ents\n</pre> sentence = \"Apple is looking at buying U.K. startup for $1 billion\"  doc = nlp(sentence) doc.ents Out[6]: <pre>(Apple, U.K., $1 billion)</pre> <p>We can see that Spacy was able to identify unique and named entities that were present within the sentence using the <code>doc.ents</code> property. Let's see a few more examples.</p> In\u00a0[7]: Copied! <pre>sentence = \"A knowledge graph, also known as a semantic network\\\n, represents real-world entities and their relationships\"\n\ndoc = nlp(sentence)\ndoc.ents\n</pre> sentence = \"A knowledge graph, also known as a semantic network\\ , represents real-world entities and their relationships\"  doc = nlp(sentence) doc.ents Out[7]: <pre>()</pre> In\u00a0[8]: Copied! <pre>sentence = \"For example, a node representing an author like 'J.K. Rowling'\\\ncan be connected to another node representing one of her books, 'Harry Potter'\\\n, with the edge 'author of'\"\n\ndoc = nlp(sentence)\ndoc.ents\n</pre> sentence = \"For example, a node representing an author like 'J.K. Rowling'\\ can be connected to another node representing one of her books, 'Harry Potter'\\ , with the edge 'author of'\"  doc = nlp(sentence) doc.ents Out[8]: <pre>(J.K., one, Harry Potter')</pre> <p>As we can see from the examples above, entities are not nouns. They're direct or indirect references to people, places, concepts.</p> In\u00a0[9]: Copied! <pre>import math\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef calculate_entity_density(sentence:str):\n    tokens = nltk.word_tokenize(sentence)\n    entities = nlp(sentence).ents\n    entity_density = round(len(entities)/len(tokens),3)\n\n    return len(tokens),len(entities),entity_density\n</pre> import math nlp = spacy.load(\"en_core_web_sm\")  def calculate_entity_density(sentence:str):     tokens = nltk.word_tokenize(sentence)     entities = nlp(sentence).ents     entity_density = round(len(entities)/len(tokens),3)      return len(tokens),len(entities),entity_density In\u00a0[10]: Copied! <pre>sentence_1 = \"A knowledge graph, also known as a semantic network\\\n, represents real-world entities and their relationships\"\n\ncalculate_entity_density(sentence_1)\n</pre> sentence_1 = \"A knowledge graph, also known as a semantic network\\ , represents real-world entities and their relationships\"  calculate_entity_density(sentence_1) Out[10]: <pre>(17, 0, 0.0)</pre> In\u00a0[11]: Copied! <pre>sentence_2 = \"Apple is looking at buying U.K. startup for $1 billion\"\n\ncalculate_entity_density(sentence_2)\n</pre> sentence_2 = \"Apple is looking at buying U.K. startup for $1 billion\"  calculate_entity_density(sentence_2) Out[11]: <pre>(11, 3, 0.273)</pre> <p>This gives us a quantitative method to be able to understand and compare two different sentences/summaries.</p> <p>We want summaries that are more entity-dense</p> In\u00a0[12]: Copied! <pre>summary_1 = \"\"\"\nThis article discusses an incident that occurred during the Chinese Grand Prix\ninvolving two racing drivers, Jenson Button and Pastor Maldonado. The two were \ncompeting for the 13th place when Button collided with Maldonado's vehicle, \ncausing damage to both cars. The incident resulted in a penalty for Button, \nwho was demoted to 14th place. Maldonado, on the other hand, had to retire from \nthe race due to the damage his car sustained.\n\"\"\"\n\nsummary_2 = \"\"\"\nJenson Button's McLaren collided with Pastor Maldonado's Lotus during the Chinese \nGrand Prix, causing front wing damage to Button's car and rear-end damage to \nMaldonado's, forcing his retirement. Button received a five-second penalty and \ntwo superlicence points, dropping himto 14th. Fernando Alonso advanced two places, \nwhile Button was lapped by Nico Rosberg and Alonso by Sebastian Vettel and \nKimi Raikkonen.\n\"\"\"\n\ncalculate_entity_density(summary_1),calculate_entity_density(summary_2)\n</pre> summary_1 = \"\"\" This article discusses an incident that occurred during the Chinese Grand Prix involving two racing drivers, Jenson Button and Pastor Maldonado. The two were  competing for the 13th place when Button collided with Maldonado's vehicle,  causing damage to both cars. The incident resulted in a penalty for Button,  who was demoted to 14th place. Maldonado, on the other hand, had to retire from  the race due to the damage his car sustained. \"\"\"  summary_2 = \"\"\" Jenson Button's McLaren collided with Pastor Maldonado's Lotus during the Chinese  Grand Prix, causing front wing damage to Button's car and rear-end damage to  Maldonado's, forcing his retirement. Button received a five-second penalty and  two superlicence points, dropping himto 14th. Fernando Alonso advanced two places,  while Button was lapped by Nico Rosberg and Alonso by Sebastian Vettel and  Kimi Raikkonen. \"\"\"  calculate_entity_density(summary_1),calculate_entity_density(summary_2) Out[12]: <pre>((82, 11, 0.134), (71, 17, 0.239))</pre> <p>We can see that the final summary is almost twice as dense as the first summary and is hence more entity dense.</p> In\u00a0[13]: Copied! <pre>from pydantic import BaseModel,Field,field_validator\nfrom typing import List\n</pre> from pydantic import BaseModel,Field,field_validator from typing import List In\u00a0[14]: Copied! <pre>class InitialSummary(BaseModel):\n    \"\"\"\n    This is an initial summary which should be long ( 4-5 sentences, ~80 words)\n    yet highly non-specific, containing little information beyond the entities marked as missing.\n    Use overly verbose languages and fillers (Eg. This article discusses) to reach ~80 words.\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This is a summary of the article provided which is overly verbose and uses fillers. \\\n        It should be roughly 80 words in length\",\n    )\n</pre> class InitialSummary(BaseModel):     \"\"\"     This is an initial summary which should be long ( 4-5 sentences, ~80 words)     yet highly non-specific, containing little information beyond the entities marked as missing.     Use overly verbose languages and fillers (Eg. This article discusses) to reach ~80 words.     \"\"\"      summary: str = Field(         ...,         description=\"This is a summary of the article provided which is overly verbose and uses fillers. \\         It should be roughly 80 words in length\",     ) <p>Pydantic is extremely handy because it allows us to do two things</p> <ol> <li>We can validate that our generated outputs are consistent with what we want, and write vanilla python to validate so</li> <li>We can export the generated class definition into a simple schema that fits in perfectly with OpenAI's function calling</li> </ol> In\u00a0[15]: Copied! <pre>InitialSummary.model_json_schema()\n</pre> InitialSummary.model_json_schema() Out[15]: <pre>{'description': 'This is an initial summary which should be long ( 4-5 sentences, ~80 words)\\nyet highly non-specific, containing little information beyond the entities marked as missing.\\nUse overly verbose languages and fillers (Eg. This article discusses) to reach ~80 words.',\n 'properties': {'summary': {'description': 'This is a summary of the article provided which is overly verbose and uses fillers.         It should be roughly 80 words in length',\n   'title': 'Summary',\n   'type': 'string'}},\n 'required': ['summary'],\n 'title': 'InitialSummary',\n 'type': 'object'}</pre> <p>It's important here to provide a good description of the overall class and the respective fields. This is because all of the descriptions that we write for the individual fields and the class itself are directly used by the llm when generating outputs.</p> <p>Now, as a quick recap, when we rewrite our summaries at each step, we're performing a few things</p> <ol> <li>We identify any entities from the original article that are relevant which are missing from our current summary</li> <li>We then rewrite our summary, making sure to include as many of these new entities as possible with the goal of increasing the entity density of the new summary</li> <li>We then make sure that we have included all of the entities in our previous summary in the new rewritten summary.</li> </ol> <p>We can express this in the form of the data model seen below called <code>RewrittenSummary</code>.</p> In\u00a0[16]: Copied! <pre>class RewrittenSummary(BaseModel):\n    \"\"\"\n    This is a new, denser summary of identical length which covers every entity\n    and detail from the previous summary plus the Missing Entities.\n\n    Guidelines\n    - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities\n    - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n    - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.\n    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\"\n    - Missing entities can appear anywhere in the new summary\n\n    An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\",\n    )\n    absent: List[str] = Field(\n        ...,\n        default_factory=list,\n        description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\",\n    )\n    missing: List[str] = Field(\n        default_factory=list,\n        description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\",\n    )\n</pre> class RewrittenSummary(BaseModel):     \"\"\"     This is a new, denser summary of identical length which covers every entity     and detail from the previous summary plus the Missing Entities.      Guidelines     - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities     - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.     - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.     - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\"     - Missing entities can appear anywhere in the new summary      An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.     \"\"\"      summary: str = Field(         ...,         description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\",     )     absent: List[str] = Field(         ...,         default_factory=list,         description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\",     )     missing: List[str] = Field(         default_factory=list,         description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\",     ) <p>We'd also want our rewritten summary to have</p> <ol> <li>No missing entities =&gt; <code>absent</code> should have a length of 0</li> <li>New entities to be added in the next rewrite -&gt; <code>missing</code> should have at least 1 entry</li> <li>A minimum length of 60 tokens and to have a density of at least 0.08 ( NOTE: 60 tokens and the 0.08 cut off are chosen arbitrarily, feel free to adjust them even higher if you wish. However, this might require you to add more retries in your code )</li> </ol> <p>We can do so using the <code>field_validator</code> that we learnt in the previous lesson. This allows us to add in a validator for a specific field to ensure it meets our requirements.</p> <p>This gives us the final definition of our <code>RewrittenSummary</code> class as seen below</p> In\u00a0[17]: Copied! <pre>class RewrittenSummary(BaseModel):\n    \"\"\"\n    This is a new, denser summary of identical length which covers every entity\n    and detail from the previous summary plus the Missing Entities.\n\n    Guidelines\n    - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities\n    - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n    - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.\n    - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\"\n    - Missing entities can appear anywhere in the new summary\n\n    An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.\n    \"\"\"\n\n    summary: str = Field(\n        ...,\n        description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\",\n    )\n    absent: List[str] = Field(\n        ...,\n        default_factory=list,\n        description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\",\n    )\n    missing: List[str] = Field(\n        default_factory=list,\n        description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\",\n    )\n        \n    \n    @field_validator(\"summary\")\n    def min_length(cls, v: str):\n        tokens = nltk.word_tokenize(v) \n        num_tokens = len(tokens)\n        if num_tokens &lt; 60:\n            raise ValueError(\n                \"The current summary is too short. Please make sure that you generate a new summary that is around 80 words long.\"\n            )\n        return v\n    \n    @field_validator(\"missing\")\n    def has_missing_entities(cls, missing_entities: List[str]):\n        if len(missing_entities) == 0:\n            raise ValueError(\n                \"You must identify 1-3 informative Entities from the Article which are missing from the previously generated summary to be used in a new summary\"\n            )\n        return missing_entities\n    \n    @field_validator(\"absent\")\n    def has_no_absent_entities(cls, absent_entities: List[str]):\n        absent_entity_string = \",\".join(absent_entities)\n        if len(absent_entities) &gt; 0:\n            print(f\"Detected absent entities of {absent_entity_string}\")\n            raise ValueError(\n                f\"Do not omit the following Entities {absent_entity_string} from the new summary\"\n            )\n        return absent_entities\n    \n    @field_validator(\"summary\")\n    def min_entity_density(cls, v: str):\n        tokens = nltk.word_tokenize(v)\n        num_tokens = len(tokens)\n    \n        # Extract Entities\n        doc = nlp(v) \n        num_entities = len(doc.ents)\n    \n        density = num_entities / num_tokens\n        if density &lt; 0.08: \n            raise ValueError(\n                f\"The summary of {v} has too few entities. Please regenerate a new summary with more new entities added to it. Remember that new entities can be added at any point of the summary.\"\n            )\n    \n        return v\n</pre> class RewrittenSummary(BaseModel):     \"\"\"     This is a new, denser summary of identical length which covers every entity     and detail from the previous summary plus the Missing Entities.      Guidelines     - Make every word count : Rewrite the previous summary to improve flow and make space for additional entities     - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.     - The new summary should be highly dense and concise yet self-contained, eg., easily understood without the Article.     - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\"     - Missing entities can appear anywhere in the new summary      An Entity is a real-world object that's assigned a name - for example, a person, country a product or a book title.     \"\"\"      summary: str = Field(         ...,         description=\"This is a new, denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities. It should have the same length ( ~ 80 words ) as the previous summary and should be easily understood without the Article\",     )     absent: List[str] = Field(         ...,         default_factory=list,         description=\"this is a list of Entities found absent from the new summary that were present in the previous summary\",     )     missing: List[str] = Field(         default_factory=list,         description=\"This is a list of 1-3 informative Entities from the Article that are missing from the new summary which should be included in the next generated summary.\",     )                   @field_validator(\"summary\")     def min_length(cls, v: str):         tokens = nltk.word_tokenize(v)          num_tokens = len(tokens)         if num_tokens &lt; 60:             raise ValueError(                 \"The current summary is too short. Please make sure that you generate a new summary that is around 80 words long.\"             )         return v          @field_validator(\"missing\")     def has_missing_entities(cls, missing_entities: List[str]):         if len(missing_entities) == 0:             raise ValueError(                 \"You must identify 1-3 informative Entities from the Article which are missing from the previously generated summary to be used in a new summary\"             )         return missing_entities          @field_validator(\"absent\")     def has_no_absent_entities(cls, absent_entities: List[str]):         absent_entity_string = \",\".join(absent_entities)         if len(absent_entities) &gt; 0:             print(f\"Detected absent entities of {absent_entity_string}\")             raise ValueError(                 f\"Do not omit the following Entities {absent_entity_string} from the new summary\"             )         return absent_entities          @field_validator(\"summary\")     def min_entity_density(cls, v: str):         tokens = nltk.word_tokenize(v)         num_tokens = len(tokens)              # Extract Entities         doc = nlp(v)          num_entities = len(doc.ents)              density = num_entities / num_tokens         if density &lt; 0.08:              raise ValueError(                 f\"The summary of {v} has too few entities. Please regenerate a new summary with more new entities added to it. Remember that new entities can be added at any point of the summary.\"             )              return v In\u00a0[18]: Copied! <pre>from openai import OpenAI\nimport instructor\n\nclient = instructor.patch(OpenAI()) \n\ndef summarize_article(article: str, summary_steps: int = 3):\n    summary_chain = []\n    # We first generate an initial summary\n    summary: InitialSummary = client.chat.completions.create(  \n        model=\"gpt-4-1106-preview\",\n        response_model=InitialSummary,\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"Write a summary about the article that is long (4-5 sentences) yet highly non-specific. Use overly, verbose language and fillers(eg.,'this article discusses') to reach ~80 words\",\n            },\n            {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"},\n            {\n                \"role\": \"user\",\n                \"content\": \"The generated summary should be about 80 words.\",\n            },\n        ],\n        max_retries=2,\n    )\n    prev_summary = None\n    summary_chain.append(summary.summary)\n    for i in range(summary_steps):\n        missing_entity_message = (\n            []\n            if prev_summary is None\n            else [\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Please include these Missing Entities: {','.join(prev_summary.missing)}\",\n                },\n            ]\n        )\n        new_summary: RewrittenSummary = client.chat.completions.create( \n            model=\"gpt-4-1106-preview\",\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"\n                You are going to generate an increasingly concise,entity-dense summary of the following article.\n\n                Perform the following two tasks\n                - Identify 1-3 informative entities from the following article which is missing from the previous summary\n                - Write a new denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities\n\n                Guidelines\n                - Make every word count: re-write the previous summary to improve flow and make space for additional entities\n                - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n                - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.\n                - Missing entities can appear anywhere in the new summary\n                - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.\n                \"\"\",\n                },\n                {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"},\n                {\n                    \"role\": \"user\",\n                    \"content\": f\"Here is the previous summary: {summary_chain[-1]}\",\n                },\n                *missing_entity_message,\n            ],\n            max_retries=3, \n            max_tokens=1000,\n            response_model=RewrittenSummary,\n        )\n        summary_chain.append(new_summary.summary)\n        prev_summary = new_summary\n\n    return summary_chain\n</pre> from openai import OpenAI import instructor  client = instructor.patch(OpenAI())   def summarize_article(article: str, summary_steps: int = 3):     summary_chain = []     # We first generate an initial summary     summary: InitialSummary = client.chat.completions.create(           model=\"gpt-4-1106-preview\",         response_model=InitialSummary,         messages=[             {                 \"role\": \"system\",                 \"content\": \"Write a summary about the article that is long (4-5 sentences) yet highly non-specific. Use overly, verbose language and fillers(eg.,'this article discusses') to reach ~80 words\",             },             {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"},             {                 \"role\": \"user\",                 \"content\": \"The generated summary should be about 80 words.\",             },         ],         max_retries=2,     )     prev_summary = None     summary_chain.append(summary.summary)     for i in range(summary_steps):         missing_entity_message = (             []             if prev_summary is None             else [                 {                     \"role\": \"user\",                     \"content\": f\"Please include these Missing Entities: {','.join(prev_summary.missing)}\",                 },             ]         )         new_summary: RewrittenSummary = client.chat.completions.create(              model=\"gpt-4-1106-preview\",             messages=[                 {                     \"role\": \"system\",                     \"content\": \"\"\"                 You are going to generate an increasingly concise,entity-dense summary of the following article.                  Perform the following two tasks                 - Identify 1-3 informative entities from the following article which is missing from the previous summary                 - Write a new denser summary of identical length which covers every entity and detail from the previous summary plus the Missing Entities                  Guidelines                 - Make every word count: re-write the previous summary to improve flow and make space for additional entities                 - Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".                 - The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Article.                 - Missing entities can appear anywhere in the new summary                 - Never drop entities from the previous summary. If space cannot be made, add fewer new entities.                 \"\"\",                 },                 {\"role\": \"user\", \"content\": f\"Here is the Article: {article}\"},                 {                     \"role\": \"user\",                     \"content\": f\"Here is the previous summary: {summary_chain[-1]}\",                 },                 *missing_entity_message,             ],             max_retries=3,              max_tokens=1000,             response_model=RewrittenSummary,         )         summary_chain.append(new_summary.summary)         prev_summary = new_summary      return summary_chain In\u00a0[19]: Copied! <pre>with open(\"./assets/article.txt\",\"r+\") as file:\n    article = file.readline()\n</pre> with open(\"./assets/article.txt\",\"r+\") as file:     article = file.readline() In\u00a0[\u00a0]: Copied! <pre>%%time\n\nsummaries = summarize_article(article)\n</pre> %%time  summaries = summarize_article(article) <p>We can see that it took roughly 40 seconds to do an iterative chain of density using this article. But does our approach increase the density of each individual summary? We can check by calculating the entity density of each summary in our list of summaries using the <code>calculate_entity_density</code> function we defined above.</p> In\u00a0[\u00a0]: Copied! <pre>for index,summary in enumerate(summaries):\n    tokens,entity,density = calculate_entity_density(summary)\n    print(f\"Article {index+1} -&gt; Results (Tokens: {tokens}, Entity Count: {entity}, Density: {density})\")\n</pre> for index,summary in enumerate(summaries):     tokens,entity,density = calculate_entity_density(summary)     print(f\"Article {index+1} -&gt; Results (Tokens: {tokens}, Entity Count: {entity}, Density: {density})\") <p>We can take a look at the articles themselves to see if they qualitatively show improvement</p> In\u00a0[\u00a0]: Copied! <pre>for summary in summaries:\n    print(f\"\\n{summary}\\n\")\n</pre> for summary in summaries:     print(f\"\\n{summary}\\n\") <p>As we can see, the articles progressively introduce more entities and become more entity dense. We've performed 4 rounds of summarization here but you could definitely do with maybe 2-3 if latency is a significant issue.</p> <p>This guide showed how to to generate complex summaries using chain of density summarization. We spent some time covering how to apply more complex validators - using <code>spaCy</code> and <code>NLTK</code> to ensure we had a minimum number of tokens and entity density as well as how you might apply instructor in a multi-stage process.</p> <p>By building in validation at each step of the proccess, this helps to improve the performance of your LLM across various tasks.</p> <p>For those looking to delve deeper, here are some to-do lists to explore.</p> <ul> <li>Validate Increasing Entity Density: <code>Pydantic</code> exposes a more complex validator that can take in an arbitrary python dictionary. Use the validation context to check the entity density of the previous summary and the new summary to validate that our model has generated a more entity-dense rewrite</li> <li>Fine-Tuning : <code>Instructor</code> comes with a simple to use interface to help you fine-tune other OpenAI models for your needs. This can be accomplished by capturing the outputs of LLMs using the <code>Instructions</code> module to generate training data for fine-tuning. In this specific case, finetuning a model to generate dense summaries could decrease latency and cost significantly by replacing the iterative LLM calls that we make .</li> </ul> <p>By accomplishing these tasks, you'll gain practical experience in tuning your models to suit your specific tasks as well as build in more complex validation processes when working with LLMs to ensure more reliable, accurate and consistent outputs.</p>"},{"location":"tutorials/6-chain-of-density/#chain-of-density-summarization","title":"Chain Of Density Summarization\u00b6","text":""},{"location":"tutorials/6-chain-of-density/#introduction","title":"Introduction\u00b6","text":"<p>What is Chain Of Density summarization?</p> <p>Summarizing extensive texts with AI can be challenging. Initially, an AI produces a summary, then refines it through multiple iterations, adding missing article entities. Each iteration adds new article entities to the summary, keeping length consistent, leading to an entity-dense, informative summary called Chain Of Density.</p> <p>It was first introduced in the paper - From Sparse to Dense : GPT-4 Summarization with Chain of Density prompting.</p> <p>This was done in the original paper by asking GPT-4 to generate all of the rewritten summaries in a single go with the following prompt below.</p>"},{"location":"tutorials/6-chain-of-density/#setup-and-dependencies","title":"Setup and Dependencies\u00b6","text":"<p>We'll be using two new libraries for our demonstration</p> <ol> <li><code>spaCy</code> : This provides a handful of useful utilities to do generic NLP tasks with</li> <li><code>nltk</code> : This was used by the original paper to count the number of tokens in our generated summaries</li> </ol>"},{"location":"tutorials/6-chain-of-density/#definitions","title":"Definitions\u00b6","text":""},{"location":"tutorials/6-chain-of-density/#tokens-and-tokenizers","title":"Tokens and Tokenizers\u00b6","text":"<p>In the original paper, the authors used <code>NLTK</code> to split the generated summary into tokens. These represent the smallest units that each sentence could be broken into where each hold semantic meaning.</p> <p>Let's walk through a simple example to see how the <code>NLTK</code> tokenizer might work</p>"},{"location":"tutorials/6-chain-of-density/#entities","title":"Entities\u00b6","text":"<p>A named entity is an object in the real-world that we identify using a name. Common examples include people, countries, products or even books that we know and love. We can use the <code>spaCy</code> library for us to be able to detect the number of entities in a given sentence.</p>"},{"location":"tutorials/6-chain-of-density/#entity-density","title":"Entity Density\u00b6","text":"<p>Now that we know what tokens and tokens are, we can move on to our last concept - that of entity density. Entity density is simply the mean number of entities present per token within your string of text.</p>"},{"location":"tutorials/6-chain-of-density/#implementation","title":"Implementation\u00b6","text":""},{"location":"tutorials/6-chain-of-density/#data-classes","title":"Data Classes\u00b6","text":"<p>Let's start by walking through some of the data models that we'll be using as the response_model for our open ai function calls. We'll need a total of two different classes</p> <ol> <li>Initial Summary: which is the lengthy and overly verbose article</li> <li>Rewritten Summary : which represents</li> </ol>"},{"location":"tutorials/6-chain-of-density/#putting-it-all-together","title":"Putting it all together\u00b6","text":"<p>Now that we have our models, let's implement a function to summarize a piece of text using a Chain Of Density summarization</p>"},{"location":"tutorials/6-chain-of-density/#trial-run","title":"Trial Run\u00b6","text":"<p>Let's try running this on some sample text which we can import in from our repository. We've provided a sample article in a file called <code>article.txt</code></p>"},{"location":"tutorials/6-chain-of-density/#future-steps","title":"Future Steps\u00b6","text":""},{"location":"tutorials/7-synthetic-data-generation/","title":"Synthetic Data Generation","text":"In\u00a0[91]: Copied! <pre>!uv pip install instructor openai datasets lancedb tantivy tenacity tqdm\n</pre> !uv pip install instructor openai datasets lancedb tantivy tenacity tqdm <pre>Audited 7 packages in 301ms\n</pre> <p>We're using <code>lancedb</code> here to easily ingest large amounts of data. This is preferable since we can define our table schema using a <code>Pydantic</code> Schema and also have LanceDB automatically handle the generation of the embeddings using their <code>get_registry()</code> method that we can define as an object property.</p> In\u00a0[6]: Copied! <pre>from lancedb import connect\n\n\nDB_PATH = \"./db\"\nDB_TABLE = \"ms_marco\"\n\n# Create a db at the path `./db`\ndb = connect(DB_PATH)\n</pre> from lancedb import connect   DB_PATH = \"./db\" DB_TABLE = \"ms_marco\"  # Create a db at the path `./db` db = connect(DB_PATH)   In\u00a0[31]: Copied! <pre>from lancedb.pydantic import LanceModel, Vector\nfrom lancedb.embeddings import get_registry\n\n\n\nfunc = get_registry().get(\"openai\").create(name=\"text-embedding-3-small\")\n\nclass Chunk(LanceModel):\n    passage:str = func.SourceField()\n    chunk_id:str\n    embedding:Vector(func.ndims()) = func.VectorField()\n\ntable = db.create_table(DB_TABLE, schema=Chunk, exist_ok=True, mode=\"overwrite\")\n</pre> from lancedb.pydantic import LanceModel, Vector from lancedb.embeddings import get_registry    func = get_registry().get(\"openai\").create(name=\"text-embedding-3-small\")  class Chunk(LanceModel):     passage:str = func.SourceField()     chunk_id:str     embedding:Vector(func.ndims()) = func.VectorField()  table = db.create_table(DB_TABLE, schema=Chunk, exist_ok=True, mode=\"overwrite\")  In\u00a0[32]: Copied! <pre>from datasets import load_dataset\n\nN_ROWS = 200\n\ndataset = load_dataset(\"ms_marco\", \"v1.1\", split=\"train\", streaming=True).take(N_ROWS)\n</pre> from datasets import load_dataset  N_ROWS = 200  dataset = load_dataset(\"ms_marco\", \"v1.1\", split=\"train\", streaming=True).take(N_ROWS)  In\u00a0[33]: Copied! <pre># from itertools import islice\nfirst_item = next(iter(dataset))\nfirst_item.keys()\n</pre> # from itertools import islice first_item = next(iter(dataset)) first_item.keys() Out[33]: <pre>dict_keys(['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'])</pre> In\u00a0[36]: Copied! <pre>first_item['passages']['passage_text'][:3]\n</pre> first_item['passages']['passage_text'][:3] Out[36]: <pre>[\"Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\",\n \"The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\",\n 'RBA Recognized with the 2014 Microsoft US Regional Partner of the ... by PR Newswire. Contract Awarded for supply and support the. Securitisations System used for risk management and analysis. ']</pre> In\u00a0[34]: Copied! <pre>import hashlib\nfrom itertools import batched\n\ndef get_passages(dataset):\n    for row in dataset:\n        for passage in row['passages']['passage_text']:\n            yield {\n                \"passage\":passage,\n                \"chunk_id\":hashlib.md5(passage.encode()).hexdigest()\n            }\n\n\npassages = batched(get_passages(dataset),10)\n\nfor passage_batch in passages:\n    # print(passage_batch)\n    table.add(list(passage_batch))\n</pre> import hashlib from itertools import batched  def get_passages(dataset):     for row in dataset:         for passage in row['passages']['passage_text']:             yield {                 \"passage\":passage,                 \"chunk_id\":hashlib.md5(passage.encode()).hexdigest()             }   passages = batched(get_passages(dataset),10)  for passage_batch in passages:     # print(passage_batch)     table.add(list(passage_batch))   In\u00a0[35]: Copied! <pre>from pydantic import BaseModel,Field\n\nclass QuestionAnswerPair(BaseModel):\n    \"\"\"\n    This model represents a pair of a question generated from a text chunk, its corresponding answer,\n    and the chain of thought leading to the answer. The chain of thought provides insight into how the answer\n    was derived from the question.\n    \"\"\"\n\n    chain_of_thought: str = Field(\n        description=\"The reasoning process leading to the answer.\"\n    )\n    question: str = Field(\n        description=\"The generated question from the text chunk.\"\n    )\n    answer: str = Field(description=\"The answer to the generated question.\")\n</pre> from pydantic import BaseModel,Field  class QuestionAnswerPair(BaseModel):     \"\"\"     This model represents a pair of a question generated from a text chunk, its corresponding answer,     and the chain of thought leading to the answer. The chain of thought provides insight into how the answer     was derived from the question.     \"\"\"      chain_of_thought: str = Field(         description=\"The reasoning process leading to the answer.\"     )     question: str = Field(         description=\"The generated question from the text chunk.\"     )     answer: str = Field(description=\"The answer to the generated question.\")   <p>Once we've defined this data-model, we can then use it in an instructor call to generate a synthetic question.</p> In\u00a0[42]: Copied! <pre>from openai import OpenAI\nfrom instructor import from_openai\n\nclient = from_openai(OpenAI())\n\ndef generate_question(chunk:str)-&gt;QuestionAnswerPair:\n    return client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a world class AI that excels at generating hypothethical search queries. You're about to be given a text snippet and asked to generate a search query which is specific to the specific text chunk that you'll be given. Make sure to use information from the text chunk.\",\n            },\n            {\n                \"role\": \"user\",\n                \"content\": f\"Here is the text chunk: {chunk}\"\n            }\n        ],\n        response_model=QuestionAnswerPair\n    )\n\ntext_chunk = \"\"\"\nThe Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\n\"\"\"\nprint(generate_question(text_chunk).model_dump_json(indent=2))\n</pre> from openai import OpenAI from instructor import from_openai  client = from_openai(OpenAI())  def generate_question(chunk:str)-&gt;QuestionAnswerPair:     return client.chat.completions.create(         model=\"gpt-4o\",         messages=[             {                 \"role\": \"system\",                 \"content\": \"You are a world class AI that excels at generating hypothethical search queries. You're about to be given a text snippet and asked to generate a search query which is specific to the specific text chunk that you'll be given. Make sure to use information from the text chunk.\",             },             {                 \"role\": \"user\",                 \"content\": f\"Here is the text chunk: {chunk}\"             }         ],         response_model=QuestionAnswerPair     )  text_chunk = \"\"\" The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site. \"\"\" print(generate_question(text_chunk).model_dump_json(indent=2))  <pre>{\n  \"chain_of_thought\": \"To form a specific question from the given text chunk, I should focus on the unique details provided about the Reserve Bank of Australia, such as its creation, functions, and assets.\",\n  \"question\": \"When was the Reserve Bank of Australia established as Australia's central bank and banknote issuing authority?\",\n  \"answer\": \"The Reserve Bank of Australia was established as Australia's central bank and banknote issuing authority on 14 January 1960.\"\n}\n</pre> <p>Now that we've seen how to generate a single question, let's see how we might be able to scale this up. We can do so by taking advantage of the <code>asyncio</code> library and <code>tenacity</code> to handle retries.</p> In\u00a0[56]: Copied! <pre>chunks = table.to_pandas()\nchunks = [item for item in chunks['passage']]\nchunks[:2]\n</pre> chunks = table.to_pandas() chunks = [item for item in chunks['passage']] chunks[:2] Out[56]: <pre>[\"Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\",\n \"The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\"]</pre> In\u00a0[98]: Copied! <pre>from asyncio import Semaphore\nfrom tenacity import retry, stop_after_attempt, wait_exponential\nfrom openai import AsyncOpenAI\nimport asyncio\nimport random\n\nclient = from_openai(AsyncOpenAI())\n\nasync def generate_questions(chunks:list[str],max_queries:int):\n    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n    async def generate_question(chunk:str,sem:Semaphore)-&gt;tuple[QuestionAnswerPair,str]:\n        async with sem:\n            return (await client.chat.completions.create(\n                model=\"gpt-3.5-turbo\",\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"You are a world class AI that excels at generating hypothethical search queries. You're about to be given a text snippet and asked to generate a search query which is specific to the specific text chunk that you'll be given. Make sure to use information from the text chunk.\",\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"Here is the text chunk: {chunk}\"\n                    }\n                ],\n                response_model=QuestionAnswerPair\n            ), chunk)\n    sem = Semaphore(max_queries)\n    coros = [\n        generate_question(chunk,sem)\n        for chunk in\n        chunks\n    ]\n    return await asyncio.gather(*coros)\n\n\nquestions = await generate_questions(chunks[:300],10)\n</pre> from asyncio import Semaphore from tenacity import retry, stop_after_attempt, wait_exponential from openai import AsyncOpenAI import asyncio import random  client = from_openai(AsyncOpenAI())  async def generate_questions(chunks:list[str],max_queries:int):     @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))     async def generate_question(chunk:str,sem:Semaphore)-&gt;tuple[QuestionAnswerPair,str]:         async with sem:             return (await client.chat.completions.create(                 model=\"gpt-3.5-turbo\",                 messages=[                     {                         \"role\": \"system\",                         \"content\": \"You are a world class AI that excels at generating hypothethical search queries. You're about to be given a text snippet and asked to generate a search query which is specific to the specific text chunk that you'll be given. Make sure to use information from the text chunk.\",                     },                     {                         \"role\": \"user\",                         \"content\": f\"Here is the text chunk: {chunk}\"                     }                 ],                 response_model=QuestionAnswerPair             ), chunk)     sem = Semaphore(max_queries)     coros = [         generate_question(chunk,sem)         for chunk in         chunks     ]     return await asyncio.gather(*coros)   questions = await generate_questions(chunks[:300],10) In\u00a0[64]: Copied! <pre>table.create_fts_index(\"passage\",replace=True)\n</pre> table.create_fts_index(\"passage\",replace=True) <p>This allows us to then use the <code>.search</code> function on each table to query it using full text search. Let's see an example below.</p> In\u00a0[67]: Copied! <pre>for entry in table.search(\"RBA\",query_type=\"fts\").limit(2).to_list():\n    print(entry['passage'])\n</pre> for entry in table.search(\"RBA\",query_type=\"fts\").limit(2).to_list():     print(entry['passage']) <pre>A rebuildable atomizer (RBA), often referred to as simply a \u201crebuildable,\u201d is just a special type of atomizer used in the Vape Pen and Mod Industry that connects to a personal vaporizer. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.\nResults-Based Accountability\u00ae (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;\n</pre> In\u00a0[84]: Copied! <pre>def rr(results,labels):\n    return max(\n        [round(1/(results.index(label)+1),2) if label in results else 0\n        for label in labels]\n        \n    )\n</pre> def rr(results,labels):     return max(         [round(1/(results.index(label)+1),2) if label in results else 0         for label in labels]              ) <p>This is an aggressive metric and once we get to an position of &gt; 10, the value doesn't change much anymore. Most of the big changes happen at indexes &lt; 10.</p> In\u00a0[69]: Copied! <pre>def recall(results,relevant_chunks):\n    return sum([1 if chunk in results else 0 for chunk in relevant_chunks])/len(relevant_chunks)\n</pre> def recall(results,relevant_chunks):     return sum([1 if chunk in results else 0 for chunk in relevant_chunks])/len(relevant_chunks) In\u00a0[86]: Copied! <pre>import hashlib\nsample_question,chunk = questions[0]\n\nchunk_id = hashlib.md5(chunk.encode()).hexdigest()\nchunk_id, sample_question.question, chunk\n</pre> import hashlib sample_question,chunk = questions[0]  chunk_id = hashlib.md5(chunk.encode()).hexdigest() chunk_id, sample_question.question, chunk Out[86]: <pre>('b6d9bf888fd53590ee69a913bd9bf8a4',\n \"What factors influence the average salary for people with a bachelor's degree?\",\n \"However, the average salary for people with a bachelor's degree varies widely based upon several factors, including their major, job position, location and years of experience. The National Association of Colleges and Employers conducted a salary survey that determined the average starting salary for graduates of various bachelor's degree programs.\")</pre> In\u00a0[81]: Copied! <pre>retrieved_results = table.search(sample_question.question,query_type='fts').limit(25).to_list()\nretrieved_chunk_ids = [item['chunk_id'] for item in retrieved_results]\n\nretrieved_chunk_ids[:3]\n</pre> retrieved_results = table.search(sample_question.question,query_type='fts').limit(25).to_list() retrieved_chunk_ids = [item['chunk_id'] for item in retrieved_results]  retrieved_chunk_ids[:3] Out[81]: <pre>['b6d9bf888fd53590ee69a913bd9bf8a4',\n '7a0254c9dc709220367857dcb67f2c8d',\n '04e7e6f91463033aa87b4104ea16b477']</pre> <p>We can now compute the results for the retrieved items that we've obtained using full text search relative to the ground truth label that we have - the original chunk that we generated it from</p> In\u00a0[85]: Copied! <pre>recall(retrieved_chunk_ids,[chunk_id]), rr(retrieved_chunk_ids,[chunk_id])\n</pre> recall(retrieved_chunk_ids,[chunk_id]), rr(retrieved_chunk_ids,[chunk_id]) Out[85]: <pre>(1.0, 1.0)</pre> <p>Scaling it up for different values of <code>k</code>, where we can see how this value changes for different subsets of the retrieved items is relatively simple.</p> <p>We can generate this mapping automatically using <code>itertools.product</code></p> In\u00a0[112]: Copied! <pre>from itertools import product\n\nSIZES = [3,5,10,15,25]\nMETRICS = [\n    [\"mrr\",rr],\n    [\"recall\",recall]\n]\n\nscore_fns = {}\n\nfor metric,size in product(METRICS,SIZES):\n    metric_name, score_fn = metric\n    score_fns[f\"{metric_name}@{size}\"] = lambda predictions,labels : score_fn(predictions[:size],labels)\n</pre> from itertools import product  SIZES = [3,5,10,15,25] METRICS = [     [\"mrr\",rr],     [\"recall\",recall] ]  score_fns = {}  for metric,size in product(METRICS,SIZES):     metric_name, score_fn = metric     score_fns[f\"{metric_name}@{size}\"] = lambda predictions,labels : score_fn(predictions[:size],labels) <p>We can now use the code above to run a test to see how our full text search performs for our synthetic questions.</p> In\u00a0[114]: Copied! <pre>import hashlib\nfrom tqdm import tqdm\n\nfts_results = []\n\nfor sample_qn, chunk in tqdm(questions):\n    chunk_id = hashlib.md5(chunk.encode()).hexdigest()\n    cleaned_question = ''.join(char for char in sample_qn.question if char.isalnum() or char.isspace())\n    retrieved_results = table.search(cleaned_question, query_type='fts').limit(25).to_list()\n    retrieved_chunk_ids = [item['chunk_id'] for item in retrieved_results]\n\n    fts_results.append(\n        {\n            metric: score_fn(retrieved_chunk_ids,[chunk_id])\n            for metric,score_fn\n            in score_fns.items()\n        }\n    )\n</pre> import hashlib from tqdm import tqdm  fts_results = []  for sample_qn, chunk in tqdm(questions):     chunk_id = hashlib.md5(chunk.encode()).hexdigest()     cleaned_question = ''.join(char for char in sample_qn.question if char.isalnum() or char.isspace())     retrieved_results = table.search(cleaned_question, query_type='fts').limit(25).to_list()     retrieved_chunk_ids = [item['chunk_id'] for item in retrieved_results]      fts_results.append(         {             metric: score_fn(retrieved_chunk_ids,[chunk_id])             for metric,score_fn             in score_fns.items()         }     ) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 300/300 [00:07&lt;00:00, 41.64it/s]\n</pre> In\u00a0[115]: Copied! <pre>import pandas as pd\n\ndf = pd.DataFrame(fts_results)\ndf.mean()\n</pre> import pandas as pd  df = pd.DataFrame(fts_results) df.mean() Out[115]: <pre>mrr@3        0.784267\nmrr@5        0.791267\nmrr@10       0.797633\nmrr@15       0.798133\nmrr@25       0.798433\nrecall@3     0.896667\nrecall@5     0.926667\nrecall@10    0.973333\nrecall@15    0.980000\nrecall@25    0.986667\ndtype: float64</pre> <p>We can see that on average full text search is able to surface the relevant item 97-98% of the time if we take <code>k=10</code> and that we have the relevant item in between the first and second item here.</p> <p>Now, because these are synthetic question, there's likely to be a large amount of overlap in the phrases used in the questions and the original source text, leading to the high values.</p> <p>In actual production applications and your domain specific dataset, it's useful to do these experiments and see what works best for your needs.</p>"},{"location":"tutorials/7-synthetic-data-generation/#synthetic-data-generation","title":"Synthetic Data Generation\u00b6","text":"<p>RAG Applications are often tricky to evaluate, especially when you haven't obtained any user queries to begin. In this notebook, we'll see how we can use <code>instructor</code> to quickly generate synthetic questions from a dataset to benchmark your retrieval systems using some simple metrics.</p>"},{"location":"tutorials/7-synthetic-data-generation/#data-ingestion","title":"Data Ingestion\u00b6","text":"<p>Let's first start by installing the required packages and ingesting the first 200 rows of the <code>ms-marco</code> dataset into our local database.</p>"},{"location":"tutorials/7-synthetic-data-generation/#synthetic-questions","title":"Synthetic Questions\u00b6","text":"<p>Now that we have the first ~2000 passages from the MS-Marco dataset ingested into our database. Let's start generating some synthetic questions using the chunks we've ingested.</p> <p>Let's see how we might do so using <code>instructor</code> by defining a datamodel that can help support this use-case.</p>"},{"location":"tutorials/7-synthetic-data-generation/#benchmarking-retrieval","title":"Benchmarking Retrieval\u00b6","text":"<p>Now that we've generated a list of questions to query our database with, let's do a quick benchmark to see how full text search compares against that of hybrid search. We'll use two simple metrics here - Mean Reciprocal Rank ( MRR ) and Recall.</p> <p>Let's start by making sure we have an inverted index created on our table above that we can perform full text search on</p>"},{"location":"tutorials/7-synthetic-data-generation/#metrics","title":"Metrics\u00b6","text":"<p>Now that we've figured out how we might be able to query our table using full text search. Let's take a step back and see how we can implement some metrics to quantiatively evaluate the retrieved items. It's important to note that when we want to evalute the quality of our listings, we always take it at some subset of k.</p> <p>This is important because k is often constrained by a business outcome and can help us determine how well our solution works</p> <p>Eg. Here are some hypothetical scenarios</p> <ul> <li>k=5 : We'd like to display some recomended items based of a user query (Eg. Help me plan out a dinner with Jonathan next week -&gt; Display 5 possible actions)</li> <li>k=10 : We have a small carousel with recomended items for a user to buy</li> <li>k=25 : We're using a re-ranker, is it filtering out the irrelevant chunks from the relevant chunks well?</li> <li>k=50 : We have a pipeline that fetches information for a model to respond with, are we fetching all relevant bits of information</li> </ul>"},{"location":"tutorials/7-synthetic-data-generation/#reciprocal-rank","title":"Reciprocal Rank\u00b6","text":"<p>Reciprocal Rank Imagine we're spotify and we want to suggest a couple of songs to the user. Which is a better result among the two lists of retrieved songs below? ( Note that 2 is the answer we want )</p> <ul> <li>[0,1,2,3,4]</li> <li>[0,1,3,4,2]</li> </ul> <p>Obviously if we're suggesting songs to the user, we want the first relevant song to be listed as early as possible! Therefore we'd prefer 1 over 2 in the example above because 2 is ordered earlier in the first case. A metric that works well for this is the Reciprocal Rank (RR).</p> <p></p>"},{"location":"tutorials/7-synthetic-data-generation/#recall","title":"Recall\u00b6","text":"<p>Another metric that we can track is recall which measures how many of our retrieved items were retrieved.</p> <p></p>"},{"location":"tutorials/7-synthetic-data-generation/#using-our-questions","title":"Using Our Questions\u00b6","text":"<p>Now that we've seen two metrics that we can use and how we might be able to generate some synthetic questions, let's try it out on an actual question.</p> <p>To do so, we'll first generate a unique chunk id for our original passage that we generated the question from.</p> <p>We'll then compare the chunk_ids of the retrieved chunks and then compute the <code>mrr</code> and the <code>recall</code> of the retrieved results.</p>"},{"location":"tutorials/7-synthetic-data-generation/#running-an-evaluation","title":"Running an Evaluation\u00b6","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/category/partnerships/","title":"Partnerships","text":""},{"location":"blog/category/observability/","title":"Observability","text":""},{"location":"blog/category/rag/","title":"RAG","text":""},{"location":"blog/category/python/","title":"Python","text":""},{"location":"blog/category/summarization/","title":"Summarization","text":""},{"location":"blog/category/open-source/","title":"Open Source","text":""},{"location":"blog/category/ollama/","title":"Ollama","text":""},{"location":"blog/category/anyscale/","title":"Anyscale","text":""},{"location":"blog/category/groq/","title":"Groq","text":""},{"location":"blog/category/mistral/","title":"Mistral","text":""},{"location":"blog/category/langchain/","title":"LangChain","text":""},{"location":"blog/category/validation/","title":"Validation","text":""},{"location":"blog/category/finetuning/","title":"Finetuning","text":""},{"location":"blog/page/2/","title":"Welcome to the Instructor Blog","text":""},{"location":"blog/page/3/","title":"Welcome to the Instructor Blog","text":""},{"location":"blog/archive/2024/page/2/","title":"2024","text":""}]}